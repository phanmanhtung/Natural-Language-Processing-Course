{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Programming_Assignment_5_Word_Vectors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanmanhtung/Natural-Language-Processing-Course/blob/master/Programming_Assignment_5_Word_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVrNgZu1vUqT",
        "colab_type": "text"
      },
      "source": [
        "# Programming Assignment 5 - Word Vectors\n",
        "\n",
        "In this programming assignment, we will implement a distributional method for computing word similarities.\n",
        "\n",
        "You may want to refer to the Section 20.7 \"Word Similarity: Distributional Methods\" in the textbook [1].\n",
        "\n",
        "The due for the assignment is on **April 29 (Wednesday)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R_xLrs0qe-d",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 0: Write your name and student ID\n",
        "\n",
        "Write your name and student id\n",
        "\n",
        "- Name: Phan Manh Tung\n",
        "- Student ID: Bi8-160"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkUzvYvkqOUc",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We use the data in the file [enwiki-tokenized-v2.txt](https://drive.google.com/file/d/1TUvn3xT3CwH1V9RhC0C4KLNCEOh7eolC/view?usp=sharing). This file contains tokenized text files sampled from Wikipedia.\n",
        "\n",
        "First, we will download the text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FIAgiYKqVCW",
        "colab_type": "code",
        "outputId": "cc80b9c9-5401-44df-a9f4-2129d86c106a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -f enwiki-tokenized-v2.txt\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id=\"1TUvn3xT3CwH1V9RhC0C4KLNCEOh7eolC\",\n",
        "                                    dest_path=\"./enwiki-tokenized-v2.txt\",\n",
        "                                    unzip=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1TUvn3xT3CwH1V9RhC0C4KLNCEOh7eolC into ./enwiki-tokenized-v2.txt... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qftHJKwD6jR",
        "colab_type": "code",
        "outputId": "8f2db70a-3259-4b1d-c508-7bb3d2ad093a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!head -5 enwiki-tokenized-v2.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In May 1963 Minter suffered serious injuries when racing a Norton at Brands Hatch , and his place in the team for the TT races was taken by Phil Read , who came third to second-place team mate Hartle . The team only raced for one season in selected races .\n",
            "The city of Kozhikode is north of the state capital Thiruvananthapuram . It is located at approximately . It has an elevation of 1 metre ( 3 ft ) along the coast with the city 's eastern edges rising to at least 15 metres , with a sandy coastal belt and a lateritic midland . The city has a long shoreline and small hills dot the terrain in the eastern and central regions . To the city 's west is the Laccadive Sea and from approximately to the east rises the Sahyadri Mountains .\n",
            "Fives is a minor sport , with courts scattered across the country , and a limited presence in a handful of private schools .\n",
            "Test of highest and best use .\n",
            "Titus , a slave to Mrs. John Cabot of Salem , established a business and successfully recruited blacks as privateers during the war . Captain Jonathan Haraden was considered one of the best privateers , simultaneously fighting three armed British ships . His efforts resulted in the capture of 10,000 cannons .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJjZUcEiqYBm",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: Extracting Context (10 points)\n",
        "\n",
        "In this exercise, you will extract context of all words in the file `enwiki-tokenized-v2.txt`.\n",
        "\n",
        "For each word *t*, you will extract surrounding words in the window size 5. They are 5 words occuring before/after of the word.\n",
        "\n",
        "Let's consider the example sentence as bellow.\n",
        "\n",
        "    He was born William McKinley Randle Jr. in Detroit , Michigan .\n",
        "\n",
        "The context of the word \"McKinley\" include words {\"He\", \"was\", \"born\", \"William\", \"Randle\", \"Jr.\", \"in\", \"Detroit\", \",\"}. Words \"Michigan\" and \".\" are not included in the context of the word \"McKinley\".\n",
        "\n",
        "Write the context of words into the file \"enwiki-tokenized-context.txt\". Each line contains the word *t* and a word *c* in its context. Two words in a line are separated by a tab character.\n",
        "\n",
        "For the word William in the above sentence, we will write following lines.\n",
        "\n",
        "```\n",
        "William    He\n",
        "William    was\n",
        "William    born\n",
        "William    Randle\n",
        "William    Jr.\n",
        "William    in\n",
        "William    Detroit\n",
        "William    ,\n",
        "```\n",
        "\n",
        "Write the code to do the job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHq_cTfpLbG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing code\n",
        "\n",
        "with open(\"enwiki-tokenized-v2.txt\") as f:\n",
        "  time = 0\n",
        "  for line in f:\n",
        "    words = line.split(\" \")\n",
        "    for word in words:\n",
        "      if (words.index(word)-5) <= 0:\n",
        "        for before in words[0:(words.index(word))]:\n",
        "          print(word, \"\\t\", before, \"\\n\")\n",
        "        for after in words[(words.index(word)+1):(words.index(word)+6)]:  \n",
        "          print(word, \"\\t\", after, \"\\n\")\n",
        "      else:\n",
        "        for before in words[(words.index(word)-5):(words.index(word))]:\n",
        "          print(word, \"\\t\", before, \"\\n\")\n",
        "        for after in words[(words.index(word)+1):(words.index(word)+6)]:  \n",
        "          print(word, \"\\t\", after, \"\\n\")\n",
        "    time+=1\n",
        "    if time==3:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKPDzF-dS6-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The main writing file function\n",
        "\n",
        "with open(\"enwiki-tokenized-v2.txt\") as f:\n",
        "  for line in f:\n",
        "    words = line.split(\" \")\n",
        "    with open(\"enwiki-tokenized-context.txt\", \"w\") as fo:\n",
        "      for word in words:\n",
        "        if (words.index(word)-5) <= 0:\n",
        "          for before in words[0:(words.index(word))]:\n",
        "            fo.write(word + \"\\t\" + before + \"\\n\")\n",
        "          for after in words[(words.index(word)+1):(words.index(word)+6)]:  \n",
        "            fo.write(word + \"\\t\" + after + \"\\n\")\n",
        "        else:\n",
        "          for before in words[(words.index(word)-5):(words.index(word))]:\n",
        "            fo.write(word + \"\\t\" + before + \"\\n\")\n",
        "          for after in words[(words.index(word)+1):(words.index(word)+6)]:  \n",
        "            fo.write(word + \"\\t\" + after + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txmtjf6pb_WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The main writing file function\n",
        "\n",
        "with open(\"enwiki-tokenized-v2.txt\") as f:\n",
        "  for line in f:\n",
        "    words = line.split(\" \")\n",
        "    with open(\"enwiki-tokenized-context.txt\", \"w\") as fo:\n",
        "      for word in words:\n",
        "        if (words.index(word)-5) <= 0:\n",
        "          for before in words[0:(words.index(word))]:\n",
        "            fo.write(\"%s  %s\\n\" % (word,before))\n",
        "          for after in words[(words.index(word)+1):(words.index(word)+6)]:  \n",
        "            fo.write(\"%s  %s\\n\" % (word,after))\n",
        "        else:\n",
        "          for before in words[(words.index(word)-5):(words.index(word))]:\n",
        "            fo.write(\"%s  %s\\n\" % (word,before))\n",
        "          for after in words[(words.index(word)+1):(words.index(word)+6)]:  \n",
        "            fo.write(\"%s  %s\\n\" % (word,after))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTIH7Pe1T4IB",
        "colab_type": "code",
        "outputId": "b7d6d183-de43-4b1d-e17a-1589cd37ec1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!head -20 enwiki-tokenized-context.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In  the\n",
            "In  British\n",
            "In  period\n",
            "In  it\n",
            "In  was\n",
            "the  In\n",
            "the  British\n",
            "the  period\n",
            "the  it\n",
            "the  was\n",
            "the  a\n",
            "British  In\n",
            "British  the\n",
            "British  period\n",
            "British  it\n",
            "British  was\n",
            "British  a\n",
            "British  Royal\n",
            "period  In\n",
            "period  the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McUN7-x8t3Mc",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2: Computing Frequency of Words in Contexts (20 points)\n",
        "\n",
        "In this exercise, you will write code to calculate frequencies of words in contexts you extracted in the Exercise 1 (File \"enwiki-tokenized-context.txt\"). Specifically, you will compute following values:\n",
        "\n",
        "- *f*(*t*,*c*): The number of times word *t* and context word *c* co-occur.\n",
        "- *f*(*t*,\\*): The number of time word *t* occurs.\n",
        "- *f*(\\*,*c*): The number of time the context word *c* occurs.\n",
        "- *N*: The total number of times words and their context words occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0E30GUGvPzC",
        "colab_type": "code",
        "outputId": "b19444be-b380-443c-cd1b-32b1c346d31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "f_tc = defaultdict(int)\n",
        "f_t = defaultdict(int)\n",
        "f_c = defaultdict(int)\n",
        "N = 0\n",
        "\n",
        "with open(\"enwiki-tokenized-context.txt\", \"r\") as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    text = line.split(\"  \")\n",
        "    word = text[0]\n",
        "    context = text[1]\n",
        "    f_tc[word + '\\t' + context] += 1\n",
        "    f_t[word] += 1\n",
        "    f_c[context] += 1\n",
        "    N += 1\n",
        "\n",
        "print(f_tc)\n",
        "print(f_t)\n",
        "print(f_c)\n",
        "print(N)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-bf22bafa5d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mf_tc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mf_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yp0zSsXXoGL",
        "colab_type": "code",
        "outputId": "498195cc-3d10-490b-a6dd-5fed8b5dfbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with open(\"enwiki-tokenized-context.txt\") as f:\n",
        "  for line in f:\n",
        "    a = line.split(\"  \")\n",
        "    b = a[1]\n",
        "    print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "In\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            "of\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            "of\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "hangars\n",
            "\n",
            "of\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "of\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            "stand\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            "In\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            "of\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            "After\n",
            "\n",
            "Independence\n",
            "\n",
            "Dr.\n",
            "\n",
            "hangars\n",
            "\n",
            "of\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            ".\n",
            "\n",
            "After\n",
            "\n",
            "Independence\n",
            "\n",
            "Dr.\n",
            "\n",
            "Bidhan\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            "Independence\n",
            "\n",
            "Dr.\n",
            "\n",
            "Bidhan\n",
            "\n",
            "Chandra\n",
            "\n",
            "Roy\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            "After\n",
            "\n",
            "Dr.\n",
            "\n",
            "Bidhan\n",
            "\n",
            "Chandra\n",
            "\n",
            "Roy\n",
            "\n",
            ",\n",
            "\n",
            "still\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            "After\n",
            "\n",
            "Independence\n",
            "\n",
            "Bidhan\n",
            "\n",
            "Chandra\n",
            "\n",
            "Roy\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            "After\n",
            "\n",
            "Independence\n",
            "\n",
            "Dr.\n",
            "\n",
            "Chandra\n",
            "\n",
            "Roy\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "then\n",
            "\n",
            ".\n",
            "\n",
            "After\n",
            "\n",
            "Independence\n",
            "\n",
            "Dr.\n",
            "\n",
            "Bidhan\n",
            "\n",
            "Roy\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "then\n",
            "\n",
            "CM\n",
            "\n",
            "After\n",
            "\n",
            "Independence\n",
            "\n",
            "Dr.\n",
            "\n",
            "Bidhan\n",
            "\n",
            "Chandra\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "then\n",
            "\n",
            "CM\n",
            "\n",
            "of\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "In\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Bidhan\n",
            "\n",
            "Chandra\n",
            "\n",
            "Roy\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "CM\n",
            "\n",
            "of\n",
            "\n",
            "West\n",
            "\n",
            "Bengal\n",
            "\n",
            "developed\n",
            "\n",
            "Chandra\n",
            "\n",
            "Roy\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "then\n",
            "\n",
            "of\n",
            "\n",
            "West\n",
            "\n",
            "Bengal\n",
            "\n",
            "developed\n",
            "\n",
            "that\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            "the\n",
            "\n",
            "RAF\n",
            "\n",
            "still\n",
            "\n",
            "stand\n",
            "\n",
            ".\n",
            "\n",
            ",\n",
            "\n",
            "the\n",
            "\n",
            "then\n",
            "\n",
            "CM\n",
            "\n",
            "of\n",
            "\n",
            "Bengal\n",
            "\n",
            "developed\n",
            "\n",
            "that\n",
            "\n",
            "airfield\n",
            "\n",
            "to\n",
            "\n",
            "the\n",
            "\n",
            "then\n",
            "\n",
            "CM\n",
            "\n",
            "of\n",
            "\n",
            "West\n",
            "\n",
            "developed\n",
            "\n",
            "that\n",
            "\n",
            "airfield\n",
            "\n",
            "to\n",
            "\n",
            "a\n",
            "\n",
            "then\n",
            "\n",
            "CM\n",
            "\n",
            "of\n",
            "\n",
            "West\n",
            "\n",
            "Bengal\n",
            "\n",
            "that\n",
            "\n",
            "airfield\n",
            "\n",
            "to\n",
            "\n",
            "a\n",
            "\n",
            "planned\n",
            "\n",
            "CM\n",
            "\n",
            "of\n",
            "\n",
            "West\n",
            "\n",
            "Bengal\n",
            "\n",
            "developed\n",
            "\n",
            "airfield\n",
            "\n",
            "to\n",
            "\n",
            "a\n",
            "\n",
            "planned\n",
            "\n",
            "city\n",
            "\n",
            "RAF\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airfield\n",
            "\n",
            ".\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "airlines\n",
            "\n",
            ",\n",
            "\n",
            "and\n",
            "\n",
            "three\n",
            "\n",
            "hangars\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "RAF\n",
            "\n",
            "developed\n",
            "\n",
            "that\n",
            "\n",
            "airfield\n",
            "\n",
            "to\n",
            "\n",
            "a\n",
            "\n",
            "city\n",
            "\n",
            ".\n",
            "\n",
            "Formerly\n",
            "\n",
            "this\n",
            "\n",
            "place\n",
            "\n",
            "that\n",
            "\n",
            "airfield\n",
            "\n",
            "to\n",
            "\n",
            "a\n",
            "\n",
            "planned\n",
            "\n",
            ".\n",
            "\n",
            "Formerly\n",
            "\n",
            "this\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            "to\n",
            "\n",
            "a\n",
            "\n",
            "planned\n",
            "\n",
            "city\n",
            "\n",
            ".\n",
            "\n",
            "this\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "a\n",
            "\n",
            "planned\n",
            "\n",
            "city\n",
            "\n",
            ".\n",
            "\n",
            "Formerly\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "planned\n",
            "\n",
            "city\n",
            "\n",
            ".\n",
            "\n",
            "Formerly\n",
            "\n",
            "this\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "Habra\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            ".\n",
            "\n",
            "Formerly\n",
            "\n",
            "this\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "Habra\n",
            "\n",
            "Urbun\n",
            "\n",
            "Colony\n",
            "\n",
            "Formerly\n",
            "\n",
            "this\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "``\n",
            "\n",
            "Habra\n",
            "\n",
            "Urbun\n",
            "\n",
            "Colony\n",
            "\n",
            "''\n",
            "\n",
            "this\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "Habra\n",
            "\n",
            "Urbun\n",
            "\n",
            "Colony\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n",
            "place\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "Urbun\n",
            "\n",
            "Colony\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n",
            "Later\n",
            "\n",
            "was\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "Habra\n",
            "\n",
            "Colony\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n",
            "Later\n",
            "\n",
            "it\n",
            "\n",
            "known\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "Habra\n",
            "\n",
            "Urbun\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n",
            "Later\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "as\n",
            "\n",
            "``\n",
            "\n",
            "Habra\n",
            "\n",
            "Urbun\n",
            "\n",
            "Colony\n",
            "\n",
            ".\n",
            "\n",
            "Later\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "named\n",
            "\n",
            ")\n",
            "\n",
            "station\n",
            "\n",
            ",\n",
            "\n",
            "an\n",
            "\n",
            "airfield\n",
            "\n",
            "It\n",
            "\n",
            "played\n",
            "\n",
            "host\n",
            "\n",
            "to\n",
            "\n",
            "airlines\n",
            "\n",
            "Habra\n",
            "\n",
            "Urbun\n",
            "\n",
            "Colony\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "named\n",
            "\n",
            "``\n",
            "\n",
            "Ashokenagar\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "was\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "In\n",
            "\n",
            "the\n",
            "\n",
            "British\n",
            "\n",
            "period\n",
            "\n",
            "it\n",
            "\n",
            "a\n",
            "\n",
            "Royal\n",
            "\n",
            "Air\n",
            "\n",
            "Force\n",
            "\n",
            "(\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n",
            "Later\n",
            "\n",
            "it\n",
            "\n",
            "was\n",
            "\n",
            "``\n",
            "\n",
            "Ashokenagar\n",
            "\n",
            "''\n",
            "\n",
            ".\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-c96bdb3fb093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqniyt08vUya",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 3: Computing Term-Context Matrix (20 points)\n",
        "\n",
        "In this exercise, you will use the output of Exercise 2 to compute the term/context matrix $X$. Rows of matrix represent words, and columns represent words in contexts. Values $X_{tc}$ of term/context matrix are defined as follows.\n",
        "\n",
        "- If $f(t,c) \\geq 10$, then $X_{tc}=\\text{PPMI}(t,c)=\\text{max}\\left(\\log \\frac{N\\times f(t,c)}{f(t,*)\\times f(*,c)}, 0\\right)$\n",
        "- If $f(t,c) < 10$, then $X_{tc}=0$\n",
        "\n",
        "Here $\\text{PPMI}(t, c)$ denotes Pointwise Mutual Information. Note that because the matrix size $X$ is very large, saving all matrix values to memory is not possible. You can use sparse matrix storage technique with the note that most of the values of the elements in $X$ are equal to 0.\n",
        "\n",
        "Hint: You can use [sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) matrix in scipy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlXk8nHYzAqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"Write your code to compute term-context matrix here\n",
        "\"\"\"\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgwlFHRuzGi0",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 4: Dimention Reduction with SVD (20 points)\n",
        "\n",
        "Use the SVD algorithm for the matrix obtained in Exercise 3 to reduce the number of data dimensions so that the resulting vector words have a dimension of 300.\n",
        "\n",
        "You can use [SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) implementation in sklearn.\n",
        "\n",
        "The implementation is something like as follows.\n",
        "```\n",
        "import sklearn.decomposition\n",
        "\n",
        "# matrix_x is the original sparse matrix\n",
        "\n",
        "clf = sklearn.decomposition.TruncatedSVD(300)\n",
        "matrix_x300 = clf.fit_transform(matrix_x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT74_vzm0Cjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: Write your code for dimension reduction here\n",
        "\"\"\"\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjZLHjUPz08t",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 5: Getting Word Vectors (10 points)\n",
        "\n",
        "Use word vectors obtained in Exercise 4, displaying the vector for the word \"United_States\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nLD9sic0_eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Write your code for getting word vector of a word here\n",
        "\"\"\"\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtbe8aSV1H-y",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 6: Calculating Word Similarity (20 points)\n",
        "\n",
        "Using the word vectors obtained in the Exercise 5, calculate cosine similarity for two words \"United_States\" and \"U.S\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceix6NQUt0yR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: Write your code for calculating word similarity here\n",
        "\"\"\"\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6I85ZHxqIMs",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Jurafsky et al. 2007. Speech and Language Processing (Second Edition)."
      ]
    }
  ]
}