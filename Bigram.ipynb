{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bigram.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM64xc06MtXBeZfqyEzdP+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanmanhtung/Natural-Language-Processing-Course/blob/master/Bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECsGAT_UQnj1",
        "colab_type": "code",
        "outputId": "1911a261-5de3-4ccb-e492-1be5052e6ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/ggcolab/'  #change dir to your project folder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy2Va6QCQo85",
        "colab_type": "code",
        "outputId": "f87e6dcf-8fc7-446f-8db7-1a20a45fa89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "!rm -f wiki-en-train.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
        "    \n",
        "!rm -f wiki-en-test.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
        "\n",
        "!rm -f 02-train-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
        "\n",
        "!rm -f 02-train-answer.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-06 13:07:59--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203886 (199K) [text/plain]\n",
            "Saving to: ‘wiki-en-train.word’\n",
            "\n",
            "\rwiki-en-train.word    0%[                    ]       0  --.-KB/s               \rwiki-en-train.word  100%[===================>] 199.11K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-02-06 13:07:59 (14.9 MB/s) - ‘wiki-en-train.word’ saved [203886/203886]\n",
            "\n",
            "--2020-02-06 13:08:08--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26989 (26K) [text/plain]\n",
            "Saving to: ‘wiki-en-test.word’\n",
            "\n",
            "wiki-en-test.word   100%[===================>]  26.36K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2020-02-06 13:08:09 (6.42 MB/s) - ‘wiki-en-test.word’ saved [26989/26989]\n",
            "\n",
            "--2020-02-06 13:08:17--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12 [text/plain]\n",
            "Saving to: ‘02-train-input.txt’\n",
            "\n",
            "02-train-input.txt  100%[===================>]      12  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-06 13:08:17 (1.99 MB/s) - ‘02-train-input.txt’ saved [12/12]\n",
            "\n",
            "--2020-02-06 13:08:24--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-answer.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144 [text/plain]\n",
            "Saving to: ‘02-train-answer.txt’\n",
            "\n",
            "02-train-answer.txt 100%[===================>]     144  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-06 13:08:24 (17.5 MB/s) - ‘02-train-answer.txt’ saved [144/144]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtsl8xuKG0nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# UNIGRAM\n",
        "def train_bigram(train_file, model_file):\n",
        "    \"\"\"Train trigram language model and save to model file\n",
        "    \"\"\"\n",
        "    counts = defaultdict(int)  # count the n-gram (a map count)\n",
        "    total_count = 0\n",
        "    #context_counts = defaultdict(int)   # count the context\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "\n",
        "            for word in words:\n",
        "              counts[word] += 1\n",
        "              total_count +=1\n",
        "              pass # do nothing\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "\n",
        "            fo.write((ngram + \"\\t\" + str(counts[ngram]/total_count))+ \"\\n\")\n",
        "\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFSyd3MWH-4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bigram('02-train-input.txt', 'train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSoiqxxIO-c",
        "colab_type": "code",
        "outputId": "944969c1-f8b0-4e92-dd20-7c364559fa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!cat train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\t0.25\n",
            "b\t0.25\n",
            "c\t0.125\n",
            "</s>\t0.25\n",
            "d\t0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2SlySMwKWtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# BIGRAM\n",
        "def train_bigram(train_file, model_file):\n",
        "    \"\"\"Train trigram language model and save to model file\n",
        "    \"\"\"\n",
        "    counts = defaultdict(int)  # count the n-gram (a map count)\n",
        "    total_count = 0\n",
        "    context_counts = defaultdict(int)\n",
        "    #context_counts = defaultdict(int)   # count the context\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "\n",
        "            for i in range(1, len(words)):\n",
        "                # Add bigram and bigram context\n",
        "                counts[words[i-1]+ \" \"+ words[i]] += 1             \n",
        "                context_counts[words[i-1]] += 1\n",
        "\n",
        "                # Add unigram and unigram context\n",
        "                total_count += 1\n",
        "                counts[words[i]] += 1\n",
        "                context_counts[\"\"] += 1\n",
        "                pass # do nothing\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "            # split ngram into an array of words\n",
        "            ngram_split = ngram.split()\n",
        "\n",
        "            # BIGRAM\n",
        "            if len(ngram_split) > 1: # \n",
        "              fo.write((ngram + \"\\t\" + str(counts[ngram]/context_counts[ngram_split[0]]))+\"\\n\")\n",
        "\n",
        "            # UNIGRAM\n",
        "            else:\n",
        "              fo.write((ngram + \"\\t\" + str(counts[ngram]/total_count))+\"\\n\")\n",
        "\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mw_7v5HNcDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bigram('02-train-input.txt', 'train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg4kXjQ9KWo0",
        "colab_type": "code",
        "outputId": "4d9d0043-7a25-4712-8284-d7777160c5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!cat train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> a\t1.0\n",
            "a\t0.25\n",
            "a b\t1.0\n",
            "b\t0.25\n",
            "b c\t0.5\n",
            "c\t0.125\n",
            "c </s>\t1.0\n",
            "</s>\t0.25\n",
            "b d\t0.5\n",
            "d\t0.125\n",
            "d </s>\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RoNSPutaS08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_bigram_model(model_file):\n",
        "    \"\"\"Load the model file\n",
        "\n",
        "    Args:\n",
        "        model_file (str): Path to the model file\n",
        "    \n",
        "    Returns:\n",
        "        probs (dict): Dictionary object that map from ngrams to their probabilities\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    with open(model_file, 'r') as f:\n",
        "        for line in f:\n",
        "            element = line.split(\"\\t\")\n",
        "            probs[element[0]] = float(element[1])\n",
        "            pass\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlSShLuma3dZ",
        "colab_type": "code",
        "outputId": "f769e4fa-4779-428d-8128-46d09b3de7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "probs = load_bigram_model('train-answer.txt')\n",
        "probs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'</s>': 0.25,\n",
              " '<s> a': 0.25,\n",
              " 'a': 0.25,\n",
              " 'a b': 0.25,\n",
              " 'b': 0.25,\n",
              " 'b c': 0.125,\n",
              " 'b d': 0.125,\n",
              " 'c': 0.125,\n",
              " 'c </s>': 0.125,\n",
              " 'd': 0.125,\n",
              " 'd </s>': 0.125}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKf0hzF34z-o",
        "colab_type": "code",
        "outputId": "20c29fbc-7857-4d71-e16c-a269d9aba8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!cat 02-train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "</s>\t0.250000\n",
            "<s> a\t1.000000\n",
            "a\t0.250000\n",
            "a b\t1.000000\n",
            "b\t0.250000\n",
            "b c\t0.500000\n",
            "b d\t0.500000\n",
            "c\t0.125000\n",
            "c </s>\t1.000000\n",
            "d\t0.125000\n",
            "d </s>\t1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95py3Lhvo3eF",
        "colab_type": "code",
        "outputId": "b934282e-f1a7-4a12-b710-89bcdc939302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "print(probs[\"a b\"])\n",
        "print(probs.get(\"a b\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-2a6ef84ce7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'a b'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax8brlRppuc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bigram('wiki-en-train.word', 'bigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siaOeHclp31K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs = load_bigram_model('bigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52x6FrK0bMS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def test_bigram(test_file, model_file, lambda2=0.95, lambda1=0.95, N=1000000):\n",
        "    W = 0 # Total word count\n",
        "    H = 0\n",
        "    probs = load_bigram_model(model_file)\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "            # get rid of the whitespace\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "\n",
        "            # split line into array of words\n",
        "            words = line.split()\n",
        "\n",
        "            # add to the end and beginning\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "            \n",
        "            for i in range(1, len(words)):  # Note: starting at 1, after <s>\n",
        "                # TODO: Write code to calculate smoothed unigram probabilties\n",
        "                # and smoothed bigram probabilities\n",
        "                # You should use calculate p1 as smoothed unigram probability\n",
        "                # and p2 as smoothed bigram probability\n",
        "\n",
        "                # If the unigram exists in the dictionary\n",
        "                if (probs.get(words[i])):\n",
        "                  p1 = lambda1 * probs.get(words[i]) + (1-lambda1) * (1/N)\n",
        "                else: # We assume the value of probs.get(words[i]) to be 0\n",
        "                  p1 = lambda1 * 0 + (1-lambda1) * (1/N) \n",
        "                \n",
        "                # If the bigram exists in the dictionary\n",
        "                if (probs.get(words[i-1] + \" \" + words[i])):\n",
        "                  p2 = lambda2 * probs.get(words[i-1] + \" \" + words[i])+ (1-lambda2) * p1\n",
        "                else: # We assume the value of probs.get(words[i-1] + \" \" + words[i]) to be 0\n",
        "                  p2 = lambda2 * 0 + (1-lambda2) * p1\n",
        "\n",
        "                W += 1  # Count the words\n",
        "                H += -math.log2(p2) # We use logarithm to avoid underflow\n",
        "    H = H/W\n",
        "    P = 2**H\n",
        "    \n",
        "    print(\"Entropy: {}\".format(H))\n",
        "    print(\"Perplexity: {}\".format(P))\n",
        "\n",
        "    return P\n",
        "\n",
        "\n",
        "# So the error occur when there is no \"In computational\" in the testing set, which appear None type in the probs dictionary\n",
        "# The solution is to quantify the missing value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaIsfTuznZ6N",
        "colab_type": "code",
        "outputId": "0bad9517-5231-4c32-f9bb-ede28e0d57a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_bigram('wiki-en-train.word', 'bigram_model.txt')\n",
        "test_bigram('wiki-en-test.word', 'bigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy: 11.284859117885485\n",
            "Perplexity: 2495.0605603552463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2495.0605603552463"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jkIAYu76RdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8k_9gQDQo_6",
        "colab_type": "code",
        "outputId": "2547ca2d-09fb-4e34-a84b-a095025ec300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "with open(\"/content/wiki-en-test.word\", \"r\") as f:\n",
        "  sample_text = f.read()\n",
        "\n",
        "sample_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In computational linguistics , word-sense disambiguation -LRB- WSD -RRB- is an open problem of natural language processing , which governs the process of identifying which sense of a word -LRB- i.e. meaning -RRB- is used in a sentence , when the word has multiple meanings -LRB- polysemy -RRB- .\\nThe solution to this problem impacts other computer-related writing , such as discourse , improving relevance of search engines , anaphora resolution , coherence , inference et cetera .\\nResearch has progressed steadily to the point where WSD systems achieve sufficiently high levels of accuracy on a variety of word types and ambiguities .\\nA rich variety of techniques have been researched , from dictionary-based methods that use the knowledge encoded in lexical resources , to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples , to completely unsupervised methods that cluster occurrences of words , thereby inducing word senses .\\nAmong these , supervised learning approaches have been the most successful algorithms to date .\\nCurrent accuracy is difficult to state without a host of caveats .\\nIn English , accuracy at the coarse-grained -LRB- homograph -RRB- level is routinely above 90 % , with some methods on particular homographs achieving over 96 % .\\nOn finer-grained sense distinctions , top accuracies from 59.1 % to 69.0 % have been reported in recent evaluation exercises -LRB- SemEval-2007 , Senseval-2 -RRB- , where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4 % and 57 % , respectively .\\nWSD task has two variants : `` lexical sample '' and `` all words '' task .\\nThe former comprises disambiguating the occurrences of a small sample of target words which were previously selected , while in the latter all the words in a piece of running text need to be disambiguated .\\nThe latter is deemed a more realistic form of evaluation , but the corpus is more expensive to produce because human annotators have to read the definitions for each word in the sequence every time they need to make a tagging judgement , rather than once for a block of instances for the same target word .\\nTo give a hint how all this works , consider two examples of the distinct senses that exist for the -LRB- written -RRB- word `` bass '' : a type of fish tones of low frequency and the sentences : I went fishing for some sea bass .\\nThe bass line of the song is too weak .\\nTo a human , it is obvious that the first sentence is using the word `` bass -LRB- fish -RRB- '' , as in the former sense above and in the second sentence , the word `` bass -LRB- instrument -RRB- '' is being used as in the latter sense below .\\nDeveloping algorithms to replicate this human ability can often be a difficult task , as is further exemplified by the implicit equivocation between `` bass -LRB- sound -RRB- '' and `` bass '' -LRB- musical instrument -RRB- .\\nHistory WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics .\\nWarren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context .\\nEarly researchers understood the significance and difficulty of WSD well .\\nIn fact , Bar-Hillel -LRB- 1960 -RRB- used the above example to argue that WSD could not be solved by `` electronic computer '' because of the need in general to model all world knowledge .\\nIn the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule-based and hand-coded they were prone to a knowledge acquisition bottleneck .\\nBy the 1980s large-scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English -LRB- OALD -RRB- , became available : hand-coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge-based or dictionary-based .\\nIn the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques .\\nThe 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser-grained senses , domain adaptation , semi-supervised and unsupervised corpus-based systems , combinations of different methods , and the return of knowledge-based systems via graph-based methods .\\nStill , supervised systems continue to perform best .\\nDifficulties Differences between dictionaries One problem with word sense disambiguation is deciding what the senses are .\\nIn cases like the word bass above , at least some senses are obviously different .\\nIn other cases , however , the different senses can be closely related -LRB- one meaning being a metaphorical or metonymic extension of another -RRB- , and in such cases division of words into senses becomes much more difficult .\\nDifferent dictionaries and thesauruses will provide different divisions of words into senses .\\nOne solution some researchers have used is to choose a particular dictionary , and just use its set of senses .\\nGenerally , however , research results using broad distinctions in senses have been much better than those using narrow ones .\\nHowever , given the lack of a full-fledged coarse-grained sense inventory , most researchers continue to work on fine-grained WSD .\\nMost research in the field of WSD is performed by using WordNet as a reference sense inventory for English .\\nWordNet is a computational lexicon that encodes concepts as synonym sets -LRB- e.g. the concept of car is encoded as -LCB- car , auto , automobile , machine , motorcar -RCB- -RRB- .\\nOther resources used for disambiguation purposes include Roget 's Thesaurus and Wikipedia .\\nPart-of-speech tagging In any real test , part-of-speech tagging and sense tagging are very closely related with each potentially making constraints to the other .\\nAnd the question whether these tasks should be kept together or decoupled is still not unanimously resolved , but recently scientists incline to test these things separately -LRB- e.g. in the Senseval\\\\/SemEval competitions parts of speech are provided as input for the text to disambiguate -RRB- .\\nIt is instructive to compare the word sense disambiguation problem with the problem of part-of-speech tagging .\\nBoth involve disambiguating or tagging with words , be it with senses or parts of speech .\\nHowever , algorithms used for one do not tend to work well for the other , mainly because the part of speech of a word is primarily determined by the immediately adjacent one to three words , whereas the sense of a word may be determined by words further away .\\nThe success rate for part-of-speech tagging algorithms is at present much higher than that for WSD , state-of-the art being around 95 % accuracy or better , as compared to less than 75 % accuracy in word sense disambiguation with supervised learning .\\nThese figures are typical for English , and may be very different from those for other languages .\\nInter-judge variance Another problem is inter-judge variance .\\nWSD systems are normally tested by having their results on a task compared against those of a human .\\nHowever , while it is relatively easy to assign parts of speech to text , training people to tag senses is far more difficult .\\nWhile users can memorize all of the possible parts of speech a word can take , it is often impossible for individuals to memorize all of the senses a word can take .\\nMoreover , humans do not agree on the task at hand -- give a list of senses and sentences , and humans will not always agree on which word belongs in which sense .\\nThus , a computer can not be expected to give better performance on such a task than a human -LRB- indeed , since the human serves as the standard , the computer being better than the human is incoherent -RRB- , -LRB- citation needed -RRB- so the human performance serves as an upper bound .\\nHuman performance , however , is much better on coarse-grained than fine-grained distinctions , so this again is why research on coarse-grained distinctions has been put to test in recent WSD evaluation exercises .\\nCommon sense Some AI researchers like Douglas Lenat argue that one can not parse meanings from words without some form of common sense ontology .\\nFor example , comparing these two sentences : `` Jill and Mary are sisters . ''\\n-- -LRB- they are sisters of each other -RRB- .\\n`` Jill and Mary are mothers . ''\\n-- -LRB- each is independently a mother -RRB- .\\nTo properly identify senses of words one must know common sense facts .\\nMoreover , sometimes the common sense is needed to disambiguate such words like pronouns in case of having anaphoras or cataphoras in the text .\\nSense inventory and algorithms ' task-dependency A task-independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task .\\nFor example , the ambiguity of ` mouse ' -LRB- animal or device -RRB- is not relevant in English-French machine translation , but is relevant in information retrieval .\\nThe opposite is true of ` river ' , which requires a choice in French -LRB- fleuve ` flows into the sea ' , or rivière ` flows into a river ' -RRB- .\\nAlso , completely different algorithms might be required by different applications .\\nIn machine translation , the problem takes the form of target word selection .\\nHere the `` senses '' are words in the target language , which often correspond to significant meaning distinctions in the source language -LRB- bank could translate to French banque ` financial bank ' or rive ` edge of river ' -RRB- .\\nIn information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant .\\nDiscreteness of senses Finally , the very notion of `` word sense '' is slippery and controversial .\\nMost people can agree in distinctions at the coarse-grained homograph level -LRB- e.g. , pen as writing instrument or enclosure -RRB- , but go down one level to fine-grained polysemy , and disagreements arise .\\nFor example , in Senseval-2 , which used fine-grained sense distinctions , human annotators agreed in only 85 % of word occurrences .\\nWord meaning is in principle infinitely variable and context sensitive .\\nIt does not divide up easily into distinct or discrete sub-meanings .\\nLexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways .\\nThe art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well-behaved semantically .\\nHowever , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations .\\nRecently , a task -- named lexical substitution -- has been proposed as a possible solution to the sense discreteness problem .\\nThe task consists of providing a substitute for a word in context that preserves the meaning of the original word -LRB- potentially , substitutes can be chosen from the full lexicon of the target language , thus overcoming discreteness -RRB- .\\nApproaches and methods As in all natural language processing , there are two main approaches to WSD -- deep approaches and shallow approaches .\\nDeep approaches presume access to a comprehensive body of world knowledge .\\nKnowledge , such as `` you can go fishing for a type of fish , but not for low frequency sounds '' and `` songs have low frequency sounds as parts , but not types of fish '' , is then used to determine in which sense the word is used .\\nThese approaches are not very successful in practice , mainly because such a body of knowledge does not exist in a computer-readable format , outside of very limited domains .\\nHowever , if such knowledge did exist , then deep approaches would be much more accurate than the shallow approaches .\\n-LRB- citation needed -RRB- Also , there is a long tradition in computational linguistics , of trying such approaches in terms of coded knowledge and in some cases , it is hard to say clearly whether the knowledge involved is linguistic or world knowledge .\\nThe first attempt was that by Margaret Masterman and her colleagues , at the Cambridge Language Research Unit in England , in the 1950s .\\nThis attempt used as data a punched-card version of Roget 's Thesaurus and its numbered `` heads '' , as an indicator of topics and looked for repetitions in text , using a set intersection algorithm .\\nIt was not very successful , but had strong relationships to later work , especially Yarowsky 's machine learning optimisation of a thesaurus method in the 1990s .\\nShallow approaches do n't try to understand the text .\\nThey just consider the surrounding words , using information such as `` if bass has words sea or fishing nearby , it probably is in the fish sense ; if bass has the words music or song nearby , it is probably in the music sense . ''\\nThese rules can be automatically derived by the computer , using a training corpus of words tagged with their word senses .\\nThis approach , while theoretically not as powerful as deep approaches , gives superior results in practice , due to the computer 's limited world knowledge .\\nHowever , it can be confused by sentences like The dogs bark at the tree which contains the word bark near both tree and dogs .\\nThere are four conventional approaches to WSD : Dictionary - and knowledge-based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence .\\nSupervised methods : These make use of sense-annotated corpora to train from .\\nSemi-supervised or minimally supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word-aligned bilingual corpus .\\nUnsupervised methods : These eschew -LRB- almost -RRB- completely external information and work directly from raw unannotated corpora .\\nThese methods are also known under the name of word sense discrimination .\\nAlmost all these approaches normally work by defining a window of n content words around each word to be disambiguated in the corpus , and statistically analyzing those n surrounding words .\\nTwo shallow approaches used to train and then disambiguate are Naïve Bayes classifiers and decision trees .\\nIn recent research , kernel-based methods such as support vector machines have shown superior performance in supervised learning .\\nGraph-based approaches have also gained much attention from the research community , and currently achieve performance close to the state of the art .\\nDictionary - and knowledge-based methods The Lesk algorithm is the seminal dictionary-based method .\\nIt is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses .\\nTwo -LRB- or more -RRB- words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions .\\nFor example , when disambiguating the words in `` pine cone '' , the definitions of the appropriate senses both include the words evergreen and tree -LRB- at least in one dictionary -RRB- .\\nAn alternative to the use of the definitions is to consider general word-sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge base such as WordNet .\\nGraph-based methods reminiscent of spreading activation research of the early days of AI research have been applied with some success .\\nMore complex graph-based approaches have been shown to perform almost as well as supervised methods or even outperforming them on specific domains .\\nRecently , it has been reported that simple graph connectivity measures , such as degree , perform state-of-the-art WSD in the presence of a sufficiently rich lexical knowledge base .\\nAlso , automatically transferring knowledge in the form of semantic relations from Wikipedia to WordNet has been shown to boost simple knowledge-based methods , enabling them to rival the best supervised systems and even outperform them in a domain-specific setting .\\nThe use of selectional preferences -LRB- or selectional restrictions -RRB- is also useful , for example , knowing that one typically cooks food , one can disambiguate the word bass in `` I am cooking basses '' -LRB- i.e. , it 's not a musical instrument -RRB- .\\nSupervised methods Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words -LRB- hence , world knowledge and reasoning are deemed unnecessary -RRB- .\\nProbably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning .\\nSupport Vector Machines and memory-based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high-dimensionality of the feature space .\\nHowever , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense-tagged corpora for training , which are laborious and expensive to create .\\nSemi-supervised methods Because of the lack of training data , many word sense disambiguation algorithms use semi-supervised learning , which allows both labeled and unlabeled data .\\nThe Yarowsky algorithm was an early example of such an algorithm .\\nIt uses the ` One sense per collocation ' and the ` One sense per discourse ' properties of human languages for word sense disambiguation .\\nFrom observation , words tend to exhibit only one sense in most given discourse and in a given collocation .\\nThe bootstrapping approach starts from a small amount of seed data for each word : either manually tagged training examples or a small number of surefire decision rules -LRB- e.g. , ` play ' in the context of ` bass ' almost always indicates the musical instrument -RRB- .\\nThe seeds are used to train an initial classifier , using any supervised method .\\nThis classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included .\\nThe process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached .\\nOther semi-supervised techniques use large quantities of untagged corpora to provide co-occurrence information that supplements the tagged corpora .\\nThese techniques have the potential to help in the adaptation of supervised models to different domains .\\nAlso , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word .\\nWord-aligned bilingual corpora have been used to infer cross-lingual sense distinctions , a kind of semi-supervised system .\\nUnsupervised methods Main article : Word sense induction Unsupervised learning is the greatest challenge for WSD researchers .\\nThe underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context , a task referred to as word sense induction or discrimination .\\nThen , new occurrences of the word can be classified into the closest induced clusters\\\\/senses .\\nPerformance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses .\\nIf a mapping to a set of dictionary senses is not desired , cluster-based evaluations -LRB- including measures of entropy and purity -RRB- can be performed .\\nAlternatively , word sense induction methods can be tested and compared within an application .\\nFor instance , it has been shown that word sense induction improves Web search result clustering by increasing the quality of result clusters and the degree diversification of result lists .\\nIt is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort .\\nOther approaches Other approaches may vary differently in their methods : Identification of dominant word senses ; Domain-driven disambiguation ; WSD using Cross-Lingual Evidence .\\nLocal impediments and summary The knowledge acquisition bottleneck is perhaps the major impediment to solving the WSD problem .\\nUnsupervised methods rely on knowledge about word senses , which is barely formulated in dictionaries and lexical databases .\\nSupervised methods depend crucially on the existence of manually annotated examples for every word sense , a requisite that can so far be met only for a handful of words for testing purposes , as it is done in the Senseval exercises .\\nTherefore , one of the most promising trends in WSD research is using the largest corpus ever accessible , the World Wide Web , to acquire lexical information automatically .\\nWSD has been traditionally understood as an intermediate language engineering technology which could improve applications such as information retrieval -LRB- IR -RRB- .\\nIn this case , however , the reverse is also true : Web search engines implement simple and robust IR techniques that can be successfully used when mining the Web for information to be employed in WSD .\\nTherefore , the lack of training data provoked appearing some new algorithms and techniques described here : Main article : Automatic Acquisition of Sense-Tagged Corpora External knowledge sources Knowledge is a fundamental component of WSD .\\nKnowledge sources provide data which are essential to associate senses with words .\\nThey can vary from corpora of texts , either unlabeled or annotated with word senses , to machine-readable dictionaries , thesauri , glossaries , ontologies , etc. .\\nThey can be classified as follows : Structured : Thesauri Machine-readable dictionaries -LRB- MRDs -RRB- Ontologies Unstructured : Corpora : raw corpora and sense-annotated corpora Collocation resources Other resources -LRB- such as word frequency lists , stoplists , domain labels , etc. -RRB- Evaluation Comparing and evaluating different WSD systems is extremely difﬁcult , because of the different test sets , sense inventories , and knowledge resources adopted .\\nBefore the organization of speciﬁc evaluation campaigns most systems were assessed on in-house , often small-scale , data sets .\\nIn order to test one 's algorithm , developers should spend their time to annotate all word occurrences .\\nAnd comparing methods even on the same corpus is not eligible if there is different sense inventories .\\nIn order to define common evaluation datasets and procedures , public evaluation campaigns have been organized .\\nSenseval -LRB- now renamed SemEval -RRB- is an international word sense disambiguation competition , held every three years since 1998 : Senseval-1 -LRB- 1998 -RRB- , Senseval-2 -LRB- 2001 -RRB- , Senseval-3 -LRB- 2004 -RRB- , and its successor , SemEval -LRB- 2007 -RRB- .\\nThe objective of the competition is to organize different lectures , preparing and hand-annotating corpus for testing systems , perform a comparative evaluation of WSD systems in several kinds of tasks , including all-words and lexical sample WSD for different languages , and , more recently , new tasks such as semantic role labeling , gloss WSD , lexical substitution , etc. .\\nThe systems submitted for evaluation to these competitions usually integrate different techniques and often combine supervised and knowledge-based methods -LRB- especially for avoiding bad performance in lack of training examples -RRB- .\\nTask Design Choices Sense Inventories .\\nDuring the first Senseval workshop the HECTOR sense inventory was adopted .\\nThe reason for adopting a previously unknown sense inventory was mainly to avoid the use of popular fine-grained word senses -LRB- such as WordNet -RRB- , which could make the experiments unfair or biased .\\nHowever , given the lack of coverage of such inventories , since the second Senseval workshop the WordNet sense inventory has been adopted .\\nA set of testing words .\\nComparison of methods can be divided in 2 groups by amount of words to test .\\nThe difference consists in the amount of analysis and processing : all-words task implies disambiguating all the words of the text lexical sample consists in disambiguating some previously chosen target words .\\nIt is assumed that the former one is more realistic evaluation , although with very laborious testing of results .\\nInitially only the latter was used in evaluation but later the former was included .\\nLexical sample organizers had to choose samples on which the systems were to be tested .\\nA criticism of earlier forays into lexical-sample WSD evaluation is that the lexical sample had been chosen according to the whim of the experimenter -LRB- or , to coincide with earlier experimenters ' selections -RRB- .\\nFor English Senseval , a sampling frame was devised in which words were classified according to their frequency -LRB- in the BNC -RRB- and their polysemy level -LRB- in WordNet -RRB- .\\nAlso , inclusion POS-tagging problem was a matter of discussion and it was decided that samples should be words with known part of speech and some indeterminants -LRB- for ex .\\n15 noun tasks , 13 verb tasks , 8 adjectives , and 5 indeterminates -RRB- .\\nBaselines .\\nFor comparison purposes , known , yet simple , algorithms named baselines are used .\\nThese include different variants of Lesk algorithm or most frequent sense algorithm .\\nSense inventory .\\nWSD exercises require a dictionary , to specify the word senses which are to be disambiguated , and a corpus of language data to be disambiguated .\\nWordNet is the most popular example of sense inventory .\\nThe reason for adopting the HECTOR database during Senseval-1 was that the WordNet inventory was already publicly available .\\nEvaluation measures .\\nDuring the evaluation of WSD systems two main performance measures are used : Precision : the fraction of system assignments made that are correct Recall : the fraction of total word instances correctly assigned by a system If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy .\\nThis model has been extended to take into account systems that return a set of senses with weights for each occurrence .\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBLNZyH5QpC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove all the punctuation (might not use)\n",
        "# This function is not working, but the idea of dealing with text is this!\n",
        "\n",
        "import regex as re\n",
        "def remove_punctuation(words):\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    new_word = re.sub(r'[\"\\w\\s]','', word)\n",
        "    if new_word != '':\n",
        "      new_words.append(new_word)\n",
        "\n",
        "  return new_words\n",
        "\n",
        "remove_punctuation(sample_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uunm79X4QpHI",
        "colab_type": "code",
        "outputId": "128be10c-7599-4ee3-9de7-9f3e0990e93f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Tokenization: seperate the words into a list\n",
        "\n",
        "sample_text = sample_text.lower()\n",
        "tokens = nltk.word_tokenize(sample_text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['in', 'computational', 'linguistics', ',', 'word-sense', 'disambiguation', '-lrb-', 'wsd', '-rrb-', 'is', 'an', 'open', 'problem', 'of', 'natural', 'language', 'processing', ',', 'which', 'governs', 'the', 'process', 'of', 'identifying', 'which', 'sense', 'of', 'a', 'word', '-lrb-', 'i.e', '.', 'meaning', '-rrb-', 'is', 'used', 'in', 'a', 'sentence', ',', 'when', 'the', 'word', 'has', 'multiple', 'meanings', '-lrb-', 'polysemy', '-rrb-', '.', 'the', 'solution', 'to', 'this', 'problem', 'impacts', 'other', 'computer-related', 'writing', ',', 'such', 'as', 'discourse', ',', 'improving', 'relevance', 'of', 'search', 'engines', ',', 'anaphora', 'resolution', ',', 'coherence', ',', 'inference', 'et', 'cetera', '.', 'research', 'has', 'progressed', 'steadily', 'to', 'the', 'point', 'where', 'wsd', 'systems', 'achieve', 'sufficiently', 'high', 'levels', 'of', 'accuracy', 'on', 'a', 'variety', 'of', 'word', 'types', 'and', 'ambiguities', '.', 'a', 'rich', 'variety', 'of', 'techniques', 'have', 'been', 'researched', ',', 'from', 'dictionary-based', 'methods', 'that', 'use', 'the', 'knowledge', 'encoded', 'in', 'lexical', 'resources', ',', 'to', 'supervised', 'machine', 'learning', 'methods', 'in', 'which', 'a', 'classifier', 'is', 'trained', 'for', 'each', 'distinct', 'word', 'on', 'a', 'corpus', 'of', 'manually', 'sense-annotated', 'examples', ',', 'to', 'completely', 'unsupervised', 'methods', 'that', 'cluster', 'occurrences', 'of', 'words', ',', 'thereby', 'inducing', 'word', 'senses', '.', 'among', 'these', ',', 'supervised', 'learning', 'approaches', 'have', 'been', 'the', 'most', 'successful', 'algorithms', 'to', 'date', '.', 'current', 'accuracy', 'is', 'difficult', 'to', 'state', 'without', 'a', 'host', 'of', 'caveats', '.', 'in', 'english', ',', 'accuracy', 'at', 'the', 'coarse-grained', '-lrb-', 'homograph', '-rrb-', 'level', 'is', 'routinely', 'above', '90', '%', ',', 'with', 'some', 'methods', 'on', 'particular', 'homographs', 'achieving', 'over', '96', '%', '.', 'on', 'finer-grained', 'sense', 'distinctions', ',', 'top', 'accuracies', 'from', '59.1', '%', 'to', '69.0', '%', 'have', 'been', 'reported', 'in', 'recent', 'evaluation', 'exercises', '-lrb-', 'semeval-2007', ',', 'senseval-2', '-rrb-', ',', 'where', 'the', 'baseline', 'accuracy', 'of', 'the', 'simplest', 'possible', 'algorithm', 'of', 'always', 'choosing', 'the', 'most', 'frequent', 'sense', 'was', '51.4', '%', 'and', '57', '%', ',', 'respectively', '.', 'wsd', 'task', 'has', 'two', 'variants', ':', '``', 'lexical', 'sample', \"''\", 'and', '``', 'all', 'words', \"''\", 'task', '.', 'the', 'former', 'comprises', 'disambiguating', 'the', 'occurrences', 'of', 'a', 'small', 'sample', 'of', 'target', 'words', 'which', 'were', 'previously', 'selected', ',', 'while', 'in', 'the', 'latter', 'all', 'the', 'words', 'in', 'a', 'piece', 'of', 'running', 'text', 'need', 'to', 'be', 'disambiguated', '.', 'the', 'latter', 'is', 'deemed', 'a', 'more', 'realistic', 'form', 'of', 'evaluation', ',', 'but', 'the', 'corpus', 'is', 'more', 'expensive', 'to', 'produce', 'because', 'human', 'annotators', 'have', 'to', 'read', 'the', 'definitions', 'for', 'each', 'word', 'in', 'the', 'sequence', 'every', 'time', 'they', 'need', 'to', 'make', 'a', 'tagging', 'judgement', ',', 'rather', 'than', 'once', 'for', 'a', 'block', 'of', 'instances', 'for', 'the', 'same', 'target', 'word', '.', 'to', 'give', 'a', 'hint', 'how', 'all', 'this', 'works', ',', 'consider', 'two', 'examples', 'of', 'the', 'distinct', 'senses', 'that', 'exist', 'for', 'the', '-lrb-', 'written', '-rrb-', 'word', '``', 'bass', \"''\", ':', 'a', 'type', 'of', 'fish', 'tones', 'of', 'low', 'frequency', 'and', 'the', 'sentences', ':', 'i', 'went', 'fishing', 'for', 'some', 'sea', 'bass', '.', 'the', 'bass', 'line', 'of', 'the', 'song', 'is', 'too', 'weak', '.', 'to', 'a', 'human', ',', 'it', 'is', 'obvious', 'that', 'the', 'first', 'sentence', 'is', 'using', 'the', 'word', '``', 'bass', '-lrb-', 'fish', '-rrb-', \"''\", ',', 'as', 'in', 'the', 'former', 'sense', 'above', 'and', 'in', 'the', 'second', 'sentence', ',', 'the', 'word', '``', 'bass', '-lrb-', 'instrument', '-rrb-', \"''\", 'is', 'being', 'used', 'as', 'in', 'the', 'latter', 'sense', 'below', '.', 'developing', 'algorithms', 'to', 'replicate', 'this', 'human', 'ability', 'can', 'often', 'be', 'a', 'difficult', 'task', ',', 'as', 'is', 'further', 'exemplified', 'by', 'the', 'implicit', 'equivocation', 'between', '``', 'bass', '-lrb-', 'sound', '-rrb-', \"''\", 'and', '``', 'bass', \"''\", '-lrb-', 'musical', 'instrument', '-rrb-', '.', 'history', 'wsd', 'was', 'first', 'formulated', 'as', 'a', 'distinct', 'computational', 'task', 'during', 'the', 'early', 'days', 'of', 'machine', 'translation', 'in', 'the', '1940s', ',', 'making', 'it', 'one', 'of', 'the', 'oldest', 'problems', 'in', 'computational', 'linguistics', '.', 'warren', 'weaver', ',', 'in', 'his', 'famous', '1949', 'memorandum', 'on', 'translation', ',', 'first', 'introduced', 'the', 'problem', 'in', 'a', 'computational', 'context', '.', 'early', 'researchers', 'understood', 'the', 'significance', 'and', 'difficulty', 'of', 'wsd', 'well', '.', 'in', 'fact', ',', 'bar-hillel', '-lrb-', '1960', '-rrb-', 'used', 'the', 'above', 'example', 'to', 'argue', 'that', 'wsd', 'could', 'not', 'be', 'solved', 'by', '``', 'electronic', 'computer', \"''\", 'because', 'of', 'the', 'need', 'in', 'general', 'to', 'model', 'all', 'world', 'knowledge', '.', 'in', 'the', '1970s', ',', 'wsd', 'was', 'a', 'subtask', 'of', 'semantic', 'interpretation', 'systems', 'developed', 'within', 'the', 'field', 'of', 'artificial', 'intelligence', ',', 'but', 'since', 'wsd', 'systems', 'were', 'largely', 'rule-based', 'and', 'hand-coded', 'they', 'were', 'prone', 'to', 'a', 'knowledge', 'acquisition', 'bottleneck', '.', 'by', 'the', '1980s', 'large-scale', 'lexical', 'resources', ',', 'such', 'as', 'the', 'oxford', 'advanced', 'learner', \"'s\", 'dictionary', 'of', 'current', 'english', '-lrb-', 'oald', '-rrb-', ',', 'became', 'available', ':', 'hand-coding', 'was', 'replaced', 'with', 'knowledge', 'automatically', 'extracted', 'from', 'these', 'resources', ',', 'but', 'disambiguation', 'was', 'still', 'knowledge-based', 'or', 'dictionary-based', '.', 'in', 'the', '1990s', ',', 'the', 'statistical', 'revolution', 'swept', 'through', 'computational', 'linguistics', ',', 'and', 'wsd', 'became', 'a', 'paradigm', 'problem', 'on', 'which', 'to', 'apply', 'supervised', 'machine', 'learning', 'techniques', '.', 'the', '2000s', 'saw', 'supervised', 'techniques', 'reach', 'a', 'plateau', 'in', 'accuracy', ',', 'and', 'so', 'attention', 'has', 'shifted', 'to', 'coarser-grained', 'senses', ',', 'domain', 'adaptation', ',', 'semi-supervised', 'and', 'unsupervised', 'corpus-based', 'systems', ',', 'combinations', 'of', 'different', 'methods', ',', 'and', 'the', 'return', 'of', 'knowledge-based', 'systems', 'via', 'graph-based', 'methods', '.', 'still', ',', 'supervised', 'systems', 'continue', 'to', 'perform', 'best', '.', 'difficulties', 'differences', 'between', 'dictionaries', 'one', 'problem', 'with', 'word', 'sense', 'disambiguation', 'is', 'deciding', 'what', 'the', 'senses', 'are', '.', 'in', 'cases', 'like', 'the', 'word', 'bass', 'above', ',', 'at', 'least', 'some', 'senses', 'are', 'obviously', 'different', '.', 'in', 'other', 'cases', ',', 'however', ',', 'the', 'different', 'senses', 'can', 'be', 'closely', 'related', '-lrb-', 'one', 'meaning', 'being', 'a', 'metaphorical', 'or', 'metonymic', 'extension', 'of', 'another', '-rrb-', ',', 'and', 'in', 'such', 'cases', 'division', 'of', 'words', 'into', 'senses', 'becomes', 'much', 'more', 'difficult', '.', 'different', 'dictionaries', 'and', 'thesauruses', 'will', 'provide', 'different', 'divisions', 'of', 'words', 'into', 'senses', '.', 'one', 'solution', 'some', 'researchers', 'have', 'used', 'is', 'to', 'choose', 'a', 'particular', 'dictionary', ',', 'and', 'just', 'use', 'its', 'set', 'of', 'senses', '.', 'generally', ',', 'however', ',', 'research', 'results', 'using', 'broad', 'distinctions', 'in', 'senses', 'have', 'been', 'much', 'better', 'than', 'those', 'using', 'narrow', 'ones', '.', 'however', ',', 'given', 'the', 'lack', 'of', 'a', 'full-fledged', 'coarse-grained', 'sense', 'inventory', ',', 'most', 'researchers', 'continue', 'to', 'work', 'on', 'fine-grained', 'wsd', '.', 'most', 'research', 'in', 'the', 'field', 'of', 'wsd', 'is', 'performed', 'by', 'using', 'wordnet', 'as', 'a', 'reference', 'sense', 'inventory', 'for', 'english', '.', 'wordnet', 'is', 'a', 'computational', 'lexicon', 'that', 'encodes', 'concepts', 'as', 'synonym', 'sets', '-lrb-', 'e.g', '.', 'the', 'concept', 'of', 'car', 'is', 'encoded', 'as', '-lcb-', 'car', ',', 'auto', ',', 'automobile', ',', 'machine', ',', 'motorcar', '-rcb-', '-rrb-', '.', 'other', 'resources', 'used', 'for', 'disambiguation', 'purposes', 'include', 'roget', \"'s\", 'thesaurus', 'and', 'wikipedia', '.', 'part-of-speech', 'tagging', 'in', 'any', 'real', 'test', ',', 'part-of-speech', 'tagging', 'and', 'sense', 'tagging', 'are', 'very', 'closely', 'related', 'with', 'each', 'potentially', 'making', 'constraints', 'to', 'the', 'other', '.', 'and', 'the', 'question', 'whether', 'these', 'tasks', 'should', 'be', 'kept', 'together', 'or', 'decoupled', 'is', 'still', 'not', 'unanimously', 'resolved', ',', 'but', 'recently', 'scientists', 'incline', 'to', 'test', 'these', 'things', 'separately', '-lrb-', 'e.g', '.', 'in', 'the', 'senseval\\\\/semeval', 'competitions', 'parts', 'of', 'speech', 'are', 'provided', 'as', 'input', 'for', 'the', 'text', 'to', 'disambiguate', '-rrb-', '.', 'it', 'is', 'instructive', 'to', 'compare', 'the', 'word', 'sense', 'disambiguation', 'problem', 'with', 'the', 'problem', 'of', 'part-of-speech', 'tagging', '.', 'both', 'involve', 'disambiguating', 'or', 'tagging', 'with', 'words', ',', 'be', 'it', 'with', 'senses', 'or', 'parts', 'of', 'speech', '.', 'however', ',', 'algorithms', 'used', 'for', 'one', 'do', 'not', 'tend', 'to', 'work', 'well', 'for', 'the', 'other', ',', 'mainly', 'because', 'the', 'part', 'of', 'speech', 'of', 'a', 'word', 'is', 'primarily', 'determined', 'by', 'the', 'immediately', 'adjacent', 'one', 'to', 'three', 'words', ',', 'whereas', 'the', 'sense', 'of', 'a', 'word', 'may', 'be', 'determined', 'by', 'words', 'further', 'away', '.', 'the', 'success', 'rate', 'for', 'part-of-speech', 'tagging', 'algorithms', 'is', 'at', 'present', 'much', 'higher', 'than', 'that', 'for', 'wsd', ',', 'state-of-the', 'art', 'being', 'around', '95', '%', 'accuracy', 'or', 'better', ',', 'as', 'compared', 'to', 'less', 'than', '75', '%', 'accuracy', 'in', 'word', 'sense', 'disambiguation', 'with', 'supervised', 'learning', '.', 'these', 'figures', 'are', 'typical', 'for', 'english', ',', 'and', 'may', 'be', 'very', 'different', 'from', 'those', 'for', 'other', 'languages', '.', 'inter-judge', 'variance', 'another', 'problem', 'is', 'inter-judge', 'variance', '.', 'wsd', 'systems', 'are', 'normally', 'tested', 'by', 'having', 'their', 'results', 'on', 'a', 'task', 'compared', 'against', 'those', 'of', 'a', 'human', '.', 'however', ',', 'while', 'it', 'is', 'relatively', 'easy', 'to', 'assign', 'parts', 'of', 'speech', 'to', 'text', ',', 'training', 'people', 'to', 'tag', 'senses', 'is', 'far', 'more', 'difficult', '.', 'while', 'users', 'can', 'memorize', 'all', 'of', 'the', 'possible', 'parts', 'of', 'speech', 'a', 'word', 'can', 'take', ',', 'it', 'is', 'often', 'impossible', 'for', 'individuals', 'to', 'memorize', 'all', 'of', 'the', 'senses', 'a', 'word', 'can', 'take', '.', 'moreover', ',', 'humans', 'do', 'not', 'agree', 'on', 'the', 'task', 'at', 'hand', '--', 'give', 'a', 'list', 'of', 'senses', 'and', 'sentences', ',', 'and', 'humans', 'will', 'not', 'always', 'agree', 'on', 'which', 'word', 'belongs', 'in', 'which', 'sense', '.', 'thus', ',', 'a', 'computer', 'can', 'not', 'be', 'expected', 'to', 'give', 'better', 'performance', 'on', 'such', 'a', 'task', 'than', 'a', 'human', '-lrb-', 'indeed', ',', 'since', 'the', 'human', 'serves', 'as', 'the', 'standard', ',', 'the', 'computer', 'being', 'better', 'than', 'the', 'human', 'is', 'incoherent', '-rrb-', ',', '-lrb-', 'citation', 'needed', '-rrb-', 'so', 'the', 'human', 'performance', 'serves', 'as', 'an', 'upper', 'bound', '.', 'human', 'performance', ',', 'however', ',', 'is', 'much', 'better', 'on', 'coarse-grained', 'than', 'fine-grained', 'distinctions', ',', 'so', 'this', 'again', 'is', 'why', 'research', 'on', 'coarse-grained', 'distinctions', 'has', 'been', 'put', 'to', 'test', 'in', 'recent', 'wsd', 'evaluation', 'exercises', '.', 'common', 'sense', 'some', 'ai', 'researchers', 'like', 'douglas', 'lenat', 'argue', 'that', 'one', 'can', 'not', 'parse', 'meanings', 'from', 'words', 'without', 'some', 'form', 'of', 'common', 'sense', 'ontology', '.', 'for', 'example', ',', 'comparing', 'these', 'two', 'sentences', ':', '``', 'jill', 'and', 'mary', 'are', 'sisters', '.', \"''\", '--', '-lrb-', 'they', 'are', 'sisters', 'of', 'each', 'other', '-rrb-', '.', '``', 'jill', 'and', 'mary', 'are', 'mothers', '.', \"''\", '--', '-lrb-', 'each', 'is', 'independently', 'a', 'mother', '-rrb-', '.', 'to', 'properly', 'identify', 'senses', 'of', 'words', 'one', 'must', 'know', 'common', 'sense', 'facts', '.', 'moreover', ',', 'sometimes', 'the', 'common', 'sense', 'is', 'needed', 'to', 'disambiguate', 'such', 'words', 'like', 'pronouns', 'in', 'case', 'of', 'having', 'anaphoras', 'or', 'cataphoras', 'in', 'the', 'text', '.', 'sense', 'inventory', 'and', 'algorithms', \"'\", 'task-dependency', 'a', 'task-independent', 'sense', 'inventory', 'is', 'not', 'a', 'coherent', 'concept', ':', 'each', 'task', 'requires', 'its', 'own', 'division', 'of', 'word', 'meaning', 'into', 'senses', 'relevant', 'to', 'the', 'task', '.', 'for', 'example', ',', 'the', 'ambiguity', 'of', '`', 'mouse', \"'\", '-lrb-', 'animal', 'or', 'device', '-rrb-', 'is', 'not', 'relevant', 'in', 'english-french', 'machine', 'translation', ',', 'but', 'is', 'relevant', 'in', 'information', 'retrieval', '.', 'the', 'opposite', 'is', 'true', 'of', '`', 'river', \"'\", ',', 'which', 'requires', 'a', 'choice', 'in', 'french', '-lrb-', 'fleuve', '`', 'flows', 'into', 'the', 'sea', \"'\", ',', 'or', 'rivière', '`', 'flows', 'into', 'a', 'river', \"'\", '-rrb-', '.', 'also', ',', 'completely', 'different', 'algorithms', 'might', 'be', 'required', 'by', 'different', 'applications', '.', 'in', 'machine', 'translation', ',', 'the', 'problem', 'takes', 'the', 'form', 'of', 'target', 'word', 'selection', '.', 'here', 'the', '``', 'senses', \"''\", 'are', 'words', 'in', 'the', 'target', 'language', ',', 'which', 'often', 'correspond', 'to', 'significant', 'meaning', 'distinctions', 'in', 'the', 'source', 'language', '-lrb-', 'bank', 'could', 'translate', 'to', 'french', 'banque', '`', 'financial', 'bank', \"'\", 'or', 'rive', '`', 'edge', 'of', 'river', \"'\", '-rrb-', '.', 'in', 'information', 'retrieval', ',', 'a', 'sense', 'inventory', 'is', 'not', 'necessarily', 'required', ',', 'because', 'it', 'is', 'enough', 'to', 'know', 'that', 'a', 'word', 'is', 'used', 'in', 'the', 'same', 'sense', 'in', 'the', 'query', 'and', 'a', 'retrieved', 'document', ';', 'what', 'sense', 'that', 'is', ',', 'is', 'unimportant', '.', 'discreteness', 'of', 'senses', 'finally', ',', 'the', 'very', 'notion', 'of', '``', 'word', 'sense', \"''\", 'is', 'slippery', 'and', 'controversial', '.', 'most', 'people', 'can', 'agree', 'in', 'distinctions', 'at', 'the', 'coarse-grained', 'homograph', 'level', '-lrb-', 'e.g', '.', ',', 'pen', 'as', 'writing', 'instrument', 'or', 'enclosure', '-rrb-', ',', 'but', 'go', 'down', 'one', 'level', 'to', 'fine-grained', 'polysemy', ',', 'and', 'disagreements', 'arise', '.', 'for', 'example', ',', 'in', 'senseval-2', ',', 'which', 'used', 'fine-grained', 'sense', 'distinctions', ',', 'human', 'annotators', 'agreed', 'in', 'only', '85', '%', 'of', 'word', 'occurrences', '.', 'word', 'meaning', 'is', 'in', 'principle', 'infinitely', 'variable', 'and', 'context', 'sensitive', '.', 'it', 'does', 'not', 'divide', 'up', 'easily', 'into', 'distinct', 'or', 'discrete', 'sub-meanings', '.', 'lexicographers', 'frequently', 'discover', 'in', 'corpora', 'loose', 'and', 'overlapping', 'word', 'meanings', ',', 'and', 'standard', 'or', 'conventional', 'meanings', 'extended', ',', 'modulated', ',', 'and', 'exploited', 'in', 'a', 'bewildering', 'variety', 'of', 'ways', '.', 'the', 'art', 'of', 'lexicography', 'is', 'to', 'generalize', 'from', 'the', 'corpus', 'to', 'definitions', 'that', 'evoke', 'and', 'explain', 'the', 'full', 'range', 'of', 'meaning', 'of', 'a', 'word', ',', 'making', 'it', 'seem', 'like', 'words', 'are', 'well-behaved', 'semantically', '.', 'however', ',', 'it', 'is', 'not', 'at', 'all', 'clear', 'if', 'these', 'same', 'meaning', 'distinctions', 'are', 'applicable', 'in', 'computational', 'applications', ',', 'as', 'the', 'decisions', 'of', 'lexicographers', 'are', 'usually', 'driven', 'by', 'other', 'considerations', '.', 'recently', ',', 'a', 'task', '--', 'named', 'lexical', 'substitution', '--', 'has', 'been', 'proposed', 'as', 'a', 'possible', 'solution', 'to', 'the', 'sense', 'discreteness', 'problem', '.', 'the', 'task', 'consists', 'of', 'providing', 'a', 'substitute', 'for', 'a', 'word', 'in', 'context', 'that', 'preserves', 'the', 'meaning', 'of', 'the', 'original', 'word', '-lrb-', 'potentially', ',', 'substitutes', 'can', 'be', 'chosen', 'from', 'the', 'full', 'lexicon', 'of', 'the', 'target', 'language', ',', 'thus', 'overcoming', 'discreteness', '-rrb-', '.', 'approaches', 'and', 'methods', 'as', 'in', 'all', 'natural', 'language', 'processing', ',', 'there', 'are', 'two', 'main', 'approaches', 'to', 'wsd', '--', 'deep', 'approaches', 'and', 'shallow', 'approaches', '.', 'deep', 'approaches', 'presume', 'access', 'to', 'a', 'comprehensive', 'body', 'of', 'world', 'knowledge', '.', 'knowledge', ',', 'such', 'as', '``', 'you', 'can', 'go', 'fishing', 'for', 'a', 'type', 'of', 'fish', ',', 'but', 'not', 'for', 'low', 'frequency', 'sounds', \"''\", 'and', '``', 'songs', 'have', 'low', 'frequency', 'sounds', 'as', 'parts', ',', 'but', 'not', 'types', 'of', 'fish', \"''\", ',', 'is', 'then', 'used', 'to', 'determine', 'in', 'which', 'sense', 'the', 'word', 'is', 'used', '.', 'these', 'approaches', 'are', 'not', 'very', 'successful', 'in', 'practice', ',', 'mainly', 'because', 'such', 'a', 'body', 'of', 'knowledge', 'does', 'not', 'exist', 'in', 'a', 'computer-readable', 'format', ',', 'outside', 'of', 'very', 'limited', 'domains', '.', 'however', ',', 'if', 'such', 'knowledge', 'did', 'exist', ',', 'then', 'deep', 'approaches', 'would', 'be', 'much', 'more', 'accurate', 'than', 'the', 'shallow', 'approaches', '.', '-lrb-', 'citation', 'needed', '-rrb-', 'also', ',', 'there', 'is', 'a', 'long', 'tradition', 'in', 'computational', 'linguistics', ',', 'of', 'trying', 'such', 'approaches', 'in', 'terms', 'of', 'coded', 'knowledge', 'and', 'in', 'some', 'cases', ',', 'it', 'is', 'hard', 'to', 'say', 'clearly', 'whether', 'the', 'knowledge', 'involved', 'is', 'linguistic', 'or', 'world', 'knowledge', '.', 'the', 'first', 'attempt', 'was', 'that', 'by', 'margaret', 'masterman', 'and', 'her', 'colleagues', ',', 'at', 'the', 'cambridge', 'language', 'research', 'unit', 'in', 'england', ',', 'in', 'the', '1950s', '.', 'this', 'attempt', 'used', 'as', 'data', 'a', 'punched-card', 'version', 'of', 'roget', \"'s\", 'thesaurus', 'and', 'its', 'numbered', '``', 'heads', \"''\", ',', 'as', 'an', 'indicator', 'of', 'topics', 'and', 'looked', 'for', 'repetitions', 'in', 'text', ',', 'using', 'a', 'set', 'intersection', 'algorithm', '.', 'it', 'was', 'not', 'very', 'successful', ',', 'but', 'had', 'strong', 'relationships', 'to', 'later', 'work', ',', 'especially', 'yarowsky', \"'s\", 'machine', 'learning', 'optimisation', 'of', 'a', 'thesaurus', 'method', 'in', 'the', '1990s', '.', 'shallow', 'approaches', 'do', \"n't\", 'try', 'to', 'understand', 'the', 'text', '.', 'they', 'just', 'consider', 'the', 'surrounding', 'words', ',', 'using', 'information', 'such', 'as', '``', 'if', 'bass', 'has', 'words', 'sea', 'or', 'fishing', 'nearby', ',', 'it', 'probably', 'is', 'in', 'the', 'fish', 'sense', ';', 'if', 'bass', 'has', 'the', 'words', 'music', 'or', 'song', 'nearby', ',', 'it', 'is', 'probably', 'in', 'the', 'music', 'sense', '.', \"''\", 'these', 'rules', 'can', 'be', 'automatically', 'derived', 'by', 'the', 'computer', ',', 'using', 'a', 'training', 'corpus', 'of', 'words', 'tagged', 'with', 'their', 'word', 'senses', '.', 'this', 'approach', ',', 'while', 'theoretically', 'not', 'as', 'powerful', 'as', 'deep', 'approaches', ',', 'gives', 'superior', 'results', 'in', 'practice', ',', 'due', 'to', 'the', 'computer', \"'s\", 'limited', 'world', 'knowledge', '.', 'however', ',', 'it', 'can', 'be', 'confused', 'by', 'sentences', 'like', 'the', 'dogs', 'bark', 'at', 'the', 'tree', 'which', 'contains', 'the', 'word', 'bark', 'near', 'both', 'tree', 'and', 'dogs', '.', 'there', 'are', 'four', 'conventional', 'approaches', 'to', 'wsd', ':', 'dictionary', '-', 'and', 'knowledge-based', 'methods', ':', 'these', 'rely', 'primarily', 'on', 'dictionaries', ',', 'thesauri', ',', 'and', 'lexical', 'knowledge', 'bases', ',', 'without', 'using', 'any', 'corpus', 'evidence', '.', 'supervised', 'methods', ':', 'these', 'make', 'use', 'of', 'sense-annotated', 'corpora', 'to', 'train', 'from', '.', 'semi-supervised', 'or', 'minimally', 'supervised', 'methods', ':', 'these', 'make', 'use', 'of', 'a', 'secondary', 'source', 'of', 'knowledge', 'such', 'as', 'a', 'small', 'annotated', 'corpus', 'as', 'seed', 'data', 'in', 'a', 'bootstrapping', 'process', ',', 'or', 'a', 'word-aligned', 'bilingual', 'corpus', '.', 'unsupervised', 'methods', ':', 'these', 'eschew', '-lrb-', 'almost', '-rrb-', 'completely', 'external', 'information', 'and', 'work', 'directly', 'from', 'raw', 'unannotated', 'corpora', '.', 'these', 'methods', 'are', 'also', 'known', 'under', 'the', 'name', 'of', 'word', 'sense', 'discrimination', '.', 'almost', 'all', 'these', 'approaches', 'normally', 'work', 'by', 'defining', 'a', 'window', 'of', 'n', 'content', 'words', 'around', 'each', 'word', 'to', 'be', 'disambiguated', 'in', 'the', 'corpus', ',', 'and', 'statistically', 'analyzing', 'those', 'n', 'surrounding', 'words', '.', 'two', 'shallow', 'approaches', 'used', 'to', 'train', 'and', 'then', 'disambiguate', 'are', 'naïve', 'bayes', 'classifiers', 'and', 'decision', 'trees', '.', 'in', 'recent', 'research', ',', 'kernel-based', 'methods', 'such', 'as', 'support', 'vector', 'machines', 'have', 'shown', 'superior', 'performance', 'in', 'supervised', 'learning', '.', 'graph-based', 'approaches', 'have', 'also', 'gained', 'much', 'attention', 'from', 'the', 'research', 'community', ',', 'and', 'currently', 'achieve', 'performance', 'close', 'to', 'the', 'state', 'of', 'the', 'art', '.', 'dictionary', '-', 'and', 'knowledge-based', 'methods', 'the', 'lesk', 'algorithm', 'is', 'the', 'seminal', 'dictionary-based', 'method', '.', 'it', 'is', 'based', 'on', 'the', 'hypothesis', 'that', 'words', 'used', 'together', 'in', 'text', 'are', 'related', 'to', 'each', 'other', 'and', 'that', 'the', 'relation', 'can', 'be', 'observed', 'in', 'the', 'definitions', 'of', 'the', 'words', 'and', 'their', 'senses', '.', 'two', '-lrb-', 'or', 'more', '-rrb-', 'words', 'are', 'disambiguated', 'by', 'finding', 'the', 'pair', 'of', 'dictionary', 'senses', 'with', 'the', 'greatest', 'word', 'overlap', 'in', 'their', 'dictionary', 'definitions', '.', 'for', 'example', ',', 'when', 'disambiguating', 'the', 'words', 'in', '``', 'pine', 'cone', \"''\", ',', 'the', 'definitions', 'of', 'the', 'appropriate', 'senses', 'both', 'include', 'the', 'words', 'evergreen', 'and', 'tree', '-lrb-', 'at', 'least', 'in', 'one', 'dictionary', '-rrb-', '.', 'an', 'alternative', 'to', 'the', 'use', 'of', 'the', 'definitions', 'is', 'to', 'consider', 'general', 'word-sense', 'relatedness', 'and', 'to', 'compute', 'the', 'semantic', 'similarity', 'of', 'each', 'pair', 'of', 'word', 'senses', 'based', 'on', 'a', 'given', 'lexical', 'knowledge', 'base', 'such', 'as', 'wordnet', '.', 'graph-based', 'methods', 'reminiscent', 'of', 'spreading', 'activation', 'research', 'of', 'the', 'early', 'days', 'of', 'ai', 'research', 'have', 'been', 'applied', 'with', 'some', 'success', '.', 'more', 'complex', 'graph-based', 'approaches', 'have', 'been', 'shown', 'to', 'perform', 'almost', 'as', 'well', 'as', 'supervised', 'methods', 'or', 'even', 'outperforming', 'them', 'on', 'specific', 'domains', '.', 'recently', ',', 'it', 'has', 'been', 'reported', 'that', 'simple', 'graph', 'connectivity', 'measures', ',', 'such', 'as', 'degree', ',', 'perform', 'state-of-the-art', 'wsd', 'in', 'the', 'presence', 'of', 'a', 'sufficiently', 'rich', 'lexical', 'knowledge', 'base', '.', 'also', ',', 'automatically', 'transferring', 'knowledge', 'in', 'the', 'form', 'of', 'semantic', 'relations', 'from', 'wikipedia', 'to', 'wordnet', 'has', 'been', 'shown', 'to', 'boost', 'simple', 'knowledge-based', 'methods', ',', 'enabling', 'them', 'to', 'rival', 'the', 'best', 'supervised', 'systems', 'and', 'even', 'outperform', 'them', 'in', 'a', 'domain-specific', 'setting', '.', 'the', 'use', 'of', 'selectional', 'preferences', '-lrb-', 'or', 'selectional', 'restrictions', '-rrb-', 'is', 'also', 'useful', ',', 'for', 'example', ',', 'knowing', 'that', 'one', 'typically', 'cooks', 'food', ',', 'one', 'can', 'disambiguate', 'the', 'word', 'bass', 'in', '``', 'i', 'am', 'cooking', 'basses', \"''\", '-lrb-', 'i.e', '.', ',', 'it', \"'s\", 'not', 'a', 'musical', 'instrument', '-rrb-', '.', 'supervised', 'methods', 'supervised', 'methods', 'are', 'based', 'on', 'the', 'assumption', 'that', 'the', 'context', 'can', 'provide', 'enough', 'evidence', 'on', 'its', 'own', 'to', 'disambiguate', 'words', '-lrb-', 'hence', ',', 'world', 'knowledge', 'and', 'reasoning', 'are', 'deemed', 'unnecessary', '-rrb-', '.', 'probably', 'every', 'machine', 'learning', 'algorithm', 'going', 'has', 'been', 'applied', 'to', 'wsd', ',', 'including', 'associated', 'techniques', 'such', 'as', 'feature', 'selection', ',', 'parameter', 'optimization', ',', 'and', 'ensemble', 'learning', '.', 'support', 'vector', 'machines', 'and', 'memory-based', 'learning', 'have', 'been', 'shown', 'to', 'be', 'the', 'most', 'successful', 'approaches', ',', 'to', 'date', ',', 'probably', 'because', 'they', 'can', 'cope', 'with', 'the', 'high-dimensionality', 'of', 'the', 'feature', 'space', '.', 'however', ',', 'these', 'supervised', 'methods', 'are', 'subject', 'to', 'a', 'new', 'knowledge', 'acquisition', 'bottleneck', 'since', 'they', 'rely', 'on', 'substantial', 'amounts', 'of', 'manually', 'sense-tagged', 'corpora', 'for', 'training', ',', 'which', 'are', 'laborious', 'and', 'expensive', 'to', 'create', '.', 'semi-supervised', 'methods', 'because', 'of', 'the', 'lack', 'of', 'training', 'data', ',', 'many', 'word', 'sense', 'disambiguation', 'algorithms', 'use', 'semi-supervised', 'learning', ',', 'which', 'allows', 'both', 'labeled', 'and', 'unlabeled', 'data', '.', 'the', 'yarowsky', 'algorithm', 'was', 'an', 'early', 'example', 'of', 'such', 'an', 'algorithm', '.', 'it', 'uses', 'the', '`', 'one', 'sense', 'per', 'collocation', \"'\", 'and', 'the', '`', 'one', 'sense', 'per', 'discourse', \"'\", 'properties', 'of', 'human', 'languages', 'for', 'word', 'sense', 'disambiguation', '.', 'from', 'observation', ',', 'words', 'tend', 'to', 'exhibit', 'only', 'one', 'sense', 'in', 'most', 'given', 'discourse', 'and', 'in', 'a', 'given', 'collocation', '.', 'the', 'bootstrapping', 'approach', 'starts', 'from', 'a', 'small', 'amount', 'of', 'seed', 'data', 'for', 'each', 'word', ':', 'either', 'manually', 'tagged', 'training', 'examples', 'or', 'a', 'small', 'number', 'of', 'surefire', 'decision', 'rules', '-lrb-', 'e.g', '.', ',', '`', 'play', \"'\", 'in', 'the', 'context', 'of', '`', 'bass', \"'\", 'almost', 'always', 'indicates', 'the', 'musical', 'instrument', '-rrb-', '.', 'the', 'seeds', 'are', 'used', 'to', 'train', 'an', 'initial', 'classifier', ',', 'using', 'any', 'supervised', 'method', '.', 'this', 'classifier', 'is', 'then', 'used', 'on', 'the', 'untagged', 'portion', 'of', 'the', 'corpus', 'to', 'extract', 'a', 'larger', 'training', 'set', ',', 'in', 'which', 'only', 'the', 'most', 'confident', 'classifications', 'are', 'included', '.', 'the', 'process', 'repeats', ',', 'each', 'new', 'classifier', 'being', 'trained', 'on', 'a', 'successively', 'larger', 'training', 'corpus', ',', 'until', 'the', 'whole', 'corpus', 'is', 'consumed', ',', 'or', 'until', 'a', 'given', 'maximum', 'number', 'of', 'iterations', 'is', 'reached', '.', 'other', 'semi-supervised', 'techniques', 'use', 'large', 'quantities', 'of', 'untagged', 'corpora', 'to', 'provide', 'co-occurrence', 'information', 'that', 'supplements', 'the', 'tagged', 'corpora', '.', 'these', 'techniques', 'have', 'the', 'potential', 'to', 'help', 'in', 'the', 'adaptation', 'of', 'supervised', 'models', 'to', 'different', 'domains', '.', 'also', ',', 'an', 'ambiguous', 'word', 'in', 'one', 'language', 'is', 'often', 'translated', 'into', 'different', 'words', 'in', 'a', 'second', 'language', 'depending', 'on', 'the', 'sense', 'of', 'the', 'word', '.', 'word-aligned', 'bilingual', 'corpora', 'have', 'been', 'used', 'to', 'infer', 'cross-lingual', 'sense', 'distinctions', ',', 'a', 'kind', 'of', 'semi-supervised', 'system', '.', 'unsupervised', 'methods', 'main', 'article', ':', 'word', 'sense', 'induction', 'unsupervised', 'learning', 'is', 'the', 'greatest', 'challenge', 'for', 'wsd', 'researchers', '.', 'the', 'underlying', 'assumption', 'is', 'that', 'similar', 'senses', 'occur', 'in', 'similar', 'contexts', ',', 'and', 'thus', 'senses', 'can', 'be', 'induced', 'from', 'text', 'by', 'clustering', 'word', 'occurrences', 'using', 'some', 'measure', 'of', 'similarity', 'of', 'context', ',', 'a', 'task', 'referred', 'to', 'as', 'word', 'sense', 'induction', 'or', 'discrimination', '.', 'then', ',', 'new', 'occurrences', 'of', 'the', 'word', 'can', 'be', 'classified', 'into', 'the', 'closest', 'induced', 'clusters\\\\/senses', '.', 'performance', 'has', 'been', 'lower', 'than', 'other', 'methods', ',', 'above', ',', 'but', 'comparisons', 'are', 'difficult', 'since', 'senses', 'induced', 'must', 'be', 'mapped', 'to', 'a', 'known', 'dictionary', 'of', 'word', 'senses', '.', 'if', 'a', 'mapping', 'to', 'a', 'set', 'of', 'dictionary', 'senses', 'is', 'not', 'desired', ',', 'cluster-based', 'evaluations', '-lrb-', 'including', 'measures', 'of', 'entropy', 'and', 'purity', '-rrb-', 'can', 'be', 'performed', '.', 'alternatively', ',', 'word', 'sense', 'induction', 'methods', 'can', 'be', 'tested', 'and', 'compared', 'within', 'an', 'application', '.', 'for', 'instance', ',', 'it', 'has', 'been', 'shown', 'that', 'word', 'sense', 'induction', 'improves', 'web', 'search', 'result', 'clustering', 'by', 'increasing', 'the', 'quality', 'of', 'result', 'clusters', 'and', 'the', 'degree', 'diversification', 'of', 'result', 'lists', '.', 'it', 'is', 'hoped', 'that', 'unsupervised', 'learning', 'will', 'overcome', 'the', 'knowledge', 'acquisition', 'bottleneck', 'because', 'they', 'are', 'not', 'dependent', 'on', 'manual', 'effort', '.', 'other', 'approaches', 'other', 'approaches', 'may', 'vary', 'differently', 'in', 'their', 'methods', ':', 'identification', 'of', 'dominant', 'word', 'senses', ';', 'domain-driven', 'disambiguation', ';', 'wsd', 'using', 'cross-lingual', 'evidence', '.', 'local', 'impediments', 'and', 'summary', 'the', 'knowledge', 'acquisition', 'bottleneck', 'is', 'perhaps', 'the', 'major', 'impediment', 'to', 'solving', 'the', 'wsd', 'problem', '.', 'unsupervised', 'methods', 'rely', 'on', 'knowledge', 'about', 'word', 'senses', ',', 'which', 'is', 'barely', 'formulated', 'in', 'dictionaries', 'and', 'lexical', 'databases', '.', 'supervised', 'methods', 'depend', 'crucially', 'on', 'the', 'existence', 'of', 'manually', 'annotated', 'examples', 'for', 'every', 'word', 'sense', ',', 'a', 'requisite', 'that', 'can', 'so', 'far', 'be', 'met', 'only', 'for', 'a', 'handful', 'of', 'words', 'for', 'testing', 'purposes', ',', 'as', 'it', 'is', 'done', 'in', 'the', 'senseval', 'exercises', '.', 'therefore', ',', 'one', 'of', 'the', 'most', 'promising', 'trends', 'in', 'wsd', 'research', 'is', 'using', 'the', 'largest', 'corpus', 'ever', 'accessible', ',', 'the', 'world', 'wide', 'web', ',', 'to', 'acquire', 'lexical', 'information', 'automatically', '.', 'wsd', 'has', 'been', 'traditionally', 'understood', 'as', 'an', 'intermediate', 'language', 'engineering', 'technology', 'which', 'could', 'improve', 'applications', 'such', 'as', 'information', 'retrieval', '-lrb-', 'ir', '-rrb-', '.', 'in', 'this', 'case', ',', 'however', ',', 'the', 'reverse', 'is', 'also', 'true', ':', 'web', 'search', 'engines', 'implement', 'simple', 'and', 'robust', 'ir', 'techniques', 'that', 'can', 'be', 'successfully', 'used', 'when', 'mining', 'the', 'web', 'for', 'information', 'to', 'be', 'employed', 'in', 'wsd', '.', 'therefore', ',', 'the', 'lack', 'of', 'training', 'data', 'provoked', 'appearing', 'some', 'new', 'algorithms', 'and', 'techniques', 'described', 'here', ':', 'main', 'article', ':', 'automatic', 'acquisition', 'of', 'sense-tagged', 'corpora', 'external', 'knowledge', 'sources', 'knowledge', 'is', 'a', 'fundamental', 'component', 'of', 'wsd', '.', 'knowledge', 'sources', 'provide', 'data', 'which', 'are', 'essential', 'to', 'associate', 'senses', 'with', 'words', '.', 'they', 'can', 'vary', 'from', 'corpora', 'of', 'texts', ',', 'either', 'unlabeled', 'or', 'annotated', 'with', 'word', 'senses', ',', 'to', 'machine-readable', 'dictionaries', ',', 'thesauri', ',', 'glossaries', ',', 'ontologies', ',', 'etc', '.', '.', 'they', 'can', 'be', 'classified', 'as', 'follows', ':', 'structured', ':', 'thesauri', 'machine-readable', 'dictionaries', '-lrb-', 'mrds', '-rrb-', 'ontologies', 'unstructured', ':', 'corpora', ':', 'raw', 'corpora', 'and', 'sense-annotated', 'corpora', 'collocation', 'resources', 'other', 'resources', '-lrb-', 'such', 'as', 'word', 'frequency', 'lists', ',', 'stoplists', ',', 'domain', 'labels', ',', 'etc', '.', '-rrb-', 'evaluation', 'comparing', 'and', 'evaluating', 'different', 'wsd', 'systems', 'is', 'extremely', 'difﬁcult', ',', 'because', 'of', 'the', 'different', 'test', 'sets', ',', 'sense', 'inventories', ',', 'and', 'knowledge', 'resources', 'adopted', '.', 'before', 'the', 'organization', 'of', 'speciﬁc', 'evaluation', 'campaigns', 'most', 'systems', 'were', 'assessed', 'on', 'in-house', ',', 'often', 'small-scale', ',', 'data', 'sets', '.', 'in', 'order', 'to', 'test', 'one', \"'s\", 'algorithm', ',', 'developers', 'should', 'spend', 'their', 'time', 'to', 'annotate', 'all', 'word', 'occurrences', '.', 'and', 'comparing', 'methods', 'even', 'on', 'the', 'same', 'corpus', 'is', 'not', 'eligible', 'if', 'there', 'is', 'different', 'sense', 'inventories', '.', 'in', 'order', 'to', 'define', 'common', 'evaluation', 'datasets', 'and', 'procedures', ',', 'public', 'evaluation', 'campaigns', 'have', 'been', 'organized', '.', 'senseval', '-lrb-', 'now', 'renamed', 'semeval', '-rrb-', 'is', 'an', 'international', 'word', 'sense', 'disambiguation', 'competition', ',', 'held', 'every', 'three', 'years', 'since', '1998', ':', 'senseval-1', '-lrb-', '1998', '-rrb-', ',', 'senseval-2', '-lrb-', '2001', '-rrb-', ',', 'senseval-3', '-lrb-', '2004', '-rrb-', ',', 'and', 'its', 'successor', ',', 'semeval', '-lrb-', '2007', '-rrb-', '.', 'the', 'objective', 'of', 'the', 'competition', 'is', 'to', 'organize', 'different', 'lectures', ',', 'preparing', 'and', 'hand-annotating', 'corpus', 'for', 'testing', 'systems', ',', 'perform', 'a', 'comparative', 'evaluation', 'of', 'wsd', 'systems', 'in', 'several', 'kinds', 'of', 'tasks', ',', 'including', 'all-words', 'and', 'lexical', 'sample', 'wsd', 'for', 'different', 'languages', ',', 'and', ',', 'more', 'recently', ',', 'new', 'tasks', 'such', 'as', 'semantic', 'role', 'labeling', ',', 'gloss', 'wsd', ',', 'lexical', 'substitution', ',', 'etc', '.', '.', 'the', 'systems', 'submitted', 'for', 'evaluation', 'to', 'these', 'competitions', 'usually', 'integrate', 'different', 'techniques', 'and', 'often', 'combine', 'supervised', 'and', 'knowledge-based', 'methods', '-lrb-', 'especially', 'for', 'avoiding', 'bad', 'performance', 'in', 'lack', 'of', 'training', 'examples', '-rrb-', '.', 'task', 'design', 'choices', 'sense', 'inventories', '.', 'during', 'the', 'first', 'senseval', 'workshop', 'the', 'hector', 'sense', 'inventory', 'was', 'adopted', '.', 'the', 'reason', 'for', 'adopting', 'a', 'previously', 'unknown', 'sense', 'inventory', 'was', 'mainly', 'to', 'avoid', 'the', 'use', 'of', 'popular', 'fine-grained', 'word', 'senses', '-lrb-', 'such', 'as', 'wordnet', '-rrb-', ',', 'which', 'could', 'make', 'the', 'experiments', 'unfair', 'or', 'biased', '.', 'however', ',', 'given', 'the', 'lack', 'of', 'coverage', 'of', 'such', 'inventories', ',', 'since', 'the', 'second', 'senseval', 'workshop', 'the', 'wordnet', 'sense', 'inventory', 'has', 'been', 'adopted', '.', 'a', 'set', 'of', 'testing', 'words', '.', 'comparison', 'of', 'methods', 'can', 'be', 'divided', 'in', '2', 'groups', 'by', 'amount', 'of', 'words', 'to', 'test', '.', 'the', 'difference', 'consists', 'in', 'the', 'amount', 'of', 'analysis', 'and', 'processing', ':', 'all-words', 'task', 'implies', 'disambiguating', 'all', 'the', 'words', 'of', 'the', 'text', 'lexical', 'sample', 'consists', 'in', 'disambiguating', 'some', 'previously', 'chosen', 'target', 'words', '.', 'it', 'is', 'assumed', 'that', 'the', 'former', 'one', 'is', 'more', 'realistic', 'evaluation', ',', 'although', 'with', 'very', 'laborious', 'testing', 'of', 'results', '.', 'initially', 'only', 'the', 'latter', 'was', 'used', 'in', 'evaluation', 'but', 'later', 'the', 'former', 'was', 'included', '.', 'lexical', 'sample', 'organizers', 'had', 'to', 'choose', 'samples', 'on', 'which', 'the', 'systems', 'were', 'to', 'be', 'tested', '.', 'a', 'criticism', 'of', 'earlier', 'forays', 'into', 'lexical-sample', 'wsd', 'evaluation', 'is', 'that', 'the', 'lexical', 'sample', 'had', 'been', 'chosen', 'according', 'to', 'the', 'whim', 'of', 'the', 'experimenter', '-lrb-', 'or', ',', 'to', 'coincide', 'with', 'earlier', 'experimenters', \"'\", 'selections', '-rrb-', '.', 'for', 'english', 'senseval', ',', 'a', 'sampling', 'frame', 'was', 'devised', 'in', 'which', 'words', 'were', 'classified', 'according', 'to', 'their', 'frequency', '-lrb-', 'in', 'the', 'bnc', '-rrb-', 'and', 'their', 'polysemy', 'level', '-lrb-', 'in', 'wordnet', '-rrb-', '.', 'also', ',', 'inclusion', 'pos-tagging', 'problem', 'was', 'a', 'matter', 'of', 'discussion', 'and', 'it', 'was', 'decided', 'that', 'samples', 'should', 'be', 'words', 'with', 'known', 'part', 'of', 'speech', 'and', 'some', 'indeterminants', '-lrb-', 'for', 'ex', '.', '15', 'noun', 'tasks', ',', '13', 'verb', 'tasks', ',', '8', 'adjectives', ',', 'and', '5', 'indeterminates', '-rrb-', '.', 'baselines', '.', 'for', 'comparison', 'purposes', ',', 'known', ',', 'yet', 'simple', ',', 'algorithms', 'named', 'baselines', 'are', 'used', '.', 'these', 'include', 'different', 'variants', 'of', 'lesk', 'algorithm', 'or', 'most', 'frequent', 'sense', 'algorithm', '.', 'sense', 'inventory', '.', 'wsd', 'exercises', 'require', 'a', 'dictionary', ',', 'to', 'specify', 'the', 'word', 'senses', 'which', 'are', 'to', 'be', 'disambiguated', ',', 'and', 'a', 'corpus', 'of', 'language', 'data', 'to', 'be', 'disambiguated', '.', 'wordnet', 'is', 'the', 'most', 'popular', 'example', 'of', 'sense', 'inventory', '.', 'the', 'reason', 'for', 'adopting', 'the', 'hector', 'database', 'during', 'senseval-1', 'was', 'that', 'the', 'wordnet', 'inventory', 'was', 'already', 'publicly', 'available', '.', 'evaluation', 'measures', '.', 'during', 'the', 'evaluation', 'of', 'wsd', 'systems', 'two', 'main', 'performance', 'measures', 'are', 'used', ':', 'precision', ':', 'the', 'fraction', 'of', 'system', 'assignments', 'made', 'that', 'are', 'correct', 'recall', ':', 'the', 'fraction', 'of', 'total', 'word', 'instances', 'correctly', 'assigned', 'by', 'a', 'system', 'if', 'a', 'system', 'makes', 'an', 'assignment', 'for', 'every', 'word', ',', 'then', 'precision', 'and', 'recall', 'are', 'the', 'same', ',', 'and', 'can', 'be', 'called', 'accuracy', '.', 'this', 'model', 'has', 'been', 'extended', 'to', 'take', 'into', 'account', 'systems', 'that', 'return', 'a', 'set', 'of', 'senses', 'with', 'weights', 'for', 'each', 'occurrence', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VqYVpjzVzo3",
        "colab_type": "code",
        "outputId": "0031b8d0-2b7d-4921-b278-adf7bf03037b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# count the number of words\n",
        "token_freq = nltk.FreqDist(tokens)\n",
        "token_freq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'in': 102,\n",
              "          'computational': 8,\n",
              "          'linguistics': 4,\n",
              "          ',': 233,\n",
              "          'word-sense': 2,\n",
              "          'disambiguation': 10,\n",
              "          '-lrb-': 47,\n",
              "          'wsd': 32,\n",
              "          '-rrb-': 47,\n",
              "          'is': 77,\n",
              "          'an': 12,\n",
              "          'open': 1,\n",
              "          'problem': 12,\n",
              "          'of': 155,\n",
              "          'natural': 2,\n",
              "          'language': 10,\n",
              "          'processing': 3,\n",
              "          'which': 22,\n",
              "          'governs': 1,\n",
              "          'the': 215,\n",
              "          'process': 3,\n",
              "          'identifying': 1,\n",
              "          'sense': 51,\n",
              "          'a': 96,\n",
              "          'word': 60,\n",
              "          'i.e': 2,\n",
              "          '.': 180,\n",
              "          'meaning': 8,\n",
              "          'used': 20,\n",
              "          'sentence': 3,\n",
              "          'when': 3,\n",
              "          'has': 16,\n",
              "          'multiple': 1,\n",
              "          'meanings': 4,\n",
              "          'polysemy': 3,\n",
              "          'solution': 3,\n",
              "          'to': 102,\n",
              "          'this': 9,\n",
              "          'impacts': 1,\n",
              "          'other': 14,\n",
              "          'computer-related': 1,\n",
              "          'writing': 2,\n",
              "          'such': 21,\n",
              "          'as': 40,\n",
              "          'discourse': 3,\n",
              "          'improving': 1,\n",
              "          'relevance': 1,\n",
              "          'search': 3,\n",
              "          'engines': 2,\n",
              "          'anaphora': 1,\n",
              "          'resolution': 1,\n",
              "          'coherence': 1,\n",
              "          'inference': 1,\n",
              "          'et': 1,\n",
              "          'cetera': 1,\n",
              "          'research': 10,\n",
              "          'progressed': 1,\n",
              "          'steadily': 1,\n",
              "          'point': 1,\n",
              "          'where': 2,\n",
              "          'systems': 16,\n",
              "          'achieve': 2,\n",
              "          'sufficiently': 2,\n",
              "          'high': 1,\n",
              "          'levels': 1,\n",
              "          'accuracy': 8,\n",
              "          'on': 29,\n",
              "          'variety': 3,\n",
              "          'types': 2,\n",
              "          'and': 87,\n",
              "          'ambiguities': 1,\n",
              "          'rich': 2,\n",
              "          'techniques': 9,\n",
              "          'have': 15,\n",
              "          'been': 20,\n",
              "          'researched': 1,\n",
              "          'from': 15,\n",
              "          'dictionary-based': 3,\n",
              "          'methods': 30,\n",
              "          'that': 30,\n",
              "          'use': 9,\n",
              "          'knowledge': 26,\n",
              "          'encoded': 2,\n",
              "          'lexical': 14,\n",
              "          'resources': 7,\n",
              "          'supervised': 18,\n",
              "          'machine': 8,\n",
              "          'learning': 12,\n",
              "          'classifier': 4,\n",
              "          'trained': 2,\n",
              "          'for': 45,\n",
              "          'each': 12,\n",
              "          'distinct': 4,\n",
              "          'corpus': 15,\n",
              "          'manually': 4,\n",
              "          'sense-annotated': 3,\n",
              "          'examples': 5,\n",
              "          'completely': 3,\n",
              "          'unsupervised': 7,\n",
              "          'cluster': 1,\n",
              "          'occurrences': 6,\n",
              "          'words': 36,\n",
              "          'thereby': 1,\n",
              "          'inducing': 1,\n",
              "          'senses': 35,\n",
              "          'among': 1,\n",
              "          'these': 19,\n",
              "          'approaches': 20,\n",
              "          'most': 12,\n",
              "          'successful': 4,\n",
              "          'algorithms': 9,\n",
              "          'date': 2,\n",
              "          'current': 2,\n",
              "          'difficult': 5,\n",
              "          'state': 2,\n",
              "          'without': 3,\n",
              "          'host': 1,\n",
              "          'caveats': 1,\n",
              "          'english': 5,\n",
              "          'at': 9,\n",
              "          'coarse-grained': 5,\n",
              "          'homograph': 2,\n",
              "          'level': 4,\n",
              "          'routinely': 1,\n",
              "          'above': 5,\n",
              "          '90': 1,\n",
              "          '%': 9,\n",
              "          'with': 18,\n",
              "          'some': 12,\n",
              "          'particular': 2,\n",
              "          'homographs': 1,\n",
              "          'achieving': 1,\n",
              "          'over': 1,\n",
              "          '96': 1,\n",
              "          'finer-grained': 1,\n",
              "          'distinctions': 9,\n",
              "          'top': 1,\n",
              "          'accuracies': 1,\n",
              "          '59.1': 1,\n",
              "          '69.0': 1,\n",
              "          'reported': 2,\n",
              "          'recent': 3,\n",
              "          'evaluation': 14,\n",
              "          'exercises': 4,\n",
              "          'semeval-2007': 1,\n",
              "          'senseval-2': 3,\n",
              "          'baseline': 1,\n",
              "          'simplest': 1,\n",
              "          'possible': 3,\n",
              "          'algorithm': 9,\n",
              "          'always': 3,\n",
              "          'choosing': 1,\n",
              "          'frequent': 2,\n",
              "          'was': 17,\n",
              "          '51.4': 1,\n",
              "          '57': 1,\n",
              "          'respectively': 1,\n",
              "          'task': 14,\n",
              "          'two': 7,\n",
              "          'variants': 2,\n",
              "          ':': 26,\n",
              "          '``': 18,\n",
              "          'sample': 6,\n",
              "          \"''\": 18,\n",
              "          'all': 11,\n",
              "          'former': 4,\n",
              "          'comprises': 1,\n",
              "          'disambiguating': 5,\n",
              "          'small': 4,\n",
              "          'target': 6,\n",
              "          'were': 6,\n",
              "          'previously': 3,\n",
              "          'selected': 1,\n",
              "          'while': 4,\n",
              "          'latter': 4,\n",
              "          'piece': 1,\n",
              "          'running': 1,\n",
              "          'text': 9,\n",
              "          'need': 3,\n",
              "          'be': 32,\n",
              "          'disambiguated': 5,\n",
              "          'deemed': 2,\n",
              "          'more': 9,\n",
              "          'realistic': 2,\n",
              "          'form': 4,\n",
              "          'but': 11,\n",
              "          'expensive': 2,\n",
              "          'produce': 1,\n",
              "          'because': 9,\n",
              "          'human': 11,\n",
              "          'annotators': 2,\n",
              "          'read': 1,\n",
              "          'definitions': 6,\n",
              "          'sequence': 1,\n",
              "          'every': 5,\n",
              "          'time': 2,\n",
              "          'they': 9,\n",
              "          'make': 4,\n",
              "          'tagging': 7,\n",
              "          'judgement': 1,\n",
              "          'rather': 1,\n",
              "          'than': 9,\n",
              "          'once': 1,\n",
              "          'block': 1,\n",
              "          'instances': 2,\n",
              "          'same': 5,\n",
              "          'give': 3,\n",
              "          'hint': 1,\n",
              "          'how': 1,\n",
              "          'works': 1,\n",
              "          'consider': 3,\n",
              "          'exist': 3,\n",
              "          'written': 1,\n",
              "          'bass': 12,\n",
              "          'type': 2,\n",
              "          'fish': 5,\n",
              "          'tones': 1,\n",
              "          'low': 3,\n",
              "          'frequency': 5,\n",
              "          'sentences': 4,\n",
              "          'i': 2,\n",
              "          'went': 1,\n",
              "          'fishing': 3,\n",
              "          'sea': 3,\n",
              "          'line': 1,\n",
              "          'song': 2,\n",
              "          'too': 1,\n",
              "          'weak': 1,\n",
              "          'it': 24,\n",
              "          'obvious': 1,\n",
              "          'first': 5,\n",
              "          'using': 12,\n",
              "          'second': 3,\n",
              "          'instrument': 5,\n",
              "          'being': 5,\n",
              "          'below': 1,\n",
              "          'developing': 1,\n",
              "          'replicate': 1,\n",
              "          'ability': 1,\n",
              "          'can': 26,\n",
              "          'often': 6,\n",
              "          'further': 2,\n",
              "          'exemplified': 1,\n",
              "          'by': 18,\n",
              "          'implicit': 1,\n",
              "          'equivocation': 1,\n",
              "          'between': 2,\n",
              "          'sound': 1,\n",
              "          'musical': 3,\n",
              "          'history': 1,\n",
              "          'formulated': 2,\n",
              "          'during': 4,\n",
              "          'early': 4,\n",
              "          'days': 2,\n",
              "          'translation': 4,\n",
              "          '1940s': 1,\n",
              "          'making': 3,\n",
              "          'one': 19,\n",
              "          'oldest': 1,\n",
              "          'problems': 1,\n",
              "          'warren': 1,\n",
              "          'weaver': 1,\n",
              "          'his': 1,\n",
              "          'famous': 1,\n",
              "          '1949': 1,\n",
              "          'memorandum': 1,\n",
              "          'introduced': 1,\n",
              "          'context': 6,\n",
              "          'researchers': 5,\n",
              "          'understood': 2,\n",
              "          'significance': 1,\n",
              "          'difficulty': 1,\n",
              "          'well': 3,\n",
              "          'fact': 1,\n",
              "          'bar-hillel': 1,\n",
              "          '1960': 1,\n",
              "          'example': 8,\n",
              "          'argue': 2,\n",
              "          'could': 4,\n",
              "          'not': 22,\n",
              "          'solved': 1,\n",
              "          'electronic': 1,\n",
              "          'computer': 5,\n",
              "          'general': 2,\n",
              "          'model': 2,\n",
              "          'world': 6,\n",
              "          '1970s': 1,\n",
              "          'subtask': 1,\n",
              "          'semantic': 4,\n",
              "          'interpretation': 1,\n",
              "          'developed': 1,\n",
              "          'within': 2,\n",
              "          'field': 2,\n",
              "          'artificial': 1,\n",
              "          'intelligence': 1,\n",
              "          'since': 6,\n",
              "          'largely': 1,\n",
              "          'rule-based': 1,\n",
              "          'hand-coded': 1,\n",
              "          'prone': 1,\n",
              "          'acquisition': 5,\n",
              "          'bottleneck': 4,\n",
              "          '1980s': 1,\n",
              "          'large-scale': 1,\n",
              "          'oxford': 1,\n",
              "          'advanced': 1,\n",
              "          'learner': 1,\n",
              "          \"'s\": 7,\n",
              "          'dictionary': 10,\n",
              "          'oald': 1,\n",
              "          'became': 2,\n",
              "          'available': 2,\n",
              "          'hand-coding': 1,\n",
              "          'replaced': 1,\n",
              "          'automatically': 4,\n",
              "          'extracted': 1,\n",
              "          'still': 3,\n",
              "          'knowledge-based': 6,\n",
              "          'or': 28,\n",
              "          '1990s': 2,\n",
              "          'statistical': 1,\n",
              "          'revolution': 1,\n",
              "          'swept': 1,\n",
              "          'through': 1,\n",
              "          'paradigm': 1,\n",
              "          'apply': 1,\n",
              "          '2000s': 1,\n",
              "          'saw': 1,\n",
              "          'reach': 1,\n",
              "          'plateau': 1,\n",
              "          'so': 4,\n",
              "          'attention': 2,\n",
              "          'shifted': 1,\n",
              "          'coarser-grained': 1,\n",
              "          'domain': 2,\n",
              "          'adaptation': 2,\n",
              "          'semi-supervised': 6,\n",
              "          'corpus-based': 1,\n",
              "          'combinations': 1,\n",
              "          'different': 17,\n",
              "          'return': 2,\n",
              "          'via': 1,\n",
              "          'graph-based': 4,\n",
              "          'continue': 2,\n",
              "          'perform': 4,\n",
              "          'best': 2,\n",
              "          'difficulties': 1,\n",
              "          'differences': 1,\n",
              "          'dictionaries': 6,\n",
              "          'deciding': 1,\n",
              "          'what': 2,\n",
              "          'are': 34,\n",
              "          'cases': 4,\n",
              "          'like': 5,\n",
              "          'least': 2,\n",
              "          'obviously': 1,\n",
              "          'however': 12,\n",
              "          'closely': 2,\n",
              "          'related': 3,\n",
              "          'metaphorical': 1,\n",
              "          'metonymic': 1,\n",
              "          'extension': 1,\n",
              "          'another': 2,\n",
              "          'division': 2,\n",
              "          'into': 10,\n",
              "          'becomes': 1,\n",
              "          'much': 6,\n",
              "          'thesauruses': 1,\n",
              "          'will': 3,\n",
              "          'provide': 4,\n",
              "          'divisions': 1,\n",
              "          'choose': 2,\n",
              "          'just': 2,\n",
              "          'its': 5,\n",
              "          'set': 6,\n",
              "          'generally': 1,\n",
              "          'results': 4,\n",
              "          'broad': 1,\n",
              "          'better': 5,\n",
              "          'those': 4,\n",
              "          'narrow': 1,\n",
              "          'ones': 1,\n",
              "          'given': 6,\n",
              "          'lack': 5,\n",
              "          'full-fledged': 1,\n",
              "          'inventory': 11,\n",
              "          'work': 5,\n",
              "          'fine-grained': 5,\n",
              "          'performed': 2,\n",
              "          'wordnet': 9,\n",
              "          'reference': 1,\n",
              "          'lexicon': 2,\n",
              "          'encodes': 1,\n",
              "          'concepts': 1,\n",
              "          'synonym': 1,\n",
              "          'sets': 3,\n",
              "          'e.g': 4,\n",
              "          'concept': 2,\n",
              "          'car': 2,\n",
              "          '-lcb-': 1,\n",
              "          'auto': 1,\n",
              "          'automobile': 1,\n",
              "          'motorcar': 1,\n",
              "          '-rcb-': 1,\n",
              "          'purposes': 3,\n",
              "          'include': 3,\n",
              "          'roget': 2,\n",
              "          'thesaurus': 3,\n",
              "          'wikipedia': 2,\n",
              "          'part-of-speech': 4,\n",
              "          'any': 3,\n",
              "          'real': 1,\n",
              "          'test': 6,\n",
              "          'very': 7,\n",
              "          'potentially': 2,\n",
              "          'constraints': 1,\n",
              "          'question': 1,\n",
              "          'whether': 2,\n",
              "          'tasks': 5,\n",
              "          'should': 3,\n",
              "          'kept': 1,\n",
              "          'together': 2,\n",
              "          'decoupled': 1,\n",
              "          'unanimously': 1,\n",
              "          'resolved': 1,\n",
              "          'recently': 4,\n",
              "          'scientists': 1,\n",
              "          'incline': 1,\n",
              "          'things': 1,\n",
              "          'separately': 1,\n",
              "          'senseval\\\\/semeval': 1,\n",
              "          'competitions': 2,\n",
              "          'parts': 5,\n",
              "          'speech': 6,\n",
              "          'provided': 1,\n",
              "          'input': 1,\n",
              "          'disambiguate': 5,\n",
              "          'instructive': 1,\n",
              "          'compare': 1,\n",
              "          'both': 4,\n",
              "          'involve': 1,\n",
              "          'do': 3,\n",
              "          'tend': 2,\n",
              "          'mainly': 3,\n",
              "          'part': 2,\n",
              "          'primarily': 2,\n",
              "          'determined': 2,\n",
              "          'immediately': 1,\n",
              "          'adjacent': 1,\n",
              "          'three': 2,\n",
              "          'whereas': 1,\n",
              "          'may': 3,\n",
              "          'away': 1,\n",
              "          'success': 2,\n",
              "          'rate': 1,\n",
              "          'present': 1,\n",
              "          'higher': 1,\n",
              "          'state-of-the': 1,\n",
              "          'art': 3,\n",
              "          'around': 2,\n",
              "          '95': 1,\n",
              "          'compared': 3,\n",
              "          'less': 1,\n",
              "          '75': 1,\n",
              "          'figures': 1,\n",
              "          'typical': 1,\n",
              "          'languages': 3,\n",
              "          'inter-judge': 2,\n",
              "          'variance': 2,\n",
              "          'normally': 2,\n",
              "          'tested': 3,\n",
              "          'having': 2,\n",
              "          'their': 8,\n",
              "          'against': 1,\n",
              "          'relatively': 1,\n",
              "          'easy': 1,\n",
              "          'assign': 1,\n",
              "          'training': 9,\n",
              "          'people': 2,\n",
              "          'tag': 1,\n",
              "          'far': 2,\n",
              "          'users': 1,\n",
              "          'memorize': 2,\n",
              "          'take': 3,\n",
              "          'impossible': 1,\n",
              "          'individuals': 1,\n",
              "          'moreover': 2,\n",
              "          'humans': 2,\n",
              "          'agree': 3,\n",
              "          'hand': 1,\n",
              "          '--': 6,\n",
              "          'list': 1,\n",
              "          'belongs': 1,\n",
              "          'thus': 3,\n",
              "          'expected': 1,\n",
              "          'performance': 8,\n",
              "          'indeed': 1,\n",
              "          'serves': 2,\n",
              "          'standard': 2,\n",
              "          'incoherent': 1,\n",
              "          'citation': 2,\n",
              "          'needed': 3,\n",
              "          'upper': 1,\n",
              "          'bound': 1,\n",
              "          'again': 1,\n",
              "          'why': 1,\n",
              "          'put': 1,\n",
              "          'common': 5,\n",
              "          'ai': 2,\n",
              "          'douglas': 1,\n",
              "          'lenat': 1,\n",
              "          'parse': 1,\n",
              "          'ontology': 1,\n",
              "          'comparing': 3,\n",
              "          'jill': 2,\n",
              "          'mary': 2,\n",
              "          'sisters': 2,\n",
              "          'mothers': 1,\n",
              "          'independently': 1,\n",
              "          'mother': 1,\n",
              "          'properly': 1,\n",
              "          'identify': 1,\n",
              "          'must': 2,\n",
              "          'know': 2,\n",
              "          'facts': 1,\n",
              "          'sometimes': 1,\n",
              "          'pronouns': 1,\n",
              "          'case': 2,\n",
              "          'anaphoras': 1,\n",
              "          'cataphoras': 1,\n",
              "          \"'\": 12,\n",
              "          'task-dependency': 1,\n",
              "          'task-independent': 1,\n",
              "          'coherent': 1,\n",
              "          'requires': 2,\n",
              "          'own': 2,\n",
              "          'relevant': 3,\n",
              "          'ambiguity': 1,\n",
              "          '`': 10,\n",
              "          'mouse': 1,\n",
              "          'animal': 1,\n",
              "          'device': 1,\n",
              "          'english-french': 1,\n",
              "          'information': 8,\n",
              "          'retrieval': 3,\n",
              "          'opposite': 1,\n",
              "          'true': 2,\n",
              "          'river': 3,\n",
              "          'choice': 1,\n",
              "          'french': 2,\n",
              "          'fleuve': 1,\n",
              "          'flows': 2,\n",
              "          'rivière': 1,\n",
              "          'also': 9,\n",
              "          'might': 1,\n",
              "          'required': 2,\n",
              "          'applications': 3,\n",
              "          'takes': 1,\n",
              "          'selection': 2,\n",
              "          'here': 2,\n",
              "          'correspond': 1,\n",
              "          'significant': 1,\n",
              "          'source': 2,\n",
              "          'bank': 2,\n",
              "          'translate': 1,\n",
              "          'banque': 1,\n",
              "          'financial': 1,\n",
              "          'rive': 1,\n",
              "          'edge': 1,\n",
              "          'necessarily': 1,\n",
              "          'enough': 2,\n",
              "          'query': 1,\n",
              "          'retrieved': 1,\n",
              "          'document': 1,\n",
              "          ';': 4,\n",
              "          'unimportant': 1,\n",
              "          'discreteness': 3,\n",
              "          'finally': 1,\n",
              "          'notion': 1,\n",
              "          'slippery': 1,\n",
              "          'controversial': 1,\n",
              "          'pen': 1,\n",
              "          'enclosure': 1,\n",
              "          'go': 2,\n",
              "          'down': 1,\n",
              "          'disagreements': 1,\n",
              "          'arise': 1,\n",
              "          'agreed': 1,\n",
              "          'only': 5,\n",
              "          '85': 1,\n",
              "          'principle': 1,\n",
              "          'infinitely': 1,\n",
              "          'variable': 1,\n",
              "          'sensitive': 1,\n",
              "          'does': 2,\n",
              "          'divide': 1,\n",
              "          'up': 1,\n",
              "          'easily': 1,\n",
              "          'discrete': 1,\n",
              "          'sub-meanings': 1,\n",
              "          'lexicographers': 2,\n",
              "          'frequently': 1,\n",
              "          'discover': 1,\n",
              "          'corpora': 12,\n",
              "          'loose': 1,\n",
              "          'overlapping': 1,\n",
              "          'conventional': 2,\n",
              "          'extended': 2,\n",
              "          'modulated': 1,\n",
              "          'exploited': 1,\n",
              "          'bewildering': 1,\n",
              "          'ways': 1,\n",
              "          'lexicography': 1,\n",
              "          'generalize': 1,\n",
              "          'evoke': 1,\n",
              "          'explain': 1,\n",
              "          'full': 2,\n",
              "          'range': 1,\n",
              "          'seem': 1,\n",
              "          'well-behaved': 1,\n",
              "          'semantically': 1,\n",
              "          'clear': 1,\n",
              "          'if': 7,\n",
              "          'applicable': 1,\n",
              "          'decisions': 1,\n",
              "          'usually': 2,\n",
              "          'driven': 1,\n",
              "          'considerations': 1,\n",
              "          'named': 2,\n",
              "          'substitution': 2,\n",
              "          'proposed': 1,\n",
              "          'consists': 3,\n",
              "          'providing': 1,\n",
              "          'substitute': 1,\n",
              "          'preserves': 1,\n",
              "          'original': 1,\n",
              "          'substitutes': 1,\n",
              "          'chosen': 3,\n",
              "          'overcoming': 1,\n",
              "          'there': 4,\n",
              "          'main': 4,\n",
              "          'deep': 4,\n",
              "          'shallow': 4,\n",
              "          'presume': 1,\n",
              "          'access': 1,\n",
              "          'comprehensive': 1,\n",
              "          'body': 2,\n",
              "          'you': 1,\n",
              "          'sounds': 2,\n",
              "          'songs': 1,\n",
              "          'then': 6,\n",
              "          'determine': 1,\n",
              "          'practice': 2,\n",
              "          'computer-readable': 1,\n",
              "          'format': 1,\n",
              "          'outside': 1,\n",
              "          'limited': 2,\n",
              "          'domains': 3,\n",
              "          'did': 1,\n",
              "          'would': 1,\n",
              "          'accurate': 1,\n",
              "          'long': 1,\n",
              "          'tradition': 1,\n",
              "          'trying': 1,\n",
              "          'terms': 1,\n",
              "          'coded': 1,\n",
              "          'hard': 1,\n",
              "          'say': 1,\n",
              "          'clearly': 1,\n",
              "          'involved': 1,\n",
              "          'linguistic': 1,\n",
              "          'attempt': 2,\n",
              "          'margaret': 1,\n",
              "          'masterman': 1,\n",
              "          'her': 1,\n",
              "          'colleagues': 1,\n",
              "          'cambridge': 1,\n",
              "          'unit': 1,\n",
              "          'england': 1,\n",
              "          '1950s': 1,\n",
              "          'data': 9,\n",
              "          'punched-card': 1,\n",
              "          'version': 1,\n",
              "          'numbered': 1,\n",
              "          'heads': 1,\n",
              "          'indicator': 1,\n",
              "          'topics': 1,\n",
              "          'looked': 1,\n",
              "          'repetitions': 1,\n",
              "          'intersection': 1,\n",
              "          'had': 3,\n",
              "          'strong': 1,\n",
              "          'relationships': 1,\n",
              "          'later': 2,\n",
              "          'especially': 2,\n",
              "          'yarowsky': 2,\n",
              "          'optimisation': 1,\n",
              "          'method': 3,\n",
              "          \"n't\": 1,\n",
              "          'try': 1,\n",
              "          'understand': 1,\n",
              "          'surrounding': 2,\n",
              "          'nearby': 2,\n",
              "          'probably': 4,\n",
              "          'music': 2,\n",
              "          'rules': 2,\n",
              "          'derived': 1,\n",
              "          'tagged': 3,\n",
              "          'approach': 2,\n",
              "          'theoretically': 1,\n",
              "          'powerful': 1,\n",
              "          'gives': 1,\n",
              "          'superior': 2,\n",
              "          'due': 1,\n",
              "          'confused': 1,\n",
              "          'dogs': 2,\n",
              "          'bark': 2,\n",
              "          'tree': 3,\n",
              "          'contains': 1,\n",
              "          'near': 1,\n",
              "          'four': 1,\n",
              "          '-': 2,\n",
              "          'rely': 3,\n",
              "          'thesauri': 3,\n",
              "          'bases': 1,\n",
              "          'evidence': 3,\n",
              "          'train': 3,\n",
              "          'minimally': 1,\n",
              "          'secondary': 1,\n",
              "          'annotated': 3,\n",
              "          'seed': 2,\n",
              "          'bootstrapping': 2,\n",
              "          'word-aligned': 2,\n",
              "          'bilingual': 2,\n",
              "          'eschew': 1,\n",
              "          'almost': 4,\n",
              "          'external': 2,\n",
              "          'directly': 1,\n",
              "          'raw': 2,\n",
              "          'unannotated': 1,\n",
              "          'known': 4,\n",
              "          'under': 1,\n",
              "          'name': 1,\n",
              "          'discrimination': 2,\n",
              "          'defining': 1,\n",
              "          'window': 1,\n",
              "          'n': 2,\n",
              "          'content': 1,\n",
              "          'statistically': 1,\n",
              "          'analyzing': 1,\n",
              "          'naïve': 1,\n",
              "          'bayes': 1,\n",
              "          'classifiers': 1,\n",
              "          'decision': 2,\n",
              "          'trees': 1,\n",
              "          'kernel-based': 1,\n",
              "          'support': 2,\n",
              "          'vector': 2,\n",
              "          'machines': 2,\n",
              "          'shown': 5,\n",
              "          'gained': 1,\n",
              "          'community': 1,\n",
              "          'currently': 1,\n",
              "          'close': 1,\n",
              "          'lesk': 2,\n",
              "          'seminal': 1,\n",
              "          'based': 3,\n",
              "          'hypothesis': 1,\n",
              "          'relation': 1,\n",
              "          'observed': 1,\n",
              "          'finding': 1,\n",
              "          'pair': 2,\n",
              "          'greatest': 2,\n",
              "          'overlap': 1,\n",
              "          'pine': 1,\n",
              "          'cone': 1,\n",
              "          'appropriate': 1,\n",
              "          'evergreen': 1,\n",
              "          'alternative': 1,\n",
              "          'relatedness': 1,\n",
              "          'compute': 1,\n",
              "          'similarity': 2,\n",
              "          'base': 2,\n",
              "          'reminiscent': 1,\n",
              "          'spreading': 1,\n",
              "          'activation': 1,\n",
              "          'applied': 2,\n",
              "          'complex': 1,\n",
              "          'even': 3,\n",
              "          'outperforming': 1,\n",
              "          'them': 3,\n",
              "          'specific': 1,\n",
              "          'simple': 4,\n",
              "          'graph': 1,\n",
              "          'connectivity': 1,\n",
              "          'measures': 4,\n",
              "          'degree': 2,\n",
              "          'state-of-the-art': 1,\n",
              "          'presence': 1,\n",
              "          'transferring': 1,\n",
              "          'relations': 1,\n",
              "          'boost': 1,\n",
              "          'enabling': 1,\n",
              "          'rival': 1,\n",
              "          'outperform': 1,\n",
              "          'domain-specific': 1,\n",
              "          'setting': 1,\n",
              "          'selectional': 2,\n",
              "          'preferences': 1,\n",
              "          'restrictions': 1,\n",
              "          'useful': 1,\n",
              "          'knowing': 1,\n",
              "          'typically': 1,\n",
              "          'cooks': 1,\n",
              "          'food': 1,\n",
              "          'am': 1,\n",
              "          'cooking': 1,\n",
              "          'basses': 1,\n",
              "          'assumption': 2,\n",
              "          'hence': 1,\n",
              "          'reasoning': 1,\n",
              "          'unnecessary': 1,\n",
              "          'going': 1,\n",
              "          'including': 3,\n",
              "          'associated': 1,\n",
              "          'feature': 2,\n",
              "          'parameter': 1,\n",
              "          'optimization': 1,\n",
              "          'ensemble': 1,\n",
              "          'memory-based': 1,\n",
              "          'cope': 1,\n",
              "          'high-dimensionality': 1,\n",
              "          'space': 1,\n",
              "          'subject': 1,\n",
              "          'new': 5,\n",
              "          'substantial': 1,\n",
              "          'amounts': 1,\n",
              "          'sense-tagged': 2,\n",
              "          'laborious': 2,\n",
              "          'create': 1,\n",
              "          'many': 1,\n",
              "          'allows': 1,\n",
              "          'labeled': 1,\n",
              "          'unlabeled': 2,\n",
              "          'uses': 1,\n",
              "          'per': 2,\n",
              "          'collocation': 3,\n",
              "          'properties': 1,\n",
              "          'observation': 1,\n",
              "          'exhibit': 1,\n",
              "          'starts': 1,\n",
              "          'amount': 3,\n",
              "          'either': 2,\n",
              "          'number': 2,\n",
              "          'surefire': 1,\n",
              "          'play': 1,\n",
              "          'indicates': 1,\n",
              "          'seeds': 1,\n",
              "          'initial': 1,\n",
              "          'untagged': 2,\n",
              "          'portion': 1,\n",
              "          'extract': 1,\n",
              "          'larger': 2,\n",
              "          'confident': 1,\n",
              "          'classifications': 1,\n",
              "          'included': 2,\n",
              "          'repeats': 1,\n",
              "          'successively': 1,\n",
              "          'until': 2,\n",
              "          'whole': 1,\n",
              "          'consumed': 1,\n",
              "          'maximum': 1,\n",
              "          'iterations': 1,\n",
              "          'reached': 1,\n",
              "          'large': 1,\n",
              "          'quantities': 1,\n",
              "          'co-occurrence': 1,\n",
              "          'supplements': 1,\n",
              "          'potential': 1,\n",
              "          'help': 1,\n",
              "          'models': 1,\n",
              "          'ambiguous': 1,\n",
              "          'translated': 1,\n",
              "          'depending': 1,\n",
              "          'infer': 1,\n",
              "          'cross-lingual': 2,\n",
              "          'kind': 1,\n",
              "          'system': 4,\n",
              "          'article': 2,\n",
              "          'induction': 4,\n",
              "          'challenge': 1,\n",
              "          'underlying': 1,\n",
              "          'similar': 2,\n",
              "          'occur': 1,\n",
              "          'contexts': 1,\n",
              "          'induced': 3,\n",
              "          'clustering': 2,\n",
              "          'measure': 1,\n",
              "          'referred': 1,\n",
              "          'classified': 3,\n",
              "          'closest': 1,\n",
              "          'clusters\\\\/senses': 1,\n",
              "          'lower': 1,\n",
              "          'comparisons': 1,\n",
              "          'mapped': 1,\n",
              "          'mapping': 1,\n",
              "          'desired': 1,\n",
              "          'cluster-based': 1,\n",
              "          'evaluations': 1,\n",
              "          'entropy': 1,\n",
              "          'purity': 1,\n",
              "          'alternatively': 1,\n",
              "          'application': 1,\n",
              "          'instance': 1,\n",
              "          'improves': 1,\n",
              "          'web': 4,\n",
              "          'result': 3,\n",
              "          'increasing': 1,\n",
              "          'quality': 1,\n",
              "          'clusters': 1,\n",
              "          'diversification': 1,\n",
              "          'lists': 2,\n",
              "          'hoped': 1,\n",
              "          'overcome': 1,\n",
              "          'dependent': 1,\n",
              "          'manual': 1,\n",
              "          'effort': 1,\n",
              "          'vary': 2,\n",
              "          'differently': 1,\n",
              "          'identification': 1,\n",
              "          'dominant': 1,\n",
              "          'domain-driven': 1,\n",
              "          'local': 1,\n",
              "          'impediments': 1,\n",
              "          'summary': 1,\n",
              "          'perhaps': 1,\n",
              "          'major': 1,\n",
              "          'impediment': 1,\n",
              "          'solving': 1,\n",
              "          'about': 1,\n",
              "          'barely': 1,\n",
              "          'databases': 1,\n",
              "          'depend': 1,\n",
              "          'crucially': 1,\n",
              "          'existence': 1,\n",
              "          'requisite': 1,\n",
              "          'met': 1,\n",
              "          'handful': 1,\n",
              "          'testing': 4,\n",
              "          'done': 1,\n",
              "          'senseval': 5,\n",
              "          'therefore': 2,\n",
              "          'promising': 1,\n",
              "          'trends': 1,\n",
              "          'largest': 1,\n",
              "          'ever': 1,\n",
              "          'accessible': 1,\n",
              "          'wide': 1,\n",
              "          'acquire': 1,\n",
              "          'traditionally': 1,\n",
              "          'intermediate': 1,\n",
              "          'engineering': 1,\n",
              "          'technology': 1,\n",
              "          'improve': 1,\n",
              "          'ir': 2,\n",
              "          'reverse': 1,\n",
              "          'implement': 1,\n",
              "          'robust': 1,\n",
              "          'successfully': 1,\n",
              "          'mining': 1,\n",
              "          'employed': 1,\n",
              "          'provoked': 1,\n",
              "          'appearing': 1,\n",
              "          'described': 1,\n",
              "          'automatic': 1,\n",
              "          'sources': 2,\n",
              "          'fundamental': 1,\n",
              "          'component': 1,\n",
              "          'essential': 1,\n",
              "          'associate': 1,\n",
              "          'texts': 1,\n",
              "          'machine-readable': 2,\n",
              "          'glossaries': 1,\n",
              "          'ontologies': 2,\n",
              "          'etc': 3,\n",
              "          'follows': 1,\n",
              "          'structured': 1,\n",
              "          'mrds': 1,\n",
              "          'unstructured': 1,\n",
              "          'stoplists': 1,\n",
              "          'labels': 1,\n",
              "          'evaluating': 1,\n",
              "          'extremely': 1,\n",
              "          'difﬁcult': 1,\n",
              "          'inventories': 4,\n",
              "          'adopted': 3,\n",
              "          'before': 1,\n",
              "          'organization': 1,\n",
              "          'speciﬁc': 1,\n",
              "          'campaigns': 2,\n",
              "          ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyjBYYYPWMiB",
        "colab_type": "code",
        "outputId": "b7c1c69c-2b97-45bb-94dc-e789f95e9834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Generate bigram\n",
        "\n",
        "#bigram = nltk.bigrams(tokens)\n",
        "bigram = nltk.ngrams(tokens, 2) # n=2 is the number of n in n_gram\n",
        "\n",
        "token_freq = nltk.FreqDist(bigram)\n",
        "for token in token_freq.keys():\n",
        "  print(token,token_freq[token])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('in', 'computational') 4\n",
            "('computational', 'linguistics') 4\n",
            "('linguistics', ',') 3\n",
            "(',', 'word-sense') 1\n",
            "('word-sense', 'disambiguation') 1\n",
            "('disambiguation', '-lrb-') 1\n",
            "('-lrb-', 'wsd') 1\n",
            "('wsd', '-rrb-') 1\n",
            "('-rrb-', 'is') 5\n",
            "('is', 'an') 2\n",
            "('an', 'open') 1\n",
            "('open', 'problem') 1\n",
            "('problem', 'of') 2\n",
            "('of', 'natural') 1\n",
            "('natural', 'language') 2\n",
            "('language', 'processing') 2\n",
            "('processing', ',') 2\n",
            "(',', 'which') 8\n",
            "('which', 'governs') 1\n",
            "('governs', 'the') 1\n",
            "('the', 'process') 2\n",
            "('process', 'of') 1\n",
            "('of', 'identifying') 1\n",
            "('identifying', 'which') 1\n",
            "('which', 'sense') 3\n",
            "('sense', 'of') 3\n",
            "('of', 'a') 10\n",
            "('a', 'word') 8\n",
            "('word', '-lrb-') 2\n",
            "('-lrb-', 'i.e') 2\n",
            "('i.e', '.') 2\n",
            "('.', 'meaning') 1\n",
            "('meaning', '-rrb-') 1\n",
            "('is', 'used') 3\n",
            "('used', 'in') 3\n",
            "('in', 'a') 9\n",
            "('a', 'sentence') 1\n",
            "('sentence', ',') 2\n",
            "(',', 'when') 2\n",
            "('when', 'the') 1\n",
            "('the', 'word') 11\n",
            "('word', 'has') 1\n",
            "('has', 'multiple') 1\n",
            "('multiple', 'meanings') 1\n",
            "('meanings', '-lrb-') 1\n",
            "('-lrb-', 'polysemy') 1\n",
            "('polysemy', '-rrb-') 1\n",
            "('-rrb-', '.') 19\n",
            "('.', 'the') 22\n",
            "('the', 'solution') 1\n",
            "('solution', 'to') 2\n",
            "('to', 'this') 1\n",
            "('this', 'problem') 1\n",
            "('problem', 'impacts') 1\n",
            "('impacts', 'other') 1\n",
            "('other', 'computer-related') 1\n",
            "('computer-related', 'writing') 1\n",
            "('writing', ',') 1\n",
            "(',', 'such') 4\n",
            "('such', 'as') 13\n",
            "('as', 'discourse') 1\n",
            "('discourse', ',') 1\n",
            "(',', 'improving') 1\n",
            "('improving', 'relevance') 1\n",
            "('relevance', 'of') 1\n",
            "('of', 'search') 1\n",
            "('search', 'engines') 2\n",
            "('engines', ',') 1\n",
            "(',', 'anaphora') 1\n",
            "('anaphora', 'resolution') 1\n",
            "('resolution', ',') 1\n",
            "(',', 'coherence') 1\n",
            "('coherence', ',') 1\n",
            "(',', 'inference') 1\n",
            "('inference', 'et') 1\n",
            "('et', 'cetera') 1\n",
            "('cetera', '.') 1\n",
            "('.', 'research') 1\n",
            "('research', 'has') 1\n",
            "('has', 'progressed') 1\n",
            "('progressed', 'steadily') 1\n",
            "('steadily', 'to') 1\n",
            "('to', 'the') 8\n",
            "('the', 'point') 1\n",
            "('point', 'where') 1\n",
            "('where', 'wsd') 1\n",
            "('wsd', 'systems') 6\n",
            "('systems', 'achieve') 1\n",
            "('achieve', 'sufficiently') 1\n",
            "('sufficiently', 'high') 1\n",
            "('high', 'levels') 1\n",
            "('levels', 'of') 1\n",
            "('of', 'accuracy') 1\n",
            "('accuracy', 'on') 1\n",
            "('on', 'a') 5\n",
            "('a', 'variety') 1\n",
            "('variety', 'of') 3\n",
            "('of', 'word') 6\n",
            "('word', 'types') 1\n",
            "('types', 'and') 1\n",
            "('and', 'ambiguities') 1\n",
            "('ambiguities', '.') 1\n",
            "('.', 'a') 3\n",
            "('a', 'rich') 1\n",
            "('rich', 'variety') 1\n",
            "('of', 'techniques') 1\n",
            "('techniques', 'have') 2\n",
            "('have', 'been') 9\n",
            "('been', 'researched') 1\n",
            "('researched', ',') 1\n",
            "(',', 'from') 1\n",
            "('from', 'dictionary-based') 1\n",
            "('dictionary-based', 'methods') 1\n",
            "('methods', 'that') 2\n",
            "('that', 'use') 1\n",
            "('use', 'the') 1\n",
            "('the', 'knowledge') 4\n",
            "('knowledge', 'encoded') 1\n",
            "('encoded', 'in') 1\n",
            "('in', 'lexical') 1\n",
            "('lexical', 'resources') 2\n",
            "('resources', ',') 3\n",
            "(',', 'to') 7\n",
            "('to', 'supervised') 1\n",
            "('supervised', 'machine') 2\n",
            "('machine', 'learning') 4\n",
            "('learning', 'methods') 1\n",
            "('methods', 'in') 1\n",
            "('in', 'which') 5\n",
            "('which', 'a') 1\n",
            "('a', 'classifier') 1\n",
            "('classifier', 'is') 2\n",
            "('is', 'trained') 1\n",
            "('trained', 'for') 1\n",
            "('for', 'each') 4\n",
            "('each', 'distinct') 1\n",
            "('distinct', 'word') 1\n",
            "('word', 'on') 1\n",
            "('a', 'corpus') 2\n",
            "('corpus', 'of') 3\n",
            "('of', 'manually') 3\n",
            "('manually', 'sense-annotated') 1\n",
            "('sense-annotated', 'examples') 1\n",
            "('examples', ',') 1\n",
            "('to', 'completely') 1\n",
            "('completely', 'unsupervised') 1\n",
            "('unsupervised', 'methods') 4\n",
            "('that', 'cluster') 1\n",
            "('cluster', 'occurrences') 1\n",
            "('occurrences', 'of') 3\n",
            "('of', 'words') 7\n",
            "('words', ',') 4\n",
            "(',', 'thereby') 1\n",
            "('thereby', 'inducing') 1\n",
            "('inducing', 'word') 1\n",
            "('word', 'senses') 9\n",
            "('senses', '.') 6\n",
            "('.', 'among') 1\n",
            "('among', 'these') 1\n",
            "('these', ',') 1\n",
            "(',', 'supervised') 2\n",
            "('supervised', 'learning') 3\n",
            "('learning', 'approaches') 1\n",
            "('approaches', 'have') 3\n",
            "('been', 'the') 1\n",
            "('the', 'most') 6\n",
            "('most', 'successful') 2\n",
            "('successful', 'algorithms') 1\n",
            "('algorithms', 'to') 2\n",
            "('to', 'date') 2\n",
            "('date', '.') 1\n",
            "('.', 'current') 1\n",
            "('current', 'accuracy') 1\n",
            "('accuracy', 'is') 1\n",
            "('is', 'difficult') 1\n",
            "('difficult', 'to') 1\n",
            "('to', 'state') 1\n",
            "('state', 'without') 1\n",
            "('without', 'a') 1\n",
            "('a', 'host') 1\n",
            "('host', 'of') 1\n",
            "('of', 'caveats') 1\n",
            "('caveats', '.') 1\n",
            "('.', 'in') 13\n",
            "('in', 'english') 1\n",
            "('english', ',') 2\n",
            "(',', 'accuracy') 1\n",
            "('accuracy', 'at') 1\n",
            "('at', 'the') 4\n",
            "('the', 'coarse-grained') 2\n",
            "('coarse-grained', '-lrb-') 1\n",
            "('-lrb-', 'homograph') 1\n",
            "('homograph', '-rrb-') 1\n",
            "('-rrb-', 'level') 1\n",
            "('level', 'is') 1\n",
            "('is', 'routinely') 1\n",
            "('routinely', 'above') 1\n",
            "('above', '90') 1\n",
            "('90', '%') 1\n",
            "('%', ',') 2\n",
            "(',', 'with') 1\n",
            "('with', 'some') 2\n",
            "('some', 'methods') 1\n",
            "('methods', 'on') 1\n",
            "('on', 'particular') 1\n",
            "('particular', 'homographs') 1\n",
            "('homographs', 'achieving') 1\n",
            "('achieving', 'over') 1\n",
            "('over', '96') 1\n",
            "('96', '%') 1\n",
            "('%', '.') 1\n",
            "('.', 'on') 1\n",
            "('on', 'finer-grained') 1\n",
            "('finer-grained', 'sense') 1\n",
            "('sense', 'distinctions') 3\n",
            "('distinctions', ',') 4\n",
            "(',', 'top') 1\n",
            "('top', 'accuracies') 1\n",
            "('accuracies', 'from') 1\n",
            "('from', '59.1') 1\n",
            "('59.1', '%') 1\n",
            "('%', 'to') 1\n",
            "('to', '69.0') 1\n",
            "('69.0', '%') 1\n",
            "('%', 'have') 1\n",
            "('been', 'reported') 2\n",
            "('reported', 'in') 1\n",
            "('in', 'recent') 3\n",
            "('recent', 'evaluation') 1\n",
            "('evaluation', 'exercises') 2\n",
            "('exercises', '-lrb-') 1\n",
            "('-lrb-', 'semeval-2007') 1\n",
            "('semeval-2007', ',') 1\n",
            "(',', 'senseval-2') 2\n",
            "('senseval-2', '-rrb-') 1\n",
            "('-rrb-', ',') 9\n",
            "(',', 'where') 1\n",
            "('where', 'the') 1\n",
            "('the', 'baseline') 1\n",
            "('baseline', 'accuracy') 1\n",
            "('accuracy', 'of') 1\n",
            "('of', 'the') 24\n",
            "('the', 'simplest') 1\n",
            "('simplest', 'possible') 1\n",
            "('possible', 'algorithm') 1\n",
            "('algorithm', 'of') 1\n",
            "('of', 'always') 1\n",
            "('always', 'choosing') 1\n",
            "('choosing', 'the') 1\n",
            "('most', 'frequent') 2\n",
            "('frequent', 'sense') 2\n",
            "('sense', 'was') 1\n",
            "('was', '51.4') 1\n",
            "('51.4', '%') 1\n",
            "('%', 'and') 1\n",
            "('and', '57') 1\n",
            "('57', '%') 1\n",
            "(',', 'respectively') 1\n",
            "('respectively', '.') 1\n",
            "('.', 'wsd') 4\n",
            "('wsd', 'task') 1\n",
            "('task', 'has') 1\n",
            "('has', 'two') 1\n",
            "('two', 'variants') 1\n",
            "('variants', ':') 1\n",
            "(':', '``') 2\n",
            "('``', 'lexical') 1\n",
            "('lexical', 'sample') 5\n",
            "('sample', \"''\") 1\n",
            "(\"''\", 'and') 3\n",
            "('and', '``') 3\n",
            "('``', 'all') 1\n",
            "('all', 'words') 1\n",
            "('words', \"''\") 1\n",
            "(\"''\", 'task') 1\n",
            "('task', '.') 2\n",
            "('the', 'former') 4\n",
            "('former', 'comprises') 1\n",
            "('comprises', 'disambiguating') 1\n",
            "('disambiguating', 'the') 2\n",
            "('the', 'occurrences') 1\n",
            "('a', 'small') 4\n",
            "('small', 'sample') 1\n",
            "('sample', 'of') 1\n",
            "('of', 'target') 2\n",
            "('target', 'words') 2\n",
            "('words', 'which') 1\n",
            "('which', 'were') 1\n",
            "('were', 'previously') 1\n",
            "('previously', 'selected') 1\n",
            "('selected', ',') 1\n",
            "(',', 'while') 3\n",
            "('while', 'in') 1\n",
            "('in', 'the') 28\n",
            "('the', 'latter') 4\n",
            "('latter', 'all') 1\n",
            "('all', 'the') 2\n",
            "('the', 'words') 6\n",
            "('words', 'in') 4\n",
            "('a', 'piece') 1\n",
            "('piece', 'of') 1\n",
            "('of', 'running') 1\n",
            "('running', 'text') 1\n",
            "('text', 'need') 1\n",
            "('need', 'to') 2\n",
            "('to', 'be') 7\n",
            "('be', 'disambiguated') 4\n",
            "('disambiguated', '.') 2\n",
            "('latter', 'is') 1\n",
            "('is', 'deemed') 1\n",
            "('deemed', 'a') 1\n",
            "('a', 'more') 1\n",
            "('more', 'realistic') 2\n",
            "('realistic', 'form') 1\n",
            "('form', 'of') 4\n",
            "('of', 'evaluation') 1\n",
            "('evaluation', ',') 2\n",
            "(',', 'but') 10\n",
            "('but', 'the') 1\n",
            "('the', 'corpus') 4\n",
            "('corpus', 'is') 3\n",
            "('is', 'more') 2\n",
            "('more', 'expensive') 1\n",
            "('expensive', 'to') 2\n",
            "('to', 'produce') 1\n",
            "('produce', 'because') 1\n",
            "('because', 'human') 1\n",
            "('human', 'annotators') 2\n",
            "('annotators', 'have') 1\n",
            "('have', 'to') 1\n",
            "('to', 'read') 1\n",
            "('read', 'the') 1\n",
            "('the', 'definitions') 4\n",
            "('definitions', 'for') 1\n",
            "('each', 'word') 3\n",
            "('word', 'in') 3\n",
            "('the', 'sequence') 1\n",
            "('sequence', 'every') 1\n",
            "('every', 'time') 1\n",
            "('time', 'they') 1\n",
            "('they', 'need') 1\n",
            "('to', 'make') 1\n",
            "('make', 'a') 1\n",
            "('a', 'tagging') 1\n",
            "('tagging', 'judgement') 1\n",
            "('judgement', ',') 1\n",
            "(',', 'rather') 1\n",
            "('rather', 'than') 1\n",
            "('than', 'once') 1\n",
            "('once', 'for') 1\n",
            "('for', 'a') 4\n",
            "('a', 'block') 1\n",
            "('block', 'of') 1\n",
            "('of', 'instances') 1\n",
            "('instances', 'for') 1\n",
            "('for', 'the') 4\n",
            "('the', 'same') 4\n",
            "('same', 'target') 1\n",
            "('target', 'word') 2\n",
            "('word', '.') 2\n",
            "('.', 'to') 3\n",
            "('to', 'give') 2\n",
            "('give', 'a') 2\n",
            "('a', 'hint') 1\n",
            "('hint', 'how') 1\n",
            "('how', 'all') 1\n",
            "('all', 'this') 1\n",
            "('this', 'works') 1\n",
            "('works', ',') 1\n",
            "(',', 'consider') 1\n",
            "('consider', 'two') 1\n",
            "('two', 'examples') 1\n",
            "('examples', 'of') 1\n",
            "('the', 'distinct') 1\n",
            "('distinct', 'senses') 1\n",
            "('senses', 'that') 1\n",
            "('that', 'exist') 1\n",
            "('exist', 'for') 1\n",
            "('the', '-lrb-') 1\n",
            "('-lrb-', 'written') 1\n",
            "('written', '-rrb-') 1\n",
            "('-rrb-', 'word') 1\n",
            "('word', '``') 3\n",
            "('``', 'bass') 5\n",
            "('bass', \"''\") 2\n",
            "(\"''\", ':') 1\n",
            "(':', 'a') 1\n",
            "('a', 'type') 2\n",
            "('type', 'of') 2\n",
            "('of', 'fish') 3\n",
            "('fish', 'tones') 1\n",
            "('tones', 'of') 1\n",
            "('of', 'low') 1\n",
            "('low', 'frequency') 3\n",
            "('frequency', 'and') 1\n",
            "('and', 'the') 5\n",
            "('the', 'sentences') 1\n",
            "('sentences', ':') 2\n",
            "(':', 'i') 1\n",
            "('i', 'went') 1\n",
            "('went', 'fishing') 1\n",
            "('fishing', 'for') 2\n",
            "('for', 'some') 1\n",
            "('some', 'sea') 1\n",
            "('sea', 'bass') 1\n",
            "('bass', '.') 1\n",
            "('the', 'bass') 1\n",
            "('bass', 'line') 1\n",
            "('line', 'of') 1\n",
            "('the', 'song') 1\n",
            "('song', 'is') 1\n",
            "('is', 'too') 1\n",
            "('too', 'weak') 1\n",
            "('weak', '.') 1\n",
            "('to', 'a') 6\n",
            "('a', 'human') 3\n",
            "('human', ',') 1\n",
            "(',', 'it') 10\n",
            "('it', 'is') 12\n",
            "('is', 'obvious') 1\n",
            "('obvious', 'that') 1\n",
            "('that', 'the') 6\n",
            "('the', 'first') 3\n",
            "('first', 'sentence') 1\n",
            "('sentence', 'is') 1\n",
            "('is', 'using') 2\n",
            "('using', 'the') 2\n",
            "('bass', '-lrb-') 3\n",
            "('-lrb-', 'fish') 1\n",
            "('fish', '-rrb-') 1\n",
            "('-rrb-', \"''\") 3\n",
            "(\"''\", ',') 4\n",
            "(',', 'as') 6\n",
            "('as', 'in') 3\n",
            "('former', 'sense') 1\n",
            "('sense', 'above') 1\n",
            "('above', 'and') 1\n",
            "('and', 'in') 4\n",
            "('the', 'second') 2\n",
            "('second', 'sentence') 1\n",
            "(',', 'the') 11\n",
            "('-lrb-', 'instrument') 1\n",
            "('instrument', '-rrb-') 4\n",
            "(\"''\", 'is') 2\n",
            "('is', 'being') 1\n",
            "('being', 'used') 1\n",
            "('used', 'as') 2\n",
            "('latter', 'sense') 1\n",
            "('sense', 'below') 1\n",
            "('below', '.') 1\n",
            "('.', 'developing') 1\n",
            "('developing', 'algorithms') 1\n",
            "('to', 'replicate') 1\n",
            "('replicate', 'this') 1\n",
            "('this', 'human') 1\n",
            "('human', 'ability') 1\n",
            "('ability', 'can') 1\n",
            "('can', 'often') 1\n",
            "('often', 'be') 1\n",
            "('be', 'a') 1\n",
            "('a', 'difficult') 1\n",
            "('difficult', 'task') 1\n",
            "('task', ',') 1\n",
            "('as', 'is') 1\n",
            "('is', 'further') 1\n",
            "('further', 'exemplified') 1\n",
            "('exemplified', 'by') 1\n",
            "('by', 'the') 4\n",
            "('the', 'implicit') 1\n",
            "('implicit', 'equivocation') 1\n",
            "('equivocation', 'between') 1\n",
            "('between', '``') 1\n",
            "('-lrb-', 'sound') 1\n",
            "('sound', '-rrb-') 1\n",
            "(\"''\", '-lrb-') 2\n",
            "('-lrb-', 'musical') 1\n",
            "('musical', 'instrument') 3\n",
            "('.', 'history') 1\n",
            "('history', 'wsd') 1\n",
            "('wsd', 'was') 2\n",
            "('was', 'first') 1\n",
            "('first', 'formulated') 1\n",
            "('formulated', 'as') 1\n",
            "('as', 'a') 4\n",
            "('a', 'distinct') 1\n",
            "('distinct', 'computational') 1\n",
            "('computational', 'task') 1\n",
            "('task', 'during') 1\n",
            "('during', 'the') 3\n",
            "('the', 'early') 2\n",
            "('early', 'days') 2\n",
            "('days', 'of') 2\n",
            "('of', 'machine') 1\n",
            "('machine', 'translation') 3\n",
            "('translation', 'in') 1\n",
            "('the', '1940s') 1\n",
            "('1940s', ',') 1\n",
            "(',', 'making') 2\n",
            "('making', 'it') 2\n",
            "('it', 'one') 1\n",
            "('one', 'of') 2\n",
            "('the', 'oldest') 1\n",
            "('oldest', 'problems') 1\n",
            "('problems', 'in') 1\n",
            "('linguistics', '.') 1\n",
            "('.', 'warren') 1\n",
            "('warren', 'weaver') 1\n",
            "('weaver', ',') 1\n",
            "(',', 'in') 4\n",
            "('in', 'his') 1\n",
            "('his', 'famous') 1\n",
            "('famous', '1949') 1\n",
            "('1949', 'memorandum') 1\n",
            "('memorandum', 'on') 1\n",
            "('on', 'translation') 1\n",
            "('translation', ',') 3\n",
            "(',', 'first') 1\n",
            "('first', 'introduced') 1\n",
            "('introduced', 'the') 1\n",
            "('the', 'problem') 3\n",
            "('problem', 'in') 1\n",
            "('a', 'computational') 2\n",
            "('computational', 'context') 1\n",
            "('context', '.') 1\n",
            "('.', 'early') 1\n",
            "('early', 'researchers') 1\n",
            "('researchers', 'understood') 1\n",
            "('understood', 'the') 1\n",
            "('the', 'significance') 1\n",
            "('significance', 'and') 1\n",
            "('and', 'difficulty') 1\n",
            "('difficulty', 'of') 1\n",
            "('of', 'wsd') 5\n",
            "('wsd', 'well') 1\n",
            "('well', '.') 1\n",
            "('in', 'fact') 1\n",
            "('fact', ',') 1\n",
            "(',', 'bar-hillel') 1\n",
            "('bar-hillel', '-lrb-') 1\n",
            "('-lrb-', '1960') 1\n",
            "('1960', '-rrb-') 1\n",
            "('-rrb-', 'used') 1\n",
            "('used', 'the') 1\n",
            "('the', 'above') 1\n",
            "('above', 'example') 1\n",
            "('example', 'to') 1\n",
            "('to', 'argue') 1\n",
            "('argue', 'that') 2\n",
            "('that', 'wsd') 1\n",
            "('wsd', 'could') 1\n",
            "('could', 'not') 1\n",
            "('not', 'be') 2\n",
            "('be', 'solved') 1\n",
            "('solved', 'by') 1\n",
            "('by', '``') 1\n",
            "('``', 'electronic') 1\n",
            "('electronic', 'computer') 1\n",
            "('computer', \"''\") 1\n",
            "(\"''\", 'because') 1\n",
            "('because', 'of') 3\n",
            "('the', 'need') 1\n",
            "('need', 'in') 1\n",
            "('in', 'general') 1\n",
            "('general', 'to') 1\n",
            "('to', 'model') 1\n",
            "('model', 'all') 1\n",
            "('all', 'world') 1\n",
            "('world', 'knowledge') 5\n",
            "('knowledge', '.') 4\n",
            "('the', '1970s') 1\n",
            "('1970s', ',') 1\n",
            "(',', 'wsd') 1\n",
            "('was', 'a') 2\n",
            "('a', 'subtask') 1\n",
            "('subtask', 'of') 1\n",
            "('of', 'semantic') 2\n",
            "('semantic', 'interpretation') 1\n",
            "('interpretation', 'systems') 1\n",
            "('systems', 'developed') 1\n",
            "('developed', 'within') 1\n",
            "('within', 'the') 1\n",
            "('the', 'field') 2\n",
            "('field', 'of') 2\n",
            "('of', 'artificial') 1\n",
            "('artificial', 'intelligence') 1\n",
            "('intelligence', ',') 1\n",
            "('but', 'since') 1\n",
            "('since', 'wsd') 1\n",
            "('systems', 'were') 3\n",
            "('were', 'largely') 1\n",
            "('largely', 'rule-based') 1\n",
            "('rule-based', 'and') 1\n",
            "('and', 'hand-coded') 1\n",
            "('hand-coded', 'they') 1\n",
            "('they', 'were') 1\n",
            "('were', 'prone') 1\n",
            "('prone', 'to') 1\n",
            "('a', 'knowledge') 1\n",
            "('knowledge', 'acquisition') 4\n",
            "('acquisition', 'bottleneck') 4\n",
            "('bottleneck', '.') 1\n",
            "('.', 'by') 1\n",
            "('the', '1980s') 1\n",
            "('1980s', 'large-scale') 1\n",
            "('large-scale', 'lexical') 1\n",
            "('as', 'the') 3\n",
            "('the', 'oxford') 1\n",
            "('oxford', 'advanced') 1\n",
            "('advanced', 'learner') 1\n",
            "('learner', \"'s\") 1\n",
            "(\"'s\", 'dictionary') 1\n",
            "('dictionary', 'of') 2\n",
            "('of', 'current') 1\n",
            "('current', 'english') 1\n",
            "('english', '-lrb-') 1\n",
            "('-lrb-', 'oald') 1\n",
            "('oald', '-rrb-') 1\n",
            "(',', 'became') 1\n",
            "('became', 'available') 1\n",
            "('available', ':') 1\n",
            "(':', 'hand-coding') 1\n",
            "('hand-coding', 'was') 1\n",
            "('was', 'replaced') 1\n",
            "('replaced', 'with') 1\n",
            "('with', 'knowledge') 1\n",
            "('knowledge', 'automatically') 1\n",
            "('automatically', 'extracted') 1\n",
            "('extracted', 'from') 1\n",
            "('from', 'these') 1\n",
            "('these', 'resources') 1\n",
            "('but', 'disambiguation') 1\n",
            "('disambiguation', 'was') 1\n",
            "('was', 'still') 1\n",
            "('still', 'knowledge-based') 1\n",
            "('knowledge-based', 'or') 1\n",
            "('or', 'dictionary-based') 1\n",
            "('dictionary-based', '.') 1\n",
            "('the', '1990s') 2\n",
            "('1990s', ',') 1\n",
            "('the', 'statistical') 1\n",
            "('statistical', 'revolution') 1\n",
            "('revolution', 'swept') 1\n",
            "('swept', 'through') 1\n",
            "('through', 'computational') 1\n",
            "(',', 'and') 21\n",
            "('and', 'wsd') 1\n",
            "('wsd', 'became') 1\n",
            "('became', 'a') 1\n",
            "('a', 'paradigm') 1\n",
            "('paradigm', 'problem') 1\n",
            "('problem', 'on') 1\n",
            "('on', 'which') 3\n",
            "('which', 'to') 1\n",
            "('to', 'apply') 1\n",
            "('apply', 'supervised') 1\n",
            "('learning', 'techniques') 1\n",
            "('techniques', '.') 1\n",
            "('the', '2000s') 1\n",
            "('2000s', 'saw') 1\n",
            "('saw', 'supervised') 1\n",
            "('supervised', 'techniques') 1\n",
            "('techniques', 'reach') 1\n",
            "('reach', 'a') 1\n",
            "('a', 'plateau') 1\n",
            "('plateau', 'in') 1\n",
            "('in', 'accuracy') 1\n",
            "('accuracy', ',') 1\n",
            "('and', 'so') 1\n",
            "('so', 'attention') 1\n",
            "('attention', 'has') 1\n",
            "('has', 'shifted') 1\n",
            "('shifted', 'to') 1\n",
            "('to', 'coarser-grained') 1\n",
            "('coarser-grained', 'senses') 1\n",
            "('senses', ',') 3\n",
            "(',', 'domain') 2\n",
            "('domain', 'adaptation') 1\n",
            "('adaptation', ',') 1\n",
            "(',', 'semi-supervised') 1\n",
            "('semi-supervised', 'and') 1\n",
            "('and', 'unsupervised') 1\n",
            "('unsupervised', 'corpus-based') 1\n",
            "('corpus-based', 'systems') 1\n",
            "('systems', ',') 2\n",
            "(',', 'combinations') 1\n",
            "('combinations', 'of') 1\n",
            "('of', 'different') 1\n",
            "('different', 'methods') 1\n",
            "('methods', ',') 3\n",
            "('the', 'return') 1\n",
            "('return', 'of') 1\n",
            "('of', 'knowledge-based') 1\n",
            "('knowledge-based', 'systems') 1\n",
            "('systems', 'via') 1\n",
            "('via', 'graph-based') 1\n",
            "('graph-based', 'methods') 2\n",
            "('methods', '.') 1\n",
            "('.', 'still') 1\n",
            "('still', ',') 1\n",
            "('supervised', 'systems') 2\n",
            "('systems', 'continue') 1\n",
            "('continue', 'to') 2\n",
            "('to', 'perform') 2\n",
            "('perform', 'best') 1\n",
            "('best', '.') 1\n",
            "('.', 'difficulties') 1\n",
            "('difficulties', 'differences') 1\n",
            "('differences', 'between') 1\n",
            "('between', 'dictionaries') 1\n",
            "('dictionaries', 'one') 1\n",
            "('one', 'problem') 1\n",
            "('problem', 'with') 2\n",
            "('with', 'word') 2\n",
            "('word', 'sense') 13\n",
            "('sense', 'disambiguation') 6\n",
            "('disambiguation', 'is') 1\n",
            "('is', 'deciding') 1\n",
            "('deciding', 'what') 1\n",
            "('what', 'the') 1\n",
            "('the', 'senses') 2\n",
            "('senses', 'are') 2\n",
            "('are', '.') 1\n",
            "('in', 'cases') 1\n",
            "('cases', 'like') 1\n",
            "('like', 'the') 2\n",
            "('word', 'bass') 2\n",
            "('bass', 'above') 1\n",
            "('above', ',') 2\n",
            "(',', 'at') 2\n",
            "('at', 'least') 2\n",
            "('least', 'some') 1\n",
            "('some', 'senses') 1\n",
            "('are', 'obviously') 1\n",
            "('obviously', 'different') 1\n",
            "('different', '.') 1\n",
            "('in', 'other') 1\n",
            "('other', 'cases') 1\n",
            "('cases', ',') 2\n",
            "(',', 'however') 4\n",
            "('however', ',') 12\n",
            "('the', 'different') 2\n",
            "('different', 'senses') 1\n",
            "('senses', 'can') 2\n",
            "('can', 'be') 13\n",
            "('be', 'closely') 1\n",
            "('closely', 'related') 2\n",
            "('related', '-lrb-') 1\n",
            "('-lrb-', 'one') 1\n",
            "('one', 'meaning') 1\n",
            "('meaning', 'being') 1\n",
            "('being', 'a') 1\n",
            "('a', 'metaphorical') 1\n",
            "('metaphorical', 'or') 1\n",
            "('or', 'metonymic') 1\n",
            "('metonymic', 'extension') 1\n",
            "('extension', 'of') 1\n",
            "('of', 'another') 1\n",
            "('another', '-rrb-') 1\n",
            "('in', 'such') 1\n",
            "('such', 'cases') 1\n",
            "('cases', 'division') 1\n",
            "('division', 'of') 2\n",
            "('words', 'into') 2\n",
            "('into', 'senses') 3\n",
            "('senses', 'becomes') 1\n",
            "('becomes', 'much') 1\n",
            "('much', 'more') 2\n",
            "('more', 'difficult') 2\n",
            "('difficult', '.') 2\n",
            "('.', 'different') 1\n",
            "('different', 'dictionaries') 1\n",
            "('dictionaries', 'and') 2\n",
            "('and', 'thesauruses') 1\n",
            "('thesauruses', 'will') 1\n",
            "('will', 'provide') 1\n",
            "('provide', 'different') 1\n",
            "('different', 'divisions') 1\n",
            "('divisions', 'of') 1\n",
            "('.', 'one') 1\n",
            "('one', 'solution') 1\n",
            "('solution', 'some') 1\n",
            "('some', 'researchers') 1\n",
            "('researchers', 'have') 1\n",
            "('have', 'used') 1\n",
            "('used', 'is') 1\n",
            "('is', 'to') 4\n",
            "('to', 'choose') 2\n",
            "('choose', 'a') 1\n",
            "('a', 'particular') 1\n",
            "('particular', 'dictionary') 1\n",
            "('dictionary', ',') 2\n",
            "('and', 'just') 1\n",
            "('just', 'use') 1\n",
            "('use', 'its') 1\n",
            "('its', 'set') 1\n",
            "('set', 'of') 4\n",
            "('of', 'senses') 4\n",
            "('.', 'generally') 1\n",
            "('generally', ',') 1\n",
            "(',', 'research') 1\n",
            "('research', 'results') 1\n",
            "('results', 'using') 1\n",
            "('using', 'broad') 1\n",
            "('broad', 'distinctions') 1\n",
            "('distinctions', 'in') 2\n",
            "('in', 'senses') 1\n",
            "('senses', 'have') 1\n",
            "('been', 'much') 1\n",
            "('much', 'better') 2\n",
            "('better', 'than') 2\n",
            "('than', 'those') 1\n",
            "('those', 'using') 1\n",
            "('using', 'narrow') 1\n",
            "('narrow', 'ones') 1\n",
            "('ones', '.') 1\n",
            "('.', 'however') 8\n",
            "(',', 'given') 2\n",
            "('given', 'the') 2\n",
            "('the', 'lack') 4\n",
            "('lack', 'of') 5\n",
            "('a', 'full-fledged') 1\n",
            "('full-fledged', 'coarse-grained') 1\n",
            "('coarse-grained', 'sense') 1\n",
            "('sense', 'inventory') 10\n",
            "('inventory', ',') 1\n",
            "(',', 'most') 1\n",
            "('most', 'researchers') 1\n",
            "('researchers', 'continue') 1\n",
            "('to', 'work') 2\n",
            "('work', 'on') 1\n",
            "('on', 'fine-grained') 1\n",
            "('fine-grained', 'wsd') 1\n",
            "('wsd', '.') 3\n",
            "('.', 'most') 2\n",
            "('most', 'research') 1\n",
            "('research', 'in') 1\n",
            "('wsd', 'is') 1\n",
            "('is', 'performed') 1\n",
            "('performed', 'by') 1\n",
            "('by', 'using') 1\n",
            "('using', 'wordnet') 1\n",
            "('wordnet', 'as') 1\n",
            "('a', 'reference') 1\n",
            "('reference', 'sense') 1\n",
            "('inventory', 'for') 1\n",
            "('for', 'english') 3\n",
            "('english', '.') 1\n",
            "('.', 'wordnet') 2\n",
            "('wordnet', 'is') 2\n",
            "('is', 'a') 3\n",
            "('computational', 'lexicon') 1\n",
            "('lexicon', 'that') 1\n",
            "('that', 'encodes') 1\n",
            "('encodes', 'concepts') 1\n",
            "('concepts', 'as') 1\n",
            "('as', 'synonym') 1\n",
            "('synonym', 'sets') 1\n",
            "('sets', '-lrb-') 1\n",
            "('-lrb-', 'e.g') 4\n",
            "('e.g', '.') 4\n",
            "('the', 'concept') 1\n",
            "('concept', 'of') 1\n",
            "('of', 'car') 1\n",
            "('car', 'is') 1\n",
            "('is', 'encoded') 1\n",
            "('encoded', 'as') 1\n",
            "('as', '-lcb-') 1\n",
            "('-lcb-', 'car') 1\n",
            "('car', ',') 1\n",
            "(',', 'auto') 1\n",
            "('auto', ',') 1\n",
            "(',', 'automobile') 1\n",
            "('automobile', ',') 1\n",
            "(',', 'machine') 1\n",
            "('machine', ',') 1\n",
            "(',', 'motorcar') 1\n",
            "('motorcar', '-rcb-') 1\n",
            "('-rcb-', '-rrb-') 1\n",
            "('.', 'other') 3\n",
            "('other', 'resources') 2\n",
            "('resources', 'used') 1\n",
            "('used', 'for') 2\n",
            "('for', 'disambiguation') 1\n",
            "('disambiguation', 'purposes') 1\n",
            "('purposes', 'include') 1\n",
            "('include', 'roget') 1\n",
            "('roget', \"'s\") 2\n",
            "(\"'s\", 'thesaurus') 2\n",
            "('thesaurus', 'and') 2\n",
            "('and', 'wikipedia') 1\n",
            "('wikipedia', '.') 1\n",
            "('.', 'part-of-speech') 1\n",
            "('part-of-speech', 'tagging') 4\n",
            "('tagging', 'in') 1\n",
            "('in', 'any') 1\n",
            "('any', 'real') 1\n",
            "('real', 'test') 1\n",
            "('test', ',') 1\n",
            "(',', 'part-of-speech') 1\n",
            "('tagging', 'and') 1\n",
            "('and', 'sense') 1\n",
            "('sense', 'tagging') 1\n",
            "('tagging', 'are') 1\n",
            "('are', 'very') 1\n",
            "('very', 'closely') 1\n",
            "('related', 'with') 1\n",
            "('with', 'each') 1\n",
            "('each', 'potentially') 1\n",
            "('potentially', 'making') 1\n",
            "('making', 'constraints') 1\n",
            "('constraints', 'to') 1\n",
            "('the', 'other') 2\n",
            "('other', '.') 1\n",
            "('.', 'and') 2\n",
            "('the', 'question') 1\n",
            "('question', 'whether') 1\n",
            "('whether', 'these') 1\n",
            "('these', 'tasks') 1\n",
            "('tasks', 'should') 1\n",
            "('should', 'be') 2\n",
            "('be', 'kept') 1\n",
            "('kept', 'together') 1\n",
            "('together', 'or') 1\n",
            "('or', 'decoupled') 1\n",
            "('decoupled', 'is') 1\n",
            "('is', 'still') 1\n",
            "('still', 'not') 1\n",
            "('not', 'unanimously') 1\n",
            "('unanimously', 'resolved') 1\n",
            "('resolved', ',') 1\n",
            "('but', 'recently') 1\n",
            "('recently', 'scientists') 1\n",
            "('scientists', 'incline') 1\n",
            "('incline', 'to') 1\n",
            "('to', 'test') 4\n",
            "('test', 'these') 1\n",
            "('these', 'things') 1\n",
            "('things', 'separately') 1\n",
            "('separately', '-lrb-') 1\n",
            "('the', 'senseval\\\\/semeval') 1\n",
            "('senseval\\\\/semeval', 'competitions') 1\n",
            "('competitions', 'parts') 1\n",
            "('parts', 'of') 4\n",
            "('of', 'speech') 6\n",
            "('speech', 'are') 1\n",
            "('are', 'provided') 1\n",
            "('provided', 'as') 1\n",
            "('as', 'input') 1\n",
            "('input', 'for') 1\n",
            "('the', 'text') 4\n",
            "('text', 'to') 1\n",
            "('to', 'disambiguate') 3\n",
            "('disambiguate', '-rrb-') 1\n",
            "('.', 'it') 7\n",
            "('is', 'instructive') 1\n",
            "('instructive', 'to') 1\n",
            "('to', 'compare') 1\n",
            "('compare', 'the') 1\n",
            "('disambiguation', 'problem') 1\n",
            "('with', 'the') 3\n",
            "('of', 'part-of-speech') 1\n",
            "('tagging', '.') 1\n",
            "('.', 'both') 1\n",
            "('both', 'involve') 1\n",
            "('involve', 'disambiguating') 1\n",
            "('disambiguating', 'or') 1\n",
            "('or', 'tagging') 1\n",
            "('tagging', 'with') 1\n",
            "('with', 'words') 2\n",
            "(',', 'be') 1\n",
            "('be', 'it') 1\n",
            "('it', 'with') 1\n",
            "('with', 'senses') 1\n",
            "('senses', 'or') 1\n",
            "('or', 'parts') 1\n",
            "('speech', '.') 1\n",
            "(',', 'algorithms') 2\n",
            "('algorithms', 'used') 1\n",
            "('for', 'one') 1\n",
            "('one', 'do') 1\n",
            "('do', 'not') 2\n",
            "('not', 'tend') 1\n",
            "('tend', 'to') 2\n",
            "('work', 'well') 1\n",
            "('well', 'for') 1\n",
            "('other', ',') 1\n",
            "(',', 'mainly') 2\n",
            "('mainly', 'because') 2\n",
            "('because', 'the') 1\n",
            "('the', 'part') 1\n",
            "('part', 'of') 2\n",
            "('speech', 'of') 1\n",
            "('word', 'is') 3\n",
            "('is', 'primarily') 1\n",
            "('primarily', 'determined') 1\n",
            "('determined', 'by') 2\n",
            "('the', 'immediately') 1\n",
            "('immediately', 'adjacent') 1\n",
            "('adjacent', 'one') 1\n",
            "('one', 'to') 1\n",
            "('to', 'three') 1\n",
            "('three', 'words') 1\n",
            "(',', 'whereas') 1\n",
            "('whereas', 'the') 1\n",
            "('the', 'sense') 3\n",
            "('word', 'may') 1\n",
            "('may', 'be') 2\n",
            "('be', 'determined') 1\n",
            "('by', 'words') 1\n",
            "('words', 'further') 1\n",
            "('further', 'away') 1\n",
            "('away', '.') 1\n",
            "('the', 'success') 1\n",
            "('success', 'rate') 1\n",
            "('rate', 'for') 1\n",
            "('for', 'part-of-speech') 1\n",
            "('tagging', 'algorithms') 1\n",
            "('algorithms', 'is') 1\n",
            "('is', 'at') 1\n",
            "('at', 'present') 1\n",
            "('present', 'much') 1\n",
            "('much', 'higher') 1\n",
            "('higher', 'than') 1\n",
            "('than', 'that') 1\n",
            "('that', 'for') 1\n",
            "('for', 'wsd') 2\n",
            "('wsd', ',') 3\n",
            "(',', 'state-of-the') 1\n",
            "('state-of-the', 'art') 1\n",
            "('art', 'being') 1\n",
            "('being', 'around') 1\n",
            "('around', '95') 1\n",
            "('95', '%') 1\n",
            "('%', 'accuracy') 2\n",
            "('accuracy', 'or') 1\n",
            "('or', 'better') 1\n",
            "('better', ',') 1\n",
            "('as', 'compared') 1\n",
            "('compared', 'to') 1\n",
            "('to', 'less') 1\n",
            "('less', 'than') 1\n",
            "('than', '75') 1\n",
            "('75', '%') 1\n",
            "('accuracy', 'in') 1\n",
            "('in', 'word') 1\n",
            "('disambiguation', 'with') 1\n",
            "('with', 'supervised') 1\n",
            "('learning', '.') 3\n",
            "('.', 'these') 5\n",
            "('these', 'figures') 1\n",
            "('figures', 'are') 1\n",
            "('are', 'typical') 1\n",
            "('typical', 'for') 1\n",
            "('and', 'may') 1\n",
            "('be', 'very') 1\n",
            "('very', 'different') 1\n",
            "('different', 'from') 1\n",
            "('from', 'those') 1\n",
            "('those', 'for') 1\n",
            "('for', 'other') 1\n",
            "('other', 'languages') 1\n",
            "('languages', '.') 1\n",
            "('.', 'inter-judge') 1\n",
            "('inter-judge', 'variance') 2\n",
            "('variance', 'another') 1\n",
            "('another', 'problem') 1\n",
            "('problem', 'is') 1\n",
            "('is', 'inter-judge') 1\n",
            "('variance', '.') 1\n",
            "('systems', 'are') 1\n",
            "('are', 'normally') 1\n",
            "('normally', 'tested') 1\n",
            "('tested', 'by') 1\n",
            "('by', 'having') 1\n",
            "('having', 'their') 1\n",
            "('their', 'results') 1\n",
            "('results', 'on') 1\n",
            "('a', 'task') 4\n",
            "('task', 'compared') 1\n",
            "('compared', 'against') 1\n",
            "('against', 'those') 1\n",
            "('those', 'of') 1\n",
            "('human', '.') 1\n",
            "('while', 'it') 1\n",
            "('is', 'relatively') 1\n",
            "('relatively', 'easy') 1\n",
            "('easy', 'to') 1\n",
            "('to', 'assign') 1\n",
            "('assign', 'parts') 1\n",
            "('speech', 'to') 1\n",
            "('to', 'text') 1\n",
            "('text', ',') 2\n",
            "(',', 'training') 1\n",
            "('training', 'people') 1\n",
            "('people', 'to') 1\n",
            "('to', 'tag') 1\n",
            "('tag', 'senses') 1\n",
            "('senses', 'is') 2\n",
            "('is', 'far') 1\n",
            "('far', 'more') 1\n",
            "('.', 'while') 1\n",
            "('while', 'users') 1\n",
            "('users', 'can') 1\n",
            "('can', 'memorize') 1\n",
            "('memorize', 'all') 2\n",
            "('all', 'of') 2\n",
            "('the', 'possible') 1\n",
            "('possible', 'parts') 1\n",
            "('speech', 'a') 1\n",
            "('word', 'can') 3\n",
            "('can', 'take') 2\n",
            "('take', ',') 1\n",
            "('is', 'often') 2\n",
            "('often', 'impossible') 1\n",
            "('impossible', 'for') 1\n",
            "('for', 'individuals') 1\n",
            "('individuals', 'to') 1\n",
            "('to', 'memorize') 1\n",
            "('senses', 'a') 1\n",
            "('take', '.') 1\n",
            "('.', 'moreover') 2\n",
            "('moreover', ',') 2\n",
            "(',', 'humans') 1\n",
            "('humans', 'do') 1\n",
            "('not', 'agree') 1\n",
            "('agree', 'on') 2\n",
            "('on', 'the') 7\n",
            "('the', 'task') 3\n",
            "('task', 'at') 1\n",
            "('at', 'hand') 1\n",
            "('hand', '--') 1\n",
            "('--', 'give') 1\n",
            "('a', 'list') 1\n",
            "('list', 'of') 1\n",
            "('senses', 'and') 1\n",
            "('and', 'sentences') 1\n",
            "('sentences', ',') 1\n",
            "('and', 'humans') 1\n",
            "('humans', 'will') 1\n",
            "('will', 'not') 1\n",
            "('not', 'always') 1\n",
            "('always', 'agree') 1\n",
            "('which', 'word') 1\n",
            "('word', 'belongs') 1\n",
            "('belongs', 'in') 1\n",
            "('sense', '.') 2\n",
            "('.', 'thus') 1\n",
            "('thus', ',') 1\n",
            "(',', 'a') 7\n",
            "('a', 'computer') 1\n",
            "('computer', 'can') 1\n",
            "('can', 'not') 2\n",
            "('be', 'expected') 1\n",
            "('expected', 'to') 1\n",
            "('give', 'better') 1\n",
            "('better', 'performance') 1\n",
            "('performance', 'on') 1\n",
            "('on', 'such') 1\n",
            "('such', 'a') 2\n",
            "('task', 'than') 1\n",
            "('than', 'a') 1\n",
            "('human', '-lrb-') 1\n",
            "('-lrb-', 'indeed') 1\n",
            "('indeed', ',') 1\n",
            "(',', 'since') 2\n",
            "('since', 'the') 2\n",
            "('the', 'human') 3\n",
            "('human', 'serves') 1\n",
            "('serves', 'as') 2\n",
            "('the', 'standard') 1\n",
            "('standard', ',') 1\n",
            "('the', 'computer') 3\n",
            "('computer', 'being') 1\n",
            "('being', 'better') 1\n",
            "('than', 'the') 2\n",
            "('human', 'is') 1\n",
            "('is', 'incoherent') 1\n",
            "('incoherent', '-rrb-') 1\n",
            "(',', '-lrb-') 1\n",
            "('-lrb-', 'citation') 2\n",
            "('citation', 'needed') 2\n",
            "('needed', '-rrb-') 2\n",
            "('-rrb-', 'so') 1\n",
            "('so', 'the') 1\n",
            "('human', 'performance') 2\n",
            "('performance', 'serves') 1\n",
            "('as', 'an') 3\n",
            "('an', 'upper') 1\n",
            "('upper', 'bound') 1\n",
            "('bound', '.') 1\n",
            "('.', 'human') 1\n",
            "('performance', ',') 1\n",
            "(',', 'is') 3\n",
            "('is', 'much') 1\n",
            "('better', 'on') 1\n",
            "('on', 'coarse-grained') 2\n",
            "('coarse-grained', 'than') 1\n",
            "('than', 'fine-grained') 1\n",
            "('fine-grained', 'distinctions') 1\n",
            "(',', 'so') 1\n",
            "('so', 'this') 1\n",
            "('this', 'again') 1\n",
            "('again', 'is') 1\n",
            "('is', 'why') 1\n",
            "('why', 'research') 1\n",
            "('research', 'on') 1\n",
            "('coarse-grained', 'distinctions') 1\n",
            "('distinctions', 'has') 1\n",
            "('has', 'been') 10\n",
            "('been', 'put') 1\n",
            "('put', 'to') 1\n",
            "('test', 'in') 1\n",
            "('recent', 'wsd') 1\n",
            "('wsd', 'evaluation') 2\n",
            "('exercises', '.') 2\n",
            "('.', 'common') 1\n",
            "('common', 'sense') 4\n",
            "('sense', 'some') 1\n",
            "('some', 'ai') 1\n",
            "('ai', 'researchers') 1\n",
            "('researchers', 'like') 1\n",
            "('like', 'douglas') 1\n",
            "('douglas', 'lenat') 1\n",
            "('lenat', 'argue') 1\n",
            "('that', 'one') 2\n",
            "('one', 'can') 2\n",
            "('not', 'parse') 1\n",
            "('parse', 'meanings') 1\n",
            "('meanings', 'from') 1\n",
            "('from', 'words') 1\n",
            "('words', 'without') 1\n",
            "('without', 'some') 1\n",
            "('some', 'form') 1\n",
            "('of', 'common') 1\n",
            "('sense', 'ontology') 1\n",
            "('ontology', '.') 1\n",
            "('.', 'for') 7\n",
            "('for', 'example') 5\n",
            "('example', ',') 5\n",
            "(',', 'comparing') 1\n",
            "('comparing', 'these') 1\n",
            "('these', 'two') 1\n",
            "('two', 'sentences') 1\n",
            "('``', 'jill') 2\n",
            "('jill', 'and') 2\n",
            "('and', 'mary') 2\n",
            "('mary', 'are') 2\n",
            "('are', 'sisters') 2\n",
            "('sisters', '.') 1\n",
            "('.', \"''\") 3\n",
            "(\"''\", '--') 2\n",
            "('--', '-lrb-') 2\n",
            "('-lrb-', 'they') 1\n",
            "('they', 'are') 2\n",
            "('sisters', 'of') 1\n",
            "('of', 'each') 2\n",
            "('each', 'other') 2\n",
            "('other', '-rrb-') 1\n",
            "('.', '``') 1\n",
            "('are', 'mothers') 1\n",
            "('mothers', '.') 1\n",
            "('-lrb-', 'each') 1\n",
            "('each', 'is') 1\n",
            "('is', 'independently') 1\n",
            "('independently', 'a') 1\n",
            "('a', 'mother') 1\n",
            "('mother', '-rrb-') 1\n",
            "('to', 'properly') 1\n",
            "('properly', 'identify') 1\n",
            "('identify', 'senses') 1\n",
            "('senses', 'of') 1\n",
            "('words', 'one') 1\n",
            "('one', 'must') 1\n",
            "('must', 'know') 1\n",
            "('know', 'common') 1\n",
            "('sense', 'facts') 1\n",
            "('facts', '.') 1\n",
            "(',', 'sometimes') 1\n",
            "('sometimes', 'the') 1\n",
            "('the', 'common') 1\n",
            "('sense', 'is') 1\n",
            "('is', 'needed') 1\n",
            "('needed', 'to') 1\n",
            "('disambiguate', 'such') 1\n",
            "('such', 'words') 1\n",
            "('words', 'like') 1\n",
            "('like', 'pronouns') 1\n",
            "('pronouns', 'in') 1\n",
            "('in', 'case') 1\n",
            "('case', 'of') 1\n",
            "('of', 'having') 1\n",
            "('having', 'anaphoras') 1\n",
            "('anaphoras', 'or') 1\n",
            "('or', 'cataphoras') 1\n",
            "('cataphoras', 'in') 1\n",
            "('text', '.') 2\n",
            "('.', 'sense') 2\n",
            "('inventory', 'and') 1\n",
            "('and', 'algorithms') 1\n",
            "('algorithms', \"'\") 1\n",
            "(\"'\", 'task-dependency') 1\n",
            "('task-dependency', 'a') 1\n",
            "('a', 'task-independent') 1\n",
            "('task-independent', 'sense') 1\n",
            "('inventory', 'is') 2\n",
            "('is', 'not') 6\n",
            "('not', 'a') 2\n",
            "('a', 'coherent') 1\n",
            "('coherent', 'concept') 1\n",
            "('concept', ':') 1\n",
            "(':', 'each') 1\n",
            "('each', 'task') 1\n",
            "('task', 'requires') 1\n",
            "('requires', 'its') 1\n",
            "('its', 'own') 2\n",
            "('own', 'division') 1\n",
            "('word', 'meaning') 2\n",
            "('meaning', 'into') 1\n",
            "('senses', 'relevant') 1\n",
            "('relevant', 'to') 1\n",
            "('the', 'ambiguity') 1\n",
            "('ambiguity', 'of') 1\n",
            "('of', '`') 3\n",
            "('`', 'mouse') 1\n",
            "('mouse', \"'\") 1\n",
            "(\"'\", '-lrb-') 1\n",
            "('-lrb-', 'animal') 1\n",
            "('animal', 'or') 1\n",
            "('or', 'device') 1\n",
            "('device', '-rrb-') 1\n",
            "('not', 'relevant') 1\n",
            "('relevant', 'in') 2\n",
            "('in', 'english-french') 1\n",
            "('english-french', 'machine') 1\n",
            "('but', 'is') 1\n",
            "('is', 'relevant') 1\n",
            "('in', 'information') 2\n",
            "('information', 'retrieval') 3\n",
            "('retrieval', '.') 1\n",
            "('the', 'opposite') 1\n",
            "('opposite', 'is') 1\n",
            "('is', 'true') 1\n",
            "('true', 'of') 1\n",
            "('`', 'river') 1\n",
            "('river', \"'\") 3\n",
            "(\"'\", ',') 2\n",
            "('which', 'requires') 1\n",
            "('requires', 'a') 1\n",
            "('a', 'choice') 1\n",
            "('choice', 'in') 1\n",
            "('in', 'french') 1\n",
            "('french', '-lrb-') 1\n",
            "('-lrb-', 'fleuve') 1\n",
            "('fleuve', '`') 1\n",
            "('`', 'flows') 2\n",
            "('flows', 'into') 2\n",
            "('into', 'the') 2\n",
            "('the', 'sea') 1\n",
            "('sea', \"'\") 1\n",
            "(',', 'or') 3\n",
            "('or', 'rivière') 1\n",
            "('rivière', '`') 1\n",
            "('into', 'a') 1\n",
            "('a', 'river') 1\n",
            "(\"'\", '-rrb-') 2\n",
            "('.', 'also') 4\n",
            "('also', ',') 5\n",
            "(',', 'completely') 1\n",
            "('completely', 'different') 1\n",
            "('different', 'algorithms') 1\n",
            "('algorithms', 'might') 1\n",
            "('might', 'be') 1\n",
            "('be', 'required') 1\n",
            "('required', 'by') 1\n",
            "('by', 'different') 1\n",
            "('different', 'applications') 1\n",
            "('applications', '.') 1\n",
            "('in', 'machine') 1\n",
            "('problem', 'takes') 1\n",
            "('takes', 'the') 1\n",
            "('the', 'form') 2\n",
            "('word', 'selection') 1\n",
            "('selection', '.') 1\n",
            "('.', 'here') 1\n",
            "('here', 'the') 1\n",
            "('the', '``') 1\n",
            "('``', 'senses') 1\n",
            "('senses', \"''\") 1\n",
            "(\"''\", 'are') 1\n",
            "('are', 'words') 1\n",
            "('the', 'target') 2\n",
            "('target', 'language') 2\n",
            "('language', ',') 2\n",
            "('which', 'often') 1\n",
            "('often', 'correspond') 1\n",
            "('correspond', 'to') 1\n",
            "('to', 'significant') 1\n",
            "('significant', 'meaning') 1\n",
            "('meaning', 'distinctions') 2\n",
            "('the', 'source') 1\n",
            "('source', 'language') 1\n",
            "('language', '-lrb-') 1\n",
            "('-lrb-', 'bank') 1\n",
            "('bank', 'could') 1\n",
            "('could', 'translate') 1\n",
            "('translate', 'to') 1\n",
            "('to', 'french') 1\n",
            "('french', 'banque') 1\n",
            "('banque', '`') 1\n",
            "('`', 'financial') 1\n",
            "('financial', 'bank') 1\n",
            "('bank', \"'\") 1\n",
            "(\"'\", 'or') 1\n",
            "('or', 'rive') 1\n",
            "('rive', '`') 1\n",
            "('`', 'edge') 1\n",
            "('edge', 'of') 1\n",
            "('of', 'river') 1\n",
            "('retrieval', ',') 1\n",
            "('a', 'sense') 1\n",
            "('not', 'necessarily') 1\n",
            "('necessarily', 'required') 1\n",
            "('required', ',') 1\n",
            "(',', 'because') 2\n",
            "('because', 'it') 1\n",
            "('is', 'enough') 1\n",
            "('enough', 'to') 1\n",
            "('to', 'know') 1\n",
            "('know', 'that') 1\n",
            "('that', 'a') 1\n",
            "('same', 'sense') 1\n",
            "('sense', 'in') 2\n",
            "('the', 'query') 1\n",
            "('query', 'and') 1\n",
            "('and', 'a') 2\n",
            "('a', 'retrieved') 1\n",
            "('retrieved', 'document') 1\n",
            "('document', ';') 1\n",
            "(';', 'what') 1\n",
            "('what', 'sense') 1\n",
            "('sense', 'that') 1\n",
            "('that', 'is') 1\n",
            "('is', ',') 1\n",
            "('is', 'unimportant') 1\n",
            "('unimportant', '.') 1\n",
            "('.', 'discreteness') 1\n",
            "('discreteness', 'of') 1\n",
            "('senses', 'finally') 1\n",
            "('finally', ',') 1\n",
            "('the', 'very') 1\n",
            "('very', 'notion') 1\n",
            "('notion', 'of') 1\n",
            "('of', '``') 1\n",
            "('``', 'word') 1\n",
            "('sense', \"''\") 1\n",
            "('is', 'slippery') 1\n",
            "('slippery', 'and') 1\n",
            "('and', 'controversial') 1\n",
            "('controversial', '.') 1\n",
            "('most', 'people') 1\n",
            "('people', 'can') 1\n",
            "('can', 'agree') 1\n",
            "('agree', 'in') 1\n",
            "('in', 'distinctions') 1\n",
            "('distinctions', 'at') 1\n",
            "('coarse-grained', 'homograph') 1\n",
            "('homograph', 'level') 1\n",
            "('level', '-lrb-') 2\n",
            "('.', ',') 3\n",
            "(',', 'pen') 1\n",
            "('pen', 'as') 1\n",
            "('as', 'writing') 1\n",
            "('writing', 'instrument') 1\n",
            "('instrument', 'or') 1\n",
            "('or', 'enclosure') 1\n",
            "('enclosure', '-rrb-') 1\n",
            "('but', 'go') 1\n",
            "('go', 'down') 1\n",
            "('down', 'one') 1\n",
            "('one', 'level') 1\n",
            "('level', 'to') 1\n",
            "('to', 'fine-grained') 1\n",
            "('fine-grained', 'polysemy') 1\n",
            "('polysemy', ',') 1\n",
            "('and', 'disagreements') 1\n",
            "('disagreements', 'arise') 1\n",
            "('arise', '.') 1\n",
            "('in', 'senseval-2') 1\n",
            "('senseval-2', ',') 1\n",
            "('which', 'used') 1\n",
            "('used', 'fine-grained') 1\n",
            "('fine-grained', 'sense') 1\n",
            "(',', 'human') 1\n",
            "('annotators', 'agreed') 1\n",
            "('agreed', 'in') 1\n",
            "('in', 'only') 1\n",
            "('only', '85') 1\n",
            "('85', '%') 1\n",
            "('%', 'of') 1\n",
            "('word', 'occurrences') 3\n",
            "('occurrences', '.') 2\n",
            "('.', 'word') 1\n",
            "('meaning', 'is') 1\n",
            "('is', 'in') 2\n",
            "('in', 'principle') 1\n",
            "('principle', 'infinitely') 1\n",
            "('infinitely', 'variable') 1\n",
            "('variable', 'and') 1\n",
            "('and', 'context') 1\n",
            "('context', 'sensitive') 1\n",
            "('sensitive', '.') 1\n",
            "('it', 'does') 1\n",
            "('does', 'not') 2\n",
            "('not', 'divide') 1\n",
            "('divide', 'up') 1\n",
            "('up', 'easily') 1\n",
            "('easily', 'into') 1\n",
            "('into', 'distinct') 1\n",
            "('distinct', 'or') 1\n",
            "('or', 'discrete') 1\n",
            "('discrete', 'sub-meanings') 1\n",
            "('sub-meanings', '.') 1\n",
            "('.', 'lexicographers') 1\n",
            "('lexicographers', 'frequently') 1\n",
            "('frequently', 'discover') 1\n",
            "('discover', 'in') 1\n",
            "('in', 'corpora') 1\n",
            "('corpora', 'loose') 1\n",
            "('loose', 'and') 1\n",
            "('and', 'overlapping') 1\n",
            "('overlapping', 'word') 1\n",
            "('word', 'meanings') 1\n",
            "('meanings', ',') 1\n",
            "('and', 'standard') 1\n",
            "('standard', 'or') 1\n",
            "('or', 'conventional') 1\n",
            "('conventional', 'meanings') 1\n",
            "('meanings', 'extended') 1\n",
            "('extended', ',') 1\n",
            "(',', 'modulated') 1\n",
            "('modulated', ',') 1\n",
            "('and', 'exploited') 1\n",
            "('exploited', 'in') 1\n",
            "('a', 'bewildering') 1\n",
            "('bewildering', 'variety') 1\n",
            "('of', 'ways') 1\n",
            "('ways', '.') 1\n",
            "('the', 'art') 2\n",
            "('art', 'of') 1\n",
            "('of', 'lexicography') 1\n",
            "('lexicography', 'is') 1\n",
            "('to', 'generalize') 1\n",
            "('generalize', 'from') 1\n",
            "('from', 'the') 3\n",
            "('corpus', 'to') 2\n",
            "('to', 'definitions') 1\n",
            "('definitions', 'that') 1\n",
            "('that', 'evoke') 1\n",
            "('evoke', 'and') 1\n",
            "('and', 'explain') 1\n",
            "('explain', 'the') 1\n",
            "('the', 'full') 2\n",
            "('full', 'range') 1\n",
            "('range', 'of') 1\n",
            "('of', 'meaning') 1\n",
            "('meaning', 'of') 2\n",
            "('word', ',') 2\n",
            "('it', 'seem') 1\n",
            "('seem', 'like') 1\n",
            "('like', 'words') 1\n",
            "('words', 'are') 2\n",
            "('are', 'well-behaved') 1\n",
            "('well-behaved', 'semantically') 1\n",
            "('semantically', '.') 1\n",
            "('not', 'at') 1\n",
            "('at', 'all') 1\n",
            "('all', 'clear') 1\n",
            "('clear', 'if') 1\n",
            "('if', 'these') 1\n",
            "('these', 'same') 1\n",
            "('same', 'meaning') 1\n",
            "('distinctions', 'are') 1\n",
            "('are', 'applicable') 1\n",
            "('applicable', 'in') 1\n",
            "('computational', 'applications') 1\n",
            "('applications', ',') 1\n",
            "('the', 'decisions') 1\n",
            "('decisions', 'of') 1\n",
            "('of', 'lexicographers') 1\n",
            "('lexicographers', 'are') 1\n",
            "('are', 'usually') 1\n",
            "('usually', 'driven') 1\n",
            "('driven', 'by') 1\n",
            "('by', 'other') 1\n",
            "('other', 'considerations') 1\n",
            "('considerations', '.') 1\n",
            "('.', 'recently') 2\n",
            "('recently', ',') 3\n",
            "('task', '--') 1\n",
            "('--', 'named') 1\n",
            "('named', 'lexical') 1\n",
            "('lexical', 'substitution') 2\n",
            "('substitution', '--') 1\n",
            "('--', 'has') 1\n",
            "('been', 'proposed') 1\n",
            "('proposed', 'as') 1\n",
            "('a', 'possible') 1\n",
            "('possible', 'solution') 1\n",
            "('sense', 'discreteness') 1\n",
            "('discreteness', 'problem') 1\n",
            "('problem', '.') 2\n",
            "('task', 'consists') 1\n",
            "('consists', 'of') 1\n",
            "('of', 'providing') 1\n",
            "('providing', 'a') 1\n",
            "('a', 'substitute') 1\n",
            "('substitute', 'for') 1\n",
            "('in', 'context') 1\n",
            "('context', 'that') 1\n",
            "('that', 'preserves') 1\n",
            "('preserves', 'the') 1\n",
            "('the', 'meaning') 1\n",
            "('the', 'original') 1\n",
            "('original', 'word') 1\n",
            "('-lrb-', 'potentially') 1\n",
            "('potentially', ',') 1\n",
            "(',', 'substitutes') 1\n",
            "('substitutes', 'can') 1\n",
            "('be', 'chosen') 1\n",
            "('chosen', 'from') 1\n",
            "('full', 'lexicon') 1\n",
            "('lexicon', 'of') 1\n",
            "(',', 'thus') 1\n",
            "('thus', 'overcoming') 1\n",
            "('overcoming', 'discreteness') 1\n",
            "('discreteness', '-rrb-') 1\n",
            "('.', 'approaches') 1\n",
            "('approaches', 'and') 2\n",
            "('and', 'methods') 1\n",
            "('methods', 'as') 1\n",
            "('in', 'all') 1\n",
            "('all', 'natural') 1\n",
            "(',', 'there') 2\n",
            "('there', 'are') 2\n",
            "('are', 'two') 1\n",
            "('two', 'main') 2\n",
            "('main', 'approaches') 1\n",
            "('approaches', 'to') 2\n",
            "('to', 'wsd') 3\n",
            "('wsd', '--') 1\n",
            "('--', 'deep') 1\n",
            "('deep', 'approaches') 4\n",
            "('and', 'shallow') 1\n",
            "('shallow', 'approaches') 4\n",
            "('approaches', '.') 2\n",
            "('.', 'deep') 1\n",
            "('approaches', 'presume') 1\n",
            "('presume', 'access') 1\n",
            "('access', 'to') 1\n",
            "('a', 'comprehensive') 1\n",
            "('comprehensive', 'body') 1\n",
            "('body', 'of') 2\n",
            "('of', 'world') 1\n",
            "('.', 'knowledge') 2\n",
            "('knowledge', ',') 1\n",
            "('as', '``') 2\n",
            "('``', 'you') 1\n",
            "('you', 'can') 1\n",
            "('can', 'go') 1\n",
            "('go', 'fishing') 1\n",
            "('fish', ',') 1\n",
            "('but', 'not') 2\n",
            "('not', 'for') 1\n",
            "('for', 'low') 1\n",
            "('frequency', 'sounds') 2\n",
            "('sounds', \"''\") 1\n",
            "('``', 'songs') 1\n",
            "('songs', 'have') 1\n",
            "('have', 'low') 1\n",
            "('sounds', 'as') 1\n",
            "('as', 'parts') 1\n",
            "('parts', ',') 1\n",
            "('not', 'types') 1\n",
            "('types', 'of') 1\n",
            "('fish', \"''\") 1\n",
            "('is', 'then') 2\n",
            "('then', 'used') 2\n",
            "('used', 'to') 4\n",
            "('to', 'determine') 1\n",
            "('determine', 'in') 1\n",
            "('sense', 'the') 1\n",
            "('used', '.') 2\n",
            "('these', 'approaches') 2\n",
            "('approaches', 'are') 1\n",
            "('are', 'not') 2\n",
            "('not', 'very') 2\n",
            "('very', 'successful') 2\n",
            "('successful', 'in') 1\n",
            "('in', 'practice') 2\n",
            "('practice', ',') 2\n",
            "('because', 'such') 1\n",
            "('a', 'body') 1\n",
            "('of', 'knowledge') 2\n",
            "('knowledge', 'does') 1\n",
            "('not', 'exist') 1\n",
            "('exist', 'in') 1\n",
            "('a', 'computer-readable') 1\n",
            "('computer-readable', 'format') 1\n",
            "('format', ',') 1\n",
            "(',', 'outside') 1\n",
            "('outside', 'of') 1\n",
            "('of', 'very') 1\n",
            "('very', 'limited') 1\n",
            "('limited', 'domains') 1\n",
            "('domains', '.') 3\n",
            "(',', 'if') 1\n",
            "('if', 'such') 1\n",
            "('such', 'knowledge') 1\n",
            "('knowledge', 'did') 1\n",
            "('did', 'exist') 1\n",
            "('exist', ',') 1\n",
            "(',', 'then') 2\n",
            "('then', 'deep') 1\n",
            "('approaches', 'would') 1\n",
            "('would', 'be') 1\n",
            "('be', 'much') 1\n",
            "('more', 'accurate') 1\n",
            "('accurate', 'than') 1\n",
            "('the', 'shallow') 1\n",
            "('.', '-lrb-') 1\n",
            "('-rrb-', 'also') 1\n",
            "('there', 'is') 2\n",
            "('a', 'long') 1\n",
            "('long', 'tradition') 1\n",
            "('tradition', 'in') 1\n",
            "(',', 'of') 1\n",
            "('of', 'trying') 1\n",
            "('trying', 'such') 1\n",
            "('such', 'approaches') 1\n",
            "('approaches', 'in') 1\n",
            "('in', 'terms') 1\n",
            "('terms', 'of') 1\n",
            "('of', 'coded') 1\n",
            "('coded', 'knowledge') 1\n",
            "('knowledge', 'and') 2\n",
            "('in', 'some') 1\n",
            "('some', 'cases') 1\n",
            "('is', 'hard') 1\n",
            "('hard', 'to') 1\n",
            "('to', 'say') 1\n",
            "('say', 'clearly') 1\n",
            "('clearly', 'whether') 1\n",
            "('whether', 'the') 1\n",
            "('knowledge', 'involved') 1\n",
            "('involved', 'is') 1\n",
            "('is', 'linguistic') 1\n",
            "('linguistic', 'or') 1\n",
            "('or', 'world') 1\n",
            "('first', 'attempt') 1\n",
            "('attempt', 'was') 1\n",
            "('was', 'that') 2\n",
            "('that', 'by') 1\n",
            "('by', 'margaret') 1\n",
            "('margaret', 'masterman') 1\n",
            "('masterman', 'and') 1\n",
            "('and', 'her') 1\n",
            "('her', 'colleagues') 1\n",
            "('colleagues', ',') 1\n",
            "('the', 'cambridge') 1\n",
            "('cambridge', 'language') 1\n",
            "('language', 'research') 1\n",
            "('research', 'unit') 1\n",
            "('unit', 'in') 1\n",
            "('in', 'england') 1\n",
            "('england', ',') 1\n",
            "('the', '1950s') 1\n",
            "('1950s', '.') 1\n",
            "('.', 'this') 4\n",
            "('this', 'attempt') 1\n",
            "('attempt', 'used') 1\n",
            "('as', 'data') 1\n",
            "('data', 'a') 1\n",
            "('a', 'punched-card') 1\n",
            "('punched-card', 'version') 1\n",
            "('version', 'of') 1\n",
            "('of', 'roget') 1\n",
            "('and', 'its') 2\n",
            "('its', 'numbered') 1\n",
            "('numbered', '``') 1\n",
            "('``', 'heads') 1\n",
            "('heads', \"''\") 1\n",
            "('an', 'indicator') 1\n",
            "('indicator', 'of') 1\n",
            "('of', 'topics') 1\n",
            "('topics', 'and') 1\n",
            "('and', 'looked') 1\n",
            "('looked', 'for') 1\n",
            "('for', 'repetitions') 1\n",
            "('repetitions', 'in') 1\n",
            "('in', 'text') 2\n",
            "(',', 'using') 4\n",
            "('using', 'a') 2\n",
            "('a', 'set') 4\n",
            "('set', 'intersection') 1\n",
            "('intersection', 'algorithm') 1\n",
            "('algorithm', '.') 3\n",
            "('it', 'was') 2\n",
            "('was', 'not') 1\n",
            "('successful', ',') 1\n",
            "('but', 'had') 1\n",
            "('had', 'strong') 1\n",
            "('strong', 'relationships') 1\n",
            "('relationships', 'to') 1\n",
            "('to', 'later') 1\n",
            "('later', 'work') 1\n",
            "('work', ',') 1\n",
            "(',', 'especially') 1\n",
            "('especially', 'yarowsky') 1\n",
            "('yarowsky', \"'s\") 1\n",
            "(\"'s\", 'machine') 1\n",
            "('learning', 'optimisation') 1\n",
            "('optimisation', 'of') 1\n",
            "('a', 'thesaurus') 1\n",
            "('thesaurus', 'method') 1\n",
            "('method', 'in') 1\n",
            "('1990s', '.') 1\n",
            "('.', 'shallow') 1\n",
            "('approaches', 'do') 1\n",
            "('do', \"n't\") 1\n",
            "(\"n't\", 'try') 1\n",
            "('try', 'to') 1\n",
            "('to', 'understand') 1\n",
            "('understand', 'the') 1\n",
            "('.', 'they') 3\n",
            "('they', 'just') 1\n",
            "('just', 'consider') 1\n",
            "('consider', 'the') 1\n",
            "('the', 'surrounding') 1\n",
            "('surrounding', 'words') 2\n",
            "('using', 'information') 1\n",
            "('information', 'such') 1\n",
            "('``', 'if') 1\n",
            "('if', 'bass') 2\n",
            "('bass', 'has') 2\n",
            "('has', 'words') 1\n",
            "('words', 'sea') 1\n",
            "('sea', 'or') 1\n",
            "('or', 'fishing') 1\n",
            "('fishing', 'nearby') 1\n",
            "('nearby', ',') 2\n",
            "('it', 'probably') 1\n",
            "('probably', 'is') 1\n",
            "('the', 'fish') 1\n",
            "('fish', 'sense') 1\n",
            "('sense', ';') 1\n",
            "(';', 'if') 1\n",
            "('has', 'the') 1\n",
            "('words', 'music') 1\n",
            "('music', 'or') 1\n",
            "('or', 'song') 1\n",
            "('song', 'nearby') 1\n",
            "('is', 'probably') 1\n",
            "('probably', 'in') 1\n",
            "('the', 'music') 1\n",
            "('music', 'sense') 1\n",
            "(\"''\", 'these') 1\n",
            "('these', 'rules') 1\n",
            "('rules', 'can') 1\n",
            "('be', 'automatically') 1\n",
            "('automatically', 'derived') 1\n",
            "('derived', 'by') 1\n",
            "('computer', ',') 1\n",
            "('a', 'training') 1\n",
            "('training', 'corpus') 2\n",
            "('words', 'tagged') 1\n",
            "('tagged', 'with') 1\n",
            "('with', 'their') 1\n",
            "('their', 'word') 1\n",
            "('this', 'approach') 1\n",
            "('approach', ',') 1\n",
            "('while', 'theoretically') 1\n",
            "('theoretically', 'not') 1\n",
            "('not', 'as') 1\n",
            "('as', 'powerful') 1\n",
            "('powerful', 'as') 1\n",
            "('as', 'deep') 1\n",
            "('approaches', ',') 2\n",
            "(',', 'gives') 1\n",
            "('gives', 'superior') 1\n",
            "('superior', 'results') 1\n",
            "('results', 'in') 1\n",
            "(',', 'due') 1\n",
            "('due', 'to') 1\n",
            "('computer', \"'s\") 1\n",
            "(\"'s\", 'limited') 1\n",
            "('limited', 'world') 1\n",
            "('it', 'can') 1\n",
            "('be', 'confused') 1\n",
            "('confused', 'by') 1\n",
            "('by', 'sentences') 1\n",
            "('sentences', 'like') 1\n",
            "('the', 'dogs') 1\n",
            "('dogs', 'bark') 1\n",
            "('bark', 'at') 1\n",
            "('the', 'tree') 1\n",
            "('tree', 'which') 1\n",
            "('which', 'contains') 1\n",
            "('contains', 'the') 1\n",
            "('word', 'bark') 1\n",
            "('bark', 'near') 1\n",
            "('near', 'both') 1\n",
            "('both', 'tree') 1\n",
            "('tree', 'and') 1\n",
            "('and', 'dogs') 1\n",
            "('dogs', '.') 1\n",
            "('.', 'there') 1\n",
            "('are', 'four') 1\n",
            "('four', 'conventional') 1\n",
            "('conventional', 'approaches') 1\n",
            "('wsd', ':') 1\n",
            "(':', 'dictionary') 1\n",
            "('dictionary', '-') 2\n",
            "('-', 'and') 2\n",
            "('and', 'knowledge-based') 3\n",
            "('knowledge-based', 'methods') 4\n",
            "('methods', ':') 5\n",
            "(':', 'these') 4\n",
            "('these', 'rely') 1\n",
            "('rely', 'primarily') 1\n",
            "('primarily', 'on') 1\n",
            "('on', 'dictionaries') 1\n",
            "('dictionaries', ',') 2\n",
            "(',', 'thesauri') 2\n",
            "('thesauri', ',') 2\n",
            "('and', 'lexical') 3\n",
            "('lexical', 'knowledge') 3\n",
            "('knowledge', 'bases') 1\n",
            "('bases', ',') 1\n",
            "(',', 'without') 1\n",
            "('without', 'using') 1\n",
            "('using', 'any') 2\n",
            "('any', 'corpus') 1\n",
            "('corpus', 'evidence') 1\n",
            "('evidence', '.') 2\n",
            "('.', 'supervised') 3\n",
            "('supervised', 'methods') 7\n",
            "('these', 'make') 2\n",
            "('make', 'use') 2\n",
            "('use', 'of') 5\n",
            "('of', 'sense-annotated') 1\n",
            "('sense-annotated', 'corpora') 2\n",
            "('corpora', 'to') 2\n",
            "('to', 'train') 3\n",
            "('train', 'from') 1\n",
            "('from', '.') 1\n",
            "('.', 'semi-supervised') 2\n",
            "('semi-supervised', 'or') 1\n",
            "('or', 'minimally') 1\n",
            "('minimally', 'supervised') 1\n",
            "('a', 'secondary') 1\n",
            "('secondary', 'source') 1\n",
            "('source', 'of') 1\n",
            "('knowledge', 'such') 1\n",
            "('small', 'annotated') 1\n",
            "('annotated', 'corpus') 1\n",
            "('corpus', 'as') 1\n",
            "('as', 'seed') 1\n",
            "('seed', 'data') 2\n",
            "('data', 'in') 1\n",
            "('a', 'bootstrapping') 1\n",
            "('bootstrapping', 'process') 1\n",
            "('process', ',') 1\n",
            "('or', 'a') 2\n",
            "('a', 'word-aligned') 1\n",
            "('word-aligned', 'bilingual') 2\n",
            "('bilingual', 'corpus') 1\n",
            "('corpus', '.') 1\n",
            "('.', 'unsupervised') 3\n",
            "('these', 'eschew') 1\n",
            "('eschew', '-lrb-') 1\n",
            "('-lrb-', 'almost') 1\n",
            "('almost', '-rrb-') 1\n",
            "('-rrb-', 'completely') 1\n",
            "('completely', 'external') 1\n",
            "('external', 'information') 1\n",
            "('information', 'and') 1\n",
            "('and', 'work') 1\n",
            "('work', 'directly') 1\n",
            "('directly', 'from') 1\n",
            "('from', 'raw') 1\n",
            "('raw', 'unannotated') 1\n",
            "('unannotated', 'corpora') 1\n",
            "('corpora', '.') 2\n",
            "('these', 'methods') 1\n",
            "('methods', 'are') 3\n",
            "('are', 'also') 1\n",
            "('also', 'known') 1\n",
            "('known', 'under') 1\n",
            "('under', 'the') 1\n",
            "('the', 'name') 1\n",
            "('name', 'of') 1\n",
            "('sense', 'discrimination') 1\n",
            "('discrimination', '.') 2\n",
            "('.', 'almost') 1\n",
            "('almost', 'all') 1\n",
            "('all', 'these') 1\n",
            "('approaches', 'normally') 1\n",
            "('normally', 'work') 1\n",
            "('work', 'by') 1\n",
            "('by', 'defining') 1\n",
            "('defining', 'a') 1\n",
            "('a', 'window') 1\n",
            "('window', 'of') 1\n",
            "('of', 'n') 1\n",
            "('n', 'content') 1\n",
            "('content', 'words') 1\n",
            "('words', 'around') 1\n",
            "('around', 'each') 1\n",
            "('word', 'to') 1\n",
            "('disambiguated', 'in') 1\n",
            "('corpus', ',') 2\n",
            "('and', 'statistically') 1\n",
            "('statistically', 'analyzing') 1\n",
            "('analyzing', 'those') 1\n",
            "('those', 'n') 1\n",
            "('n', 'surrounding') 1\n",
            "('words', '.') 4\n",
            "('.', 'two') 2\n",
            "('two', 'shallow') 1\n",
            "('approaches', 'used') 1\n",
            "('train', 'and') 1\n",
            "('and', 'then') 1\n",
            "('then', 'disambiguate') 1\n",
            "('disambiguate', 'are') 1\n",
            "('are', 'naïve') 1\n",
            "('naïve', 'bayes') 1\n",
            "('bayes', 'classifiers') 1\n",
            "('classifiers', 'and') 1\n",
            "('and', 'decision') 1\n",
            "('decision', 'trees') 1\n",
            "('trees', '.') 1\n",
            "('recent', 'research') 1\n",
            "('research', ',') 1\n",
            "(',', 'kernel-based') 1\n",
            "('kernel-based', 'methods') 1\n",
            "('methods', 'such') 1\n",
            "('as', 'support') 1\n",
            "('support', 'vector') 2\n",
            "('vector', 'machines') 2\n",
            "('machines', 'have') 1\n",
            "('have', 'shown') 1\n",
            "('shown', 'superior') 1\n",
            "('superior', 'performance') 1\n",
            "('performance', 'in') 2\n",
            "('in', 'supervised') 1\n",
            "('.', 'graph-based') 2\n",
            "('graph-based', 'approaches') 2\n",
            "('have', 'also') 1\n",
            "('also', 'gained') 1\n",
            "('gained', 'much') 1\n",
            "('much', 'attention') 1\n",
            "('attention', 'from') 1\n",
            "('the', 'research') 1\n",
            "('research', 'community') 1\n",
            "('community', ',') 1\n",
            "('and', 'currently') 1\n",
            "('currently', 'achieve') 1\n",
            "('achieve', 'performance') 1\n",
            "('performance', 'close') 1\n",
            "('close', 'to') 1\n",
            "('the', 'state') 1\n",
            "('state', 'of') 1\n",
            "('art', '.') 1\n",
            "('.', 'dictionary') 1\n",
            "('methods', 'the') 1\n",
            "('the', 'lesk') 1\n",
            "('lesk', 'algorithm') 2\n",
            "('algorithm', 'is') 1\n",
            "('is', 'the') 3\n",
            "('the', 'seminal') 1\n",
            "('seminal', 'dictionary-based') 1\n",
            "('dictionary-based', 'method') 1\n",
            "('method', '.') 2\n",
            "('is', 'based') 1\n",
            "('based', 'on') 3\n",
            "('the', 'hypothesis') 1\n",
            "('hypothesis', 'that') 1\n",
            "('that', 'words') 1\n",
            "('words', 'used') 1\n",
            "('used', 'together') 1\n",
            "('together', 'in') 1\n",
            "('text', 'are') 1\n",
            "('are', 'related') 1\n",
            "('related', 'to') 1\n",
            "('to', 'each') 1\n",
            "('other', 'and') 1\n",
            "('and', 'that') 1\n",
            "('the', 'relation') 1\n",
            "('relation', 'can') 1\n",
            "('be', 'observed') 1\n",
            "('observed', 'in') 1\n",
            "('definitions', 'of') 2\n",
            "('words', 'and') 1\n",
            "('and', 'their') 2\n",
            "('their', 'senses') 1\n",
            "('two', '-lrb-') 1\n",
            "('-lrb-', 'or') 3\n",
            "('or', 'more') 1\n",
            "('more', '-rrb-') 1\n",
            "('-rrb-', 'words') 1\n",
            "('are', 'disambiguated') 1\n",
            "('disambiguated', 'by') 1\n",
            "('by', 'finding') 1\n",
            "('finding', 'the') 1\n",
            "('the', 'pair') 1\n",
            "('pair', 'of') 2\n",
            "('of', 'dictionary') 2\n",
            "('dictionary', 'senses') 2\n",
            "('senses', 'with') 3\n",
            "('the', 'greatest') 2\n",
            "('greatest', 'word') 1\n",
            "('word', 'overlap') 1\n",
            "('overlap', 'in') 1\n",
            "('in', 'their') 2\n",
            "('their', 'dictionary') 1\n",
            "('dictionary', 'definitions') 1\n",
            "('definitions', '.') 1\n",
            "('when', 'disambiguating') 1\n",
            "('in', '``') 2\n",
            "('``', 'pine') 1\n",
            "('pine', 'cone') 1\n",
            "('cone', \"''\") 1\n",
            "('the', 'appropriate') 1\n",
            "('appropriate', 'senses') 1\n",
            "('senses', 'both') 1\n",
            "('both', 'include') 1\n",
            "('include', 'the') 1\n",
            "('words', 'evergreen') 1\n",
            "('evergreen', 'and') 1\n",
            "('and', 'tree') 1\n",
            "('tree', '-lrb-') 1\n",
            "('-lrb-', 'at') 1\n",
            "('least', 'in') 1\n",
            "('in', 'one') 2\n",
            "('one', 'dictionary') 1\n",
            "('dictionary', '-rrb-') 1\n",
            "('.', 'an') 1\n",
            "('an', 'alternative') 1\n",
            "('alternative', 'to') 1\n",
            "('the', 'use') 3\n",
            "('definitions', 'is') 1\n",
            "('to', 'consider') 1\n",
            "('consider', 'general') 1\n",
            "('general', 'word-sense') 1\n",
            "('word-sense', 'relatedness') 1\n",
            "('relatedness', 'and') 1\n",
            "('and', 'to') 1\n",
            "('to', 'compute') 1\n",
            "('compute', 'the') 1\n",
            "('the', 'semantic') 1\n",
            "('semantic', 'similarity') 1\n",
            "('similarity', 'of') 2\n",
            "('each', 'pair') 1\n",
            "('senses', 'based') 1\n",
            "('a', 'given') 3\n",
            "('given', 'lexical') 1\n",
            "('knowledge', 'base') 2\n",
            "('base', 'such') 1\n",
            "('as', 'wordnet') 2\n",
            "('wordnet', '.') 1\n",
            "('methods', 'reminiscent') 1\n",
            "('reminiscent', 'of') 1\n",
            "('of', 'spreading') 1\n",
            "('spreading', 'activation') 1\n",
            "('activation', 'research') 1\n",
            "('research', 'of') 1\n",
            "('of', 'ai') 1\n",
            "('ai', 'research') 1\n",
            "('research', 'have') 1\n",
            "('been', 'applied') 2\n",
            "('applied', 'with') 1\n",
            "('some', 'success') 1\n",
            "('success', '.') 1\n",
            "('.', 'more') 1\n",
            "('more', 'complex') 1\n",
            "('complex', 'graph-based') 1\n",
            "('been', 'shown') 4\n",
            "('shown', 'to') 3\n",
            "('perform', 'almost') 1\n",
            "('almost', 'as') 1\n",
            "('as', 'well') 1\n",
            "('well', 'as') 1\n",
            "('as', 'supervised') 1\n",
            "('methods', 'or') 1\n",
            "('or', 'even') 1\n",
            "('even', 'outperforming') 1\n",
            "('outperforming', 'them') 1\n",
            "('them', 'on') 1\n",
            "('on', 'specific') 1\n",
            "('specific', 'domains') 1\n",
            "('it', 'has') 2\n",
            "('reported', 'that') 1\n",
            "('that', 'simple') 1\n",
            "('simple', 'graph') 1\n",
            "('graph', 'connectivity') 1\n",
            "('connectivity', 'measures') 1\n",
            "('measures', ',') 1\n",
            "('as', 'degree') 1\n",
            "('degree', ',') 1\n",
            "(',', 'perform') 2\n",
            "('perform', 'state-of-the-art') 1\n",
            "('state-of-the-art', 'wsd') 1\n",
            "('wsd', 'in') 1\n",
            "('the', 'presence') 1\n",
            "('presence', 'of') 1\n",
            "('a', 'sufficiently') 1\n",
            "('sufficiently', 'rich') 1\n",
            "('rich', 'lexical') 1\n",
            "('base', '.') 1\n",
            "(',', 'automatically') 1\n",
            "('automatically', 'transferring') 1\n",
            "('transferring', 'knowledge') 1\n",
            "('knowledge', 'in') 1\n",
            "('semantic', 'relations') 1\n",
            "('relations', 'from') 1\n",
            "('from', 'wikipedia') 1\n",
            "('wikipedia', 'to') 1\n",
            "('to', 'wordnet') 1\n",
            "('wordnet', 'has') 1\n",
            "('to', 'boost') 1\n",
            "('boost', 'simple') 1\n",
            "('simple', 'knowledge-based') 1\n",
            "(',', 'enabling') 1\n",
            "('enabling', 'them') 1\n",
            "('them', 'to') 1\n",
            "('to', 'rival') 1\n",
            "('rival', 'the') 1\n",
            "('the', 'best') 1\n",
            "('best', 'supervised') 1\n",
            "('systems', 'and') 1\n",
            "('and', 'even') 1\n",
            "('even', 'outperform') 1\n",
            "('outperform', 'them') 1\n",
            "('them', 'in') 1\n",
            "('a', 'domain-specific') 1\n",
            "('domain-specific', 'setting') 1\n",
            "('setting', '.') 1\n",
            "('of', 'selectional') 1\n",
            "('selectional', 'preferences') 1\n",
            "('preferences', '-lrb-') 1\n",
            "('or', 'selectional') 1\n",
            "('selectional', 'restrictions') 1\n",
            "('restrictions', '-rrb-') 1\n",
            "('is', 'also') 2\n",
            "('also', 'useful') 1\n",
            "('useful', ',') 1\n",
            "(',', 'for') 1\n",
            "(',', 'knowing') 1\n",
            "('knowing', 'that') 1\n",
            "('one', 'typically') 1\n",
            "('typically', 'cooks') 1\n",
            "('cooks', 'food') 1\n",
            "('food', ',') 1\n",
            "(',', 'one') 2\n",
            "('can', 'disambiguate') 1\n",
            "('disambiguate', 'the') 1\n",
            "('bass', 'in') 1\n",
            "('``', 'i') 1\n",
            "('i', 'am') 1\n",
            "('am', 'cooking') 1\n",
            "('cooking', 'basses') 1\n",
            "('basses', \"''\") 1\n",
            "('it', \"'s\") 1\n",
            "(\"'s\", 'not') 1\n",
            "('a', 'musical') 1\n",
            "('methods', 'supervised') 1\n",
            "('are', 'based') 1\n",
            "('the', 'assumption') 1\n",
            "('assumption', 'that') 1\n",
            "('the', 'context') 2\n",
            "('context', 'can') 1\n",
            "('can', 'provide') 1\n",
            "('provide', 'enough') 1\n",
            "('enough', 'evidence') 1\n",
            "('evidence', 'on') 1\n",
            "('on', 'its') 1\n",
            "('own', 'to') 1\n",
            "('disambiguate', 'words') 1\n",
            "('words', '-lrb-') 1\n",
            "('-lrb-', 'hence') 1\n",
            "('hence', ',') 1\n",
            "(',', 'world') 1\n",
            "('and', 'reasoning') 1\n",
            "('reasoning', 'are') 1\n",
            "('are', 'deemed') 1\n",
            "('deemed', 'unnecessary') 1\n",
            "('unnecessary', '-rrb-') 1\n",
            "('.', 'probably') 1\n",
            "('probably', 'every') 1\n",
            "('every', 'machine') 1\n",
            "('learning', 'algorithm') 1\n",
            "('algorithm', 'going') 1\n",
            "('going', 'has') 1\n",
            "('applied', 'to') 1\n",
            "(',', 'including') 2\n",
            "('including', 'associated') 1\n",
            "('associated', 'techniques') 1\n",
            "('techniques', 'such') 1\n",
            "('as', 'feature') 1\n",
            "('feature', 'selection') 1\n",
            "('selection', ',') 1\n",
            "(',', 'parameter') 1\n",
            "('parameter', 'optimization') 1\n",
            "('optimization', ',') 1\n",
            "('and', 'ensemble') 1\n",
            "('ensemble', 'learning') 1\n",
            "('.', 'support') 1\n",
            "('machines', 'and') 1\n",
            "('and', 'memory-based') 1\n",
            "('memory-based', 'learning') 1\n",
            "('learning', 'have') 1\n",
            "('be', 'the') 1\n",
            "('successful', 'approaches') 1\n",
            "('date', ',') 1\n",
            "(',', 'probably') 1\n",
            "('probably', 'because') 1\n",
            "('because', 'they') 2\n",
            "('they', 'can') 3\n",
            "('can', 'cope') 1\n",
            "('cope', 'with') 1\n",
            "('the', 'high-dimensionality') 1\n",
            "('high-dimensionality', 'of') 1\n",
            "('the', 'feature') 1\n",
            "('feature', 'space') 1\n",
            "('space', '.') 1\n",
            "(',', 'these') 1\n",
            "('these', 'supervised') 1\n",
            "('are', 'subject') 1\n",
            "('subject', 'to') 1\n",
            "('a', 'new') 1\n",
            "('new', 'knowledge') 1\n",
            "('bottleneck', 'since') 1\n",
            "('since', 'they') 1\n",
            "('they', 'rely') 1\n",
            "('rely', 'on') 2\n",
            "('on', 'substantial') 1\n",
            "('substantial', 'amounts') 1\n",
            "('amounts', 'of') 1\n",
            "('manually', 'sense-tagged') 1\n",
            "('sense-tagged', 'corpora') 2\n",
            "('corpora', 'for') 1\n",
            "('for', 'training') 1\n",
            "('training', ',') 1\n",
            "('which', 'are') 3\n",
            "('are', 'laborious') 1\n",
            "('laborious', 'and') 1\n",
            "('and', 'expensive') 1\n",
            "('to', 'create') 1\n",
            "('create', '.') 1\n",
            "('semi-supervised', 'methods') 1\n",
            "('methods', 'because') 1\n",
            "('of', 'training') 3\n",
            "('training', 'data') 2\n",
            "('data', ',') 1\n",
            "(',', 'many') 1\n",
            "('many', 'word') 1\n",
            "('disambiguation', 'algorithms') 1\n",
            "('algorithms', 'use') 1\n",
            "('use', 'semi-supervised') 1\n",
            "('semi-supervised', 'learning') 1\n",
            "('learning', ',') 1\n",
            "('which', 'allows') 1\n",
            "('allows', 'both') 1\n",
            "('both', 'labeled') 1\n",
            "('labeled', 'and') 1\n",
            "('and', 'unlabeled') 1\n",
            "('unlabeled', 'data') 1\n",
            "('data', '.') 1\n",
            "('the', 'yarowsky') 1\n",
            "('yarowsky', 'algorithm') 1\n",
            "('algorithm', 'was') 1\n",
            "('was', 'an') 1\n",
            "('an', 'early') 1\n",
            "('early', 'example') 1\n",
            "('example', 'of') 2\n",
            "('of', 'such') 2\n",
            "('such', 'an') 1\n",
            "('an', 'algorithm') 1\n",
            "('it', 'uses') 1\n",
            "('uses', 'the') 1\n",
            "('the', '`') 2\n",
            "('`', 'one') 2\n",
            "('one', 'sense') 3\n",
            "('sense', 'per') 2\n",
            "('per', 'collocation') 1\n",
            "('collocation', \"'\") 1\n",
            "(\"'\", 'and') 1\n",
            "('per', 'discourse') 1\n",
            "('discourse', \"'\") 1\n",
            "(\"'\", 'properties') 1\n",
            "('properties', 'of') 1\n",
            "('of', 'human') 1\n",
            "('human', 'languages') 1\n",
            "('languages', 'for') 1\n",
            "('for', 'word') 1\n",
            "('disambiguation', '.') 1\n",
            "('.', 'from') 1\n",
            "('from', 'observation') 1\n",
            "('observation', ',') 1\n",
            "(',', 'words') 1\n",
            "('words', 'tend') 1\n",
            "('to', 'exhibit') 1\n",
            "('exhibit', 'only') 1\n",
            "('only', 'one') 1\n",
            "('in', 'most') 1\n",
            "('most', 'given') 1\n",
            "('given', 'discourse') 1\n",
            "('discourse', 'and') 1\n",
            "('given', 'collocation') 1\n",
            "('collocation', '.') 1\n",
            "('the', 'bootstrapping') 1\n",
            "('bootstrapping', 'approach') 1\n",
            "('approach', 'starts') 1\n",
            "('starts', 'from') 1\n",
            "('from', 'a') 1\n",
            "('small', 'amount') 1\n",
            "('amount', 'of') 3\n",
            "('of', 'seed') 1\n",
            "('data', 'for') 1\n",
            "('word', ':') 1\n",
            "(':', 'either') 1\n",
            "('either', 'manually') 1\n",
            "('manually', 'tagged') 1\n",
            "('tagged', 'training') 1\n",
            "('training', 'examples') 2\n",
            "('examples', 'or') 1\n",
            "('small', 'number') 1\n",
            "('number', 'of') 2\n",
            "('of', 'surefire') 1\n",
            "('surefire', 'decision') 1\n",
            "('decision', 'rules') 1\n",
            "('rules', '-lrb-') 1\n",
            "(',', '`') 1\n",
            "('`', 'play') 1\n",
            "('play', \"'\") 1\n",
            "(\"'\", 'in') 1\n",
            "('context', 'of') 1\n",
            "('`', 'bass') 1\n",
            "('bass', \"'\") 1\n",
            "(\"'\", 'almost') 1\n",
            "('almost', 'always') 1\n",
            "('always', 'indicates') 1\n",
            "('indicates', 'the') 1\n",
            "('the', 'musical') 1\n",
            "('the', 'seeds') 1\n",
            "('seeds', 'are') 1\n",
            "('are', 'used') 3\n",
            "('train', 'an') 1\n",
            "('an', 'initial') 1\n",
            "('initial', 'classifier') 1\n",
            "('classifier', ',') 1\n",
            "('any', 'supervised') 1\n",
            "('supervised', 'method') 1\n",
            "('this', 'classifier') 1\n",
            "('used', 'on') 1\n",
            "('the', 'untagged') 1\n",
            "('untagged', 'portion') 1\n",
            "('portion', 'of') 1\n",
            "('to', 'extract') 1\n",
            "('extract', 'a') 1\n",
            "('a', 'larger') 1\n",
            "('larger', 'training') 2\n",
            "('training', 'set') 1\n",
            "('set', ',') 1\n",
            "('which', 'only') 1\n",
            "('only', 'the') 2\n",
            "('most', 'confident') 1\n",
            "('confident', 'classifications') 1\n",
            "('classifications', 'are') 1\n",
            "('are', 'included') 1\n",
            "('included', '.') 2\n",
            "('process', 'repeats') 1\n",
            "('repeats', ',') 1\n",
            "(',', 'each') 1\n",
            "('each', 'new') 1\n",
            "('new', 'classifier') 1\n",
            "('classifier', 'being') 1\n",
            "('being', 'trained') 1\n",
            "('trained', 'on') 1\n",
            "('a', 'successively') 1\n",
            "('successively', 'larger') 1\n",
            "(',', 'until') 1\n",
            "('until', 'the') 1\n",
            "('the', 'whole') 1\n",
            "('whole', 'corpus') 1\n",
            "('is', 'consumed') 1\n",
            "('consumed', ',') 1\n",
            "('or', 'until') 1\n",
            "('until', 'a') 1\n",
            "('given', 'maximum') 1\n",
            "('maximum', 'number') 1\n",
            "('of', 'iterations') 1\n",
            "('iterations', 'is') 1\n",
            "('is', 'reached') 1\n",
            "('reached', '.') 1\n",
            "('other', 'semi-supervised') 1\n",
            "('semi-supervised', 'techniques') 1\n",
            "('techniques', 'use') 1\n",
            "('use', 'large') 1\n",
            "('large', 'quantities') 1\n",
            "('quantities', 'of') 1\n",
            "('of', 'untagged') 1\n",
            "('untagged', 'corpora') 1\n",
            "('to', 'provide') 1\n",
            "('provide', 'co-occurrence') 1\n",
            "('co-occurrence', 'information') 1\n",
            "('information', 'that') 1\n",
            "('that', 'supplements') 1\n",
            "('supplements', 'the') 1\n",
            "('the', 'tagged') 1\n",
            "('tagged', 'corpora') 1\n",
            "('these', 'techniques') 1\n",
            "('have', 'the') 1\n",
            "('the', 'potential') 1\n",
            "('potential', 'to') 1\n",
            "('to', 'help') 1\n",
            "('help', 'in') 1\n",
            "('the', 'adaptation') 1\n",
            "('adaptation', 'of') 1\n",
            "('of', 'supervised') 1\n",
            "('supervised', 'models') 1\n",
            "('models', 'to') 1\n",
            "('to', 'different') 1\n",
            "('different', 'domains') 1\n",
            "(',', 'an') 1\n",
            "('an', 'ambiguous') 1\n",
            "('ambiguous', 'word') 1\n",
            "('one', 'language') 1\n",
            "('language', 'is') 1\n",
            "('often', 'translated') 1\n",
            "('translated', 'into') 1\n",
            "('into', 'different') 1\n",
            "('different', 'words') 1\n",
            "('a', 'second') 1\n",
            "('second', 'language') 1\n",
            "('language', 'depending') 1\n",
            "('depending', 'on') 1\n",
            "('.', 'word-aligned') 1\n",
            "('bilingual', 'corpora') 1\n",
            "('corpora', 'have') 1\n",
            "('been', 'used') 1\n",
            "('to', 'infer') 1\n",
            "('infer', 'cross-lingual') 1\n",
            "('cross-lingual', 'sense') 1\n",
            "('a', 'kind') 1\n",
            "('kind', 'of') 1\n",
            "('of', 'semi-supervised') 1\n",
            "('semi-supervised', 'system') 1\n",
            "('system', '.') 1\n",
            "('methods', 'main') 1\n",
            "('main', 'article') 2\n",
            "('article', ':') 2\n",
            "(':', 'word') 1\n",
            "('sense', 'induction') 4\n",
            "('induction', 'unsupervised') 1\n",
            "('unsupervised', 'learning') 2\n",
            "('learning', 'is') 1\n",
            "('greatest', 'challenge') 1\n",
            "('challenge', 'for') 1\n",
            "('wsd', 'researchers') 1\n",
            "('researchers', '.') 1\n",
            "('the', 'underlying') 1\n",
            "('underlying', 'assumption') 1\n",
            "('assumption', 'is') 1\n",
            "('is', 'that') 2\n",
            "('that', 'similar') 1\n",
            "('similar', 'senses') 1\n",
            "('senses', 'occur') 1\n",
            "('occur', 'in') 1\n",
            "('in', 'similar') 1\n",
            "('similar', 'contexts') 1\n",
            "('contexts', ',') 1\n",
            "('and', 'thus') 1\n",
            "('thus', 'senses') 1\n",
            "('be', 'induced') 1\n",
            "('induced', 'from') 1\n",
            "('from', 'text') 1\n",
            "('text', 'by') 1\n",
            "('by', 'clustering') 1\n",
            "('clustering', 'word') 1\n",
            "('occurrences', 'using') 1\n",
            "('using', 'some') 1\n",
            "('some', 'measure') 1\n",
            "('measure', 'of') 1\n",
            "('of', 'similarity') 1\n",
            "('of', 'context') 1\n",
            "('context', ',') 1\n",
            "('task', 'referred') 1\n",
            "('referred', 'to') 1\n",
            "('to', 'as') 1\n",
            "('as', 'word') 2\n",
            "('induction', 'or') 1\n",
            "('or', 'discrimination') 1\n",
            "('.', 'then') 1\n",
            "('then', ',') 1\n",
            "(',', 'new') 2\n",
            "('new', 'occurrences') 1\n",
            "('be', 'classified') 2\n",
            "('classified', 'into') 1\n",
            "('the', 'closest') 1\n",
            "('closest', 'induced') 1\n",
            "('induced', 'clusters\\\\/senses') 1\n",
            "('clusters\\\\/senses', '.') 1\n",
            "('.', 'performance') 1\n",
            "('performance', 'has') 1\n",
            "('been', 'lower') 1\n",
            "('lower', 'than') 1\n",
            "('than', 'other') 1\n",
            "('other', 'methods') 1\n",
            "(',', 'above') 1\n",
            "('but', 'comparisons') 1\n",
            "('comparisons', 'are') 1\n",
            "('are', 'difficult') 1\n",
            "('difficult', 'since') 1\n",
            "('since', 'senses') 1\n",
            "('senses', 'induced') 1\n",
            "('induced', 'must') 1\n",
            "('must', 'be') 1\n",
            "('be', 'mapped') 1\n",
            "('mapped', 'to') 1\n",
            "('a', 'known') 1\n",
            "('known', 'dictionary') 1\n",
            "('.', 'if') 1\n",
            "('if', 'a') 2\n",
            "('a', 'mapping') 1\n",
            "('mapping', 'to') 1\n",
            "('not', 'desired') 1\n",
            "('desired', ',') 1\n",
            "(',', 'cluster-based') 1\n",
            "('cluster-based', 'evaluations') 1\n",
            "('evaluations', '-lrb-') 1\n",
            "('-lrb-', 'including') 1\n",
            "('including', 'measures') 1\n",
            "('measures', 'of') 1\n",
            "('of', 'entropy') 1\n",
            "('entropy', 'and') 1\n",
            "('and', 'purity') 1\n",
            "('purity', '-rrb-') 1\n",
            "('-rrb-', 'can') 1\n",
            "('be', 'performed') 1\n",
            "('performed', '.') 1\n",
            "('.', 'alternatively') 1\n",
            "('alternatively', ',') 1\n",
            "(',', 'word') 1\n",
            "('induction', 'methods') 1\n",
            "('methods', 'can') 2\n",
            "('be', 'tested') 2\n",
            "('tested', 'and') 1\n",
            "('and', 'compared') 1\n",
            "('compared', 'within') 1\n",
            "('within', 'an') 1\n",
            "('an', 'application') 1\n",
            "('application', '.') 1\n",
            "('for', 'instance') 1\n",
            "('instance', ',') 1\n",
            "('shown', 'that') 1\n",
            "('that', 'word') 1\n",
            "('induction', 'improves') 1\n",
            "('improves', 'web') 1\n",
            "('web', 'search') 2\n",
            "('search', 'result') 1\n",
            "('result', 'clustering') 1\n",
            "('clustering', 'by') 1\n",
            "('by', 'increasing') 1\n",
            "('increasing', 'the') 1\n",
            "('the', 'quality') 1\n",
            "('quality', 'of') 1\n",
            "('of', 'result') 2\n",
            "('result', 'clusters') 1\n",
            "('clusters', 'and') 1\n",
            "('the', 'degree') 1\n",
            "('degree', 'diversification') 1\n",
            "('diversification', 'of') 1\n",
            "('result', 'lists') 1\n",
            "('lists', '.') 1\n",
            "('is', 'hoped') 1\n",
            "('hoped', 'that') 1\n",
            "('that', 'unsupervised') 1\n",
            "('learning', 'will') 1\n",
            "('will', 'overcome') 1\n",
            "('overcome', 'the') 1\n",
            "('bottleneck', 'because') 1\n",
            "('not', 'dependent') 1\n",
            "('dependent', 'on') 1\n",
            "('on', 'manual') 1\n",
            "('manual', 'effort') 1\n",
            "('effort', '.') 1\n",
            "('other', 'approaches') 2\n",
            "('approaches', 'other') 1\n",
            "('approaches', 'may') 1\n",
            "('may', 'vary') 1\n",
            "('vary', 'differently') 1\n",
            "('differently', 'in') 1\n",
            "('their', 'methods') 1\n",
            "(':', 'identification') 1\n",
            "('identification', 'of') 1\n",
            "('of', 'dominant') 1\n",
            "('dominant', 'word') 1\n",
            "('senses', ';') 1\n",
            "(';', 'domain-driven') 1\n",
            "('domain-driven', 'disambiguation') 1\n",
            "('disambiguation', ';') 1\n",
            "(';', 'wsd') 1\n",
            "('wsd', 'using') 1\n",
            "('using', 'cross-lingual') 1\n",
            "('cross-lingual', 'evidence') 1\n",
            "('.', 'local') 1\n",
            "('local', 'impediments') 1\n",
            "('impediments', 'and') 1\n",
            "('and', 'summary') 1\n",
            "('summary', 'the') 1\n",
            "('bottleneck', 'is') 1\n",
            "('is', 'perhaps') 1\n",
            "('perhaps', 'the') 1\n",
            "('the', 'major') 1\n",
            "('major', 'impediment') 1\n",
            "('impediment', 'to') 1\n",
            "('to', 'solving') 1\n",
            "('solving', 'the') 1\n",
            "('the', 'wsd') 1\n",
            "('wsd', 'problem') 1\n",
            "('methods', 'rely') 1\n",
            "('on', 'knowledge') 1\n",
            "('knowledge', 'about') 1\n",
            "('about', 'word') 1\n",
            "('which', 'is') 1\n",
            "('is', 'barely') 1\n",
            "('barely', 'formulated') 1\n",
            "('formulated', 'in') 1\n",
            "('in', 'dictionaries') 1\n",
            "('lexical', 'databases') 1\n",
            "('databases', '.') 1\n",
            "('methods', 'depend') 1\n",
            "('depend', 'crucially') 1\n",
            "('crucially', 'on') 1\n",
            "('the', 'existence') 1\n",
            "('existence', 'of') 1\n",
            "('manually', 'annotated') 1\n",
            "('annotated', 'examples') 1\n",
            "('examples', 'for') 1\n",
            "('for', 'every') 2\n",
            "('every', 'word') 2\n",
            "('sense', ',') 1\n",
            "('a', 'requisite') 1\n",
            "('requisite', 'that') 1\n",
            "('that', 'can') 2\n",
            "('can', 'so') 1\n",
            "('so', 'far') 1\n",
            "('far', 'be') 1\n",
            "('be', 'met') 1\n",
            "('met', 'only') 1\n",
            "('only', 'for') 1\n",
            "('a', 'handful') 1\n",
            "('handful', 'of') 1\n",
            "('words', 'for') 1\n",
            "('for', 'testing') 2\n",
            "('testing', 'purposes') 1\n",
            "('purposes', ',') 2\n",
            "('as', 'it') 1\n",
            "('is', 'done') 1\n",
            "('done', 'in') 1\n",
            "('the', 'senseval') 1\n",
            "('senseval', 'exercises') 1\n",
            "('.', 'therefore') 2\n",
            "('therefore', ',') 2\n",
            "('most', 'promising') 1\n",
            "('promising', 'trends') 1\n",
            "('trends', 'in') 1\n",
            "('in', 'wsd') 2\n",
            "('wsd', 'research') 1\n",
            "('research', 'is') 1\n",
            "('the', 'largest') 1\n",
            "('largest', 'corpus') 1\n",
            "('corpus', 'ever') 1\n",
            "('ever', 'accessible') 1\n",
            "('accessible', ',') 1\n",
            "('the', 'world') 1\n",
            "('world', 'wide') 1\n",
            "('wide', 'web') 1\n",
            "('web', ',') 1\n",
            "('to', 'acquire') 1\n",
            "('acquire', 'lexical') 1\n",
            "('lexical', 'information') 1\n",
            "('information', 'automatically') 1\n",
            "('automatically', '.') 1\n",
            "('wsd', 'has') 1\n",
            "('been', 'traditionally') 1\n",
            "('traditionally', 'understood') 1\n",
            "('understood', 'as') 1\n",
            "('an', 'intermediate') 1\n",
            "('intermediate', 'language') 1\n",
            "('language', 'engineering') 1\n",
            "('engineering', 'technology') 1\n",
            "('technology', 'which') 1\n",
            "('which', 'could') 2\n",
            "('could', 'improve') 1\n",
            "('improve', 'applications') 1\n",
            "('applications', 'such') 1\n",
            "('as', 'information') 1\n",
            "('retrieval', '-lrb-') 1\n",
            "('-lrb-', 'ir') 1\n",
            "('ir', '-rrb-') 1\n",
            "('in', 'this') 1\n",
            "('this', 'case') 1\n",
            "('case', ',') 1\n",
            "('the', 'reverse') 1\n",
            "('reverse', 'is') 1\n",
            "('also', 'true') 1\n",
            "('true', ':') 1\n",
            "(':', 'web') 1\n",
            "('engines', 'implement') 1\n",
            "('implement', 'simple') 1\n",
            "('simple', 'and') 1\n",
            "('and', 'robust') 1\n",
            "('robust', 'ir') 1\n",
            "('ir', 'techniques') 1\n",
            "('techniques', 'that') 1\n",
            "('be', 'successfully') 1\n",
            "('successfully', 'used') 1\n",
            "('used', 'when') 1\n",
            "('when', 'mining') 1\n",
            "('mining', 'the') 1\n",
            "('the', 'web') 1\n",
            "('web', 'for') 1\n",
            "('for', 'information') 1\n",
            "('information', 'to') 1\n",
            "('be', 'employed') 1\n",
            "('employed', 'in') 1\n",
            "('data', 'provoked') 1\n",
            "('provoked', 'appearing') 1\n",
            "('appearing', 'some') 1\n",
            "('some', 'new') 1\n",
            "('new', 'algorithms') 1\n",
            "('algorithms', 'and') 1\n",
            "('and', 'techniques') 1\n",
            "('techniques', 'described') 1\n",
            "('described', 'here') 1\n",
            "('here', ':') 1\n",
            "(':', 'main') 1\n",
            "(':', 'automatic') 1\n",
            "('automatic', 'acquisition') 1\n",
            "('acquisition', 'of') 1\n",
            "('of', 'sense-tagged') 1\n",
            "('corpora', 'external') 1\n",
            "('external', 'knowledge') 1\n",
            "('knowledge', 'sources') 2\n",
            "('sources', 'knowledge') 1\n",
            "('knowledge', 'is') 1\n",
            "('a', 'fundamental') 1\n",
            "('fundamental', 'component') 1\n",
            "('component', 'of') 1\n",
            "('sources', 'provide') 1\n",
            "('provide', 'data') 1\n",
            "('data', 'which') 1\n",
            "('are', 'essential') 1\n",
            "('essential', 'to') 1\n",
            "('to', 'associate') 1\n",
            "('associate', 'senses') 1\n",
            "('can', 'vary') 1\n",
            "('vary', 'from') 1\n",
            "('from', 'corpora') 1\n",
            "('corpora', 'of') 1\n",
            "('of', 'texts') 1\n",
            "('texts', ',') 1\n",
            "(',', 'either') 1\n",
            "('either', 'unlabeled') 1\n",
            "('unlabeled', 'or') 1\n",
            "('or', 'annotated') 1\n",
            "('annotated', 'with') 1\n",
            "('to', 'machine-readable') 1\n",
            "('machine-readable', 'dictionaries') 2\n",
            "(',', 'glossaries') 1\n",
            "('glossaries', ',') 1\n",
            "(',', 'ontologies') 1\n",
            "('ontologies', ',') 1\n",
            "(',', 'etc') 3\n",
            "('etc', '.') 3\n",
            "('.', '.') 2\n",
            "('classified', 'as') 1\n",
            "('as', 'follows') 1\n",
            "('follows', ':') 1\n",
            "(':', 'structured') 1\n",
            "('structured', ':') 1\n",
            "(':', 'thesauri') 1\n",
            "('thesauri', 'machine-readable') 1\n",
            "('dictionaries', '-lrb-') 1\n",
            "('-lrb-', 'mrds') 1\n",
            "('mrds', '-rrb-') 1\n",
            "('-rrb-', 'ontologies') 1\n",
            "('ontologies', 'unstructured') 1\n",
            "('unstructured', ':') 1\n",
            "(':', 'corpora') 1\n",
            "('corpora', ':') 1\n",
            "(':', 'raw') 1\n",
            "('raw', 'corpora') 1\n",
            "('corpora', 'and') 1\n",
            "('and', 'sense-annotated') 1\n",
            "('corpora', 'collocation') 1\n",
            "('collocation', 'resources') 1\n",
            "('resources', 'other') 1\n",
            "('resources', '-lrb-') 1\n",
            "('-lrb-', 'such') 2\n",
            "('word', 'frequency') 1\n",
            "('frequency', 'lists') 1\n",
            "('lists', ',') 1\n",
            "(',', 'stoplists') 1\n",
            "('stoplists', ',') 1\n",
            "('domain', 'labels') 1\n",
            "('labels', ',') 1\n",
            "('.', '-rrb-') 1\n",
            "('-rrb-', 'evaluation') 1\n",
            "('evaluation', 'comparing') 1\n",
            "('comparing', 'and') 1\n",
            "('and', 'evaluating') 1\n",
            "('evaluating', 'different') 1\n",
            "('different', 'wsd') 1\n",
            "('systems', 'is') 1\n",
            "('is', 'extremely') 1\n",
            "('extremely', 'difﬁcult') 1\n",
            "('difﬁcult', ',') 1\n",
            "('different', 'test') 1\n",
            "('test', 'sets') 1\n",
            "('sets', ',') 1\n",
            "(',', 'sense') 1\n",
            "('sense', 'inventories') 3\n",
            "('inventories', ',') 2\n",
            "('and', 'knowledge') 1\n",
            "('knowledge', 'resources') 1\n",
            "('resources', 'adopted') 1\n",
            "('adopted', '.') 3\n",
            "('.', 'before') 1\n",
            "('before', 'the') 1\n",
            "('the', 'organization') 1\n",
            "('organization', 'of') 1\n",
            "('of', 'speciﬁc') 1\n",
            "('speciﬁc', 'evaluation') 1\n",
            "('evaluation', 'campaigns') 2\n",
            "('campaigns', 'most') 1\n",
            "('most', 'systems') 1\n",
            "('were', 'assessed') 1\n",
            "('assessed', 'on') 1\n",
            "('on', 'in-house') 1\n",
            "('in-house', ',') 1\n",
            "(',', 'often') 1\n",
            "('often', 'small-scale') 1\n",
            "('small-scale', ',') 1\n",
            "(',', 'data') 1\n",
            "('data', 'sets') 1\n",
            "('sets', '.') 1\n",
            "('in', 'order') 2\n",
            "('order', 'to') 2\n",
            "('test', 'one') 1\n",
            "('one', \"'s\") 1\n",
            "(\"'s\", 'algorithm') 1\n",
            "('algorithm', ',') 1\n",
            "(',', 'developers') 1\n",
            "('developers', 'should') 1\n",
            "('should', 'spend') 1\n",
            "('spend', 'their') 1\n",
            "('their', 'time') 1\n",
            "('time', 'to') 1\n",
            "('to', 'annotate') 1\n",
            "('annotate', 'all') 1\n",
            "('all', 'word') 1\n",
            "('and', 'comparing') 1\n",
            "('comparing', 'methods') 1\n",
            "('methods', 'even') 1\n",
            "('even', 'on') 1\n",
            "('same', 'corpus') 1\n",
            "('not', 'eligible') 1\n",
            "('eligible', 'if') 1\n",
            "('if', 'there') 1\n",
            "('is', 'different') 1\n",
            "('different', 'sense') 1\n",
            "('inventories', '.') 2\n",
            "('to', 'define') 1\n",
            "('define', 'common') 1\n",
            "('common', 'evaluation') 1\n",
            "('evaluation', 'datasets') 1\n",
            "('datasets', 'and') 1\n",
            "('and', 'procedures') 1\n",
            "('procedures', ',') 1\n",
            "(',', 'public') 1\n",
            "('public', 'evaluation') 1\n",
            "('campaigns', 'have') 1\n",
            "('been', 'organized') 1\n",
            "('organized', '.') 1\n",
            "('.', 'senseval') 1\n",
            "('senseval', '-lrb-') 1\n",
            "('-lrb-', 'now') 1\n",
            "('now', 'renamed') 1\n",
            "('renamed', 'semeval') 1\n",
            "('semeval', '-rrb-') 1\n",
            "('an', 'international') 1\n",
            "('international', 'word') 1\n",
            "('disambiguation', 'competition') 1\n",
            "('competition', ',') 1\n",
            "(',', 'held') 1\n",
            "('held', 'every') 1\n",
            "('every', 'three') 1\n",
            "('three', 'years') 1\n",
            "('years', 'since') 1\n",
            "('since', '1998') 1\n",
            "('1998', ':') 1\n",
            "(':', 'senseval-1') 1\n",
            "('senseval-1', '-lrb-') 1\n",
            "('-lrb-', '1998') 1\n",
            "('1998', '-rrb-') 1\n",
            "('senseval-2', '-lrb-') 1\n",
            "('-lrb-', '2001') 1\n",
            "('2001', '-rrb-') 1\n",
            "(',', 'senseval-3') 1\n",
            "('senseval-3', '-lrb-') 1\n",
            "('-lrb-', '2004') 1\n",
            "('2004', '-rrb-') 1\n",
            "('its', 'successor') 1\n",
            "('successor', ',') 1\n",
            "(',', 'semeval') 1\n",
            "('semeval', '-lrb-') 1\n",
            "('-lrb-', '2007') 1\n",
            "('2007', '-rrb-') 1\n",
            "('the', 'objective') 1\n",
            "('objective', 'of') 1\n",
            "('the', 'competition') 1\n",
            "('competition', 'is') 1\n",
            "('to', 'organize') 1\n",
            "('organize', 'different') 1\n",
            "('different', 'lectures') 1\n",
            "('lectures', ',') 1\n",
            "(',', 'preparing') 1\n",
            "('preparing', 'and') 1\n",
            "('and', 'hand-annotating') 1\n",
            "('hand-annotating', 'corpus') 1\n",
            "('corpus', 'for') 1\n",
            "('testing', 'systems') 1\n",
            "('perform', 'a') 1\n",
            "('a', 'comparative') 1\n",
            "('comparative', 'evaluation') 1\n",
            "('evaluation', 'of') 2\n",
            "('systems', 'in') 1\n",
            "('in', 'several') 1\n",
            "('several', 'kinds') 1\n",
            "('kinds', 'of') 1\n",
            "('of', 'tasks') 1\n",
            "('tasks', ',') 3\n",
            "('including', 'all-words') 1\n",
            "('all-words', 'and') 1\n",
            "('sample', 'wsd') 1\n",
            "('wsd', 'for') 1\n",
            "('for', 'different') 1\n",
            "('different', 'languages') 1\n",
            "('languages', ',') 1\n",
            "('and', ',') 1\n",
            "(',', 'more') 1\n",
            "('more', 'recently') 1\n",
            "('new', 'tasks') 1\n",
            "('tasks', 'such') 1\n",
            "('as', 'semantic') 1\n",
            "('semantic', 'role') 1\n",
            "('role', 'labeling') 1\n",
            "('labeling', ',') 1\n",
            "(',', 'gloss') 1\n",
            "('gloss', 'wsd') 1\n",
            "(',', 'lexical') 1\n",
            "('substitution', ',') 1\n",
            "('the', 'systems') 2\n",
            "('systems', 'submitted') 1\n",
            "('submitted', 'for') 1\n",
            "('for', 'evaluation') 1\n",
            "('evaluation', 'to') 1\n",
            "('to', 'these') 1\n",
            "('these', 'competitions') 1\n",
            "('competitions', 'usually') 1\n",
            "('usually', 'integrate') 1\n",
            "('integrate', 'different') 1\n",
            "('different', 'techniques') 1\n",
            "('techniques', 'and') 1\n",
            "('and', 'often') 1\n",
            "('often', 'combine') 1\n",
            "('combine', 'supervised') 1\n",
            "('supervised', 'and') 1\n",
            "('methods', '-lrb-') 1\n",
            "('-lrb-', 'especially') 1\n",
            "('especially', 'for') 1\n",
            "('for', 'avoiding') 1\n",
            "('avoiding', 'bad') 1\n",
            "('bad', 'performance') 1\n",
            "('in', 'lack') 1\n",
            "('examples', '-rrb-') 1\n",
            "('.', 'task') 1\n",
            "('task', 'design') 1\n",
            "('design', 'choices') 1\n",
            "('choices', 'sense') 1\n",
            "('.', 'during') 2\n",
            "('first', 'senseval') 1\n",
            "('senseval', 'workshop') 2\n",
            "('workshop', 'the') 2\n",
            "('the', 'hector') 2\n",
            "('hector', 'sense') 1\n",
            "('inventory', 'was') 3\n",
            "('was', 'adopted') 1\n",
            "('the', 'reason') 2\n",
            "('reason', 'for') 2\n",
            "('for', 'adopting') 2\n",
            "('adopting', 'a') 1\n",
            "('a', 'previously') 1\n",
            "('previously', 'unknown') 1\n",
            "('unknown', 'sense') 1\n",
            "('was', 'mainly') 1\n",
            "('mainly', 'to') 1\n",
            "('to', 'avoid') 1\n",
            "('avoid', 'the') 1\n",
            "('of', 'popular') 1\n",
            "('popular', 'fine-grained') 1\n",
            "('fine-grained', 'word') 1\n",
            "('senses', '-lrb-') 1\n",
            "('wordnet', '-rrb-') 2\n",
            "('could', 'make') 1\n",
            "('make', 'the') 1\n",
            "('the', 'experiments') 1\n",
            "('experiments', 'unfair') 1\n",
            "('unfair', 'or') 1\n",
            "('or', 'biased') 1\n",
            "('biased', '.') 1\n",
            "('of', 'coverage') 1\n",
            "('coverage', 'of') 1\n",
            "('such', 'inventories') 1\n",
            "('second', 'senseval') 1\n",
            "('the', 'wordnet') 2\n",
            "('wordnet', 'sense') 1\n",
            "('inventory', 'has') 1\n",
            "('been', 'adopted') 1\n",
            "('of', 'testing') 1\n",
            "('testing', 'words') 1\n",
            "('.', 'comparison') 1\n",
            "('comparison', 'of') 1\n",
            "('of', 'methods') 1\n",
            "('be', 'divided') 1\n",
            "('divided', 'in') 1\n",
            "('in', '2') 1\n",
            "('2', 'groups') 1\n",
            "('groups', 'by') 1\n",
            "('by', 'amount') 1\n",
            "('words', 'to') 1\n",
            "('test', '.') 1\n",
            "('the', 'difference') 1\n",
            "('difference', 'consists') 1\n",
            "('consists', 'in') 2\n",
            "('the', 'amount') 1\n",
            "('of', 'analysis') 1\n",
            "('analysis', 'and') 1\n",
            "('and', 'processing') 1\n",
            "('processing', ':') 1\n",
            "(':', 'all-words') 1\n",
            "('all-words', 'task') 1\n",
            "('task', 'implies') 1\n",
            "('implies', 'disambiguating') 1\n",
            "('disambiguating', 'all') 1\n",
            "('words', 'of') 1\n",
            "('text', 'lexical') 1\n",
            "('sample', 'consists') 1\n",
            "('in', 'disambiguating') 1\n",
            "('disambiguating', 'some') 1\n",
            "('some', 'previously') 1\n",
            "('previously', 'chosen') 1\n",
            "('chosen', 'target') 1\n",
            "('is', 'assumed') 1\n",
            "('assumed', 'that') 1\n",
            "('former', 'one') 1\n",
            "('one', 'is') 1\n",
            "('realistic', 'evaluation') 1\n",
            "(',', 'although') 1\n",
            "('although', 'with') 1\n",
            "('with', 'very') 1\n",
            "('very', 'laborious') 1\n",
            "('laborious', 'testing') 1\n",
            "('testing', 'of') 1\n",
            "('of', 'results') 1\n",
            "('results', '.') 1\n",
            "('.', 'initially') 1\n",
            "('initially', 'only') 1\n",
            "('latter', 'was') 1\n",
            "('was', 'used') 1\n",
            "('in', 'evaluation') 1\n",
            "('evaluation', 'but') 1\n",
            "('but', 'later') 1\n",
            "('later', 'the') 1\n",
            "('former', 'was') 1\n",
            "('was', 'included') 1\n",
            "('.', 'lexical') 1\n",
            "('sample', 'organizers') 1\n",
            "('organizers', 'had') 1\n",
            "('had', 'to') 1\n",
            "('choose', 'samples') 1\n",
            "('samples', 'on') 1\n",
            "('which', 'the') 1\n",
            "('were', 'to') 1\n",
            "('tested', '.') 1\n",
            "('a', 'criticism') 1\n",
            "('criticism', 'of') 1\n",
            "('of', 'earlier') 1\n",
            "('earlier', 'forays') 1\n",
            "('forays', 'into') 1\n",
            "('into', 'lexical-sample') 1\n",
            "('lexical-sample', 'wsd') 1\n",
            "('evaluation', 'is') 1\n",
            "('the', 'lexical') 1\n",
            "('sample', 'had') 1\n",
            "('had', 'been') 1\n",
            "('been', 'chosen') 1\n",
            "('chosen', 'according') 1\n",
            "('according', 'to') 2\n",
            "('the', 'whim') 1\n",
            "('whim', 'of') 1\n",
            "('the', 'experimenter') 1\n",
            "('experimenter', '-lrb-') 1\n",
            "('or', ',') 1\n",
            "('to', 'coincide') 1\n",
            "('coincide', 'with') 1\n",
            "('with', 'earlier') 1\n",
            "('earlier', 'experimenters') 1\n",
            "('experimenters', \"'\") 1\n",
            "(\"'\", 'selections') 1\n",
            "('selections', '-rrb-') 1\n",
            "('english', 'senseval') 1\n",
            "('senseval', ',') 1\n",
            "('a', 'sampling') 1\n",
            "('sampling', 'frame') 1\n",
            "('frame', 'was') 1\n",
            "('was', 'devised') 1\n",
            "('devised', 'in') 1\n",
            "('which', 'words') 1\n",
            "('words', 'were') 1\n",
            "('were', 'classified') 1\n",
            "('classified', 'according') 1\n",
            "('to', 'their') 1\n",
            "('their', 'frequency') 1\n",
            "('frequency', '-lrb-') 1\n",
            "('-lrb-', 'in') 2\n",
            "('the', 'bnc') 1\n",
            "('bnc', '-rrb-') 1\n",
            "('-rrb-', 'and') 1\n",
            "('their', 'polysemy') 1\n",
            "('polysemy', 'level') 1\n",
            "('in', 'wordnet') 1\n",
            "(',', 'inclusion') 1\n",
            "('inclusion', 'pos-tagging') 1\n",
            "('pos-tagging', 'problem') 1\n",
            "('problem', 'was') 1\n",
            "('a', 'matter') 1\n",
            "('matter', 'of') 1\n",
            "('of', 'discussion') 1\n",
            "('discussion', 'and') 1\n",
            "('and', 'it') 1\n",
            "('was', 'decided') 1\n",
            "('decided', 'that') 1\n",
            "('that', 'samples') 1\n",
            "('samples', 'should') 1\n",
            "('be', 'words') 1\n",
            "('words', 'with') 1\n",
            "('with', 'known') 1\n",
            "('known', 'part') 1\n",
            "('speech', 'and') 1\n",
            "('and', 'some') 1\n",
            "('some', 'indeterminants') 1\n",
            "('indeterminants', '-lrb-') 1\n",
            "('-lrb-', 'for') 1\n",
            "('for', 'ex') 1\n",
            "('ex', '.') 1\n",
            "('.', '15') 1\n",
            "('15', 'noun') 1\n",
            "('noun', 'tasks') 1\n",
            "(',', '13') 1\n",
            "('13', 'verb') 1\n",
            "('verb', 'tasks') 1\n",
            "(',', '8') 1\n",
            "('8', 'adjectives') 1\n",
            "('adjectives', ',') 1\n",
            "('and', '5') 1\n",
            "('5', 'indeterminates') 1\n",
            "('indeterminates', '-rrb-') 1\n",
            "('.', 'baselines') 1\n",
            "('baselines', '.') 1\n",
            "('for', 'comparison') 1\n",
            "('comparison', 'purposes') 1\n",
            "(',', 'known') 1\n",
            "('known', ',') 1\n",
            "(',', 'yet') 1\n",
            "('yet', 'simple') 1\n",
            "('simple', ',') 1\n",
            "('algorithms', 'named') 1\n",
            "('named', 'baselines') 1\n",
            "('baselines', 'are') 1\n",
            "('these', 'include') 1\n",
            "('include', 'different') 1\n",
            "('different', 'variants') 1\n",
            "('variants', 'of') 1\n",
            "('of', 'lesk') 1\n",
            "('algorithm', 'or') 1\n",
            "('or', 'most') 1\n",
            "('sense', 'algorithm') 1\n",
            "('inventory', '.') 2\n",
            "('wsd', 'exercises') 1\n",
            "('exercises', 'require') 1\n",
            "('require', 'a') 1\n",
            "('a', 'dictionary') 1\n",
            "('to', 'specify') 1\n",
            "('specify', 'the') 1\n",
            "('senses', 'which') 1\n",
            "('are', 'to') 1\n",
            "('disambiguated', ',') 1\n",
            "('of', 'language') 1\n",
            "('language', 'data') 1\n",
            "('data', 'to') 1\n",
            "('most', 'popular') 1\n",
            "('popular', 'example') 1\n",
            "('of', 'sense') 1\n",
            "('adopting', 'the') 1\n",
            "('hector', 'database') 1\n",
            "('database', 'during') 1\n",
            "('during', 'senseval-1') 1\n",
            "('senseval-1', 'was') 1\n",
            "('wordnet', 'inventory') 1\n",
            "('was', 'already') 1\n",
            "('already', 'publicly') 1\n",
            "('publicly', 'available') 1\n",
            "('available', '.') 1\n",
            "('.', 'evaluation') 1\n",
            "('evaluation', 'measures') 1\n",
            "('measures', '.') 1\n",
            "('the', 'evaluation') 1\n",
            "('systems', 'two') 1\n",
            "('main', 'performance') 1\n",
            "('performance', 'measures') 1\n",
            "('measures', 'are') 1\n",
            "('used', ':') 1\n",
            "(':', 'precision') 1\n",
            "('precision', ':') 1\n",
            "(':', 'the') 2\n",
            "('the', 'fraction') 2\n",
            "('fraction', 'of') 2\n",
            "('of', 'system') 1\n",
            "('system', 'assignments') 1\n",
            "('assignments', 'made') 1\n",
            "('made', 'that') 1\n",
            "('that', 'are') 1\n",
            "('are', 'correct') 1\n",
            "('correct', 'recall') 1\n",
            "('recall', ':') 1\n",
            "('of', 'total') 1\n",
            "('total', 'word') 1\n",
            "('word', 'instances') 1\n",
            "('instances', 'correctly') 1\n",
            "('correctly', 'assigned') 1\n",
            "('assigned', 'by') 1\n",
            "('by', 'a') 1\n",
            "('a', 'system') 2\n",
            "('system', 'if') 1\n",
            "('system', 'makes') 1\n",
            "('makes', 'an') 1\n",
            "('an', 'assignment') 1\n",
            "('assignment', 'for') 1\n",
            "('then', 'precision') 1\n",
            "('precision', 'and') 1\n",
            "('and', 'recall') 1\n",
            "('recall', 'are') 1\n",
            "('are', 'the') 1\n",
            "('same', ',') 1\n",
            "('and', 'can') 1\n",
            "('be', 'called') 1\n",
            "('called', 'accuracy') 1\n",
            "('accuracy', '.') 1\n",
            "('this', 'model') 1\n",
            "('model', 'has') 1\n",
            "('been', 'extended') 1\n",
            "('extended', 'to') 1\n",
            "('to', 'take') 1\n",
            "('take', 'into') 1\n",
            "('into', 'account') 1\n",
            "('account', 'systems') 1\n",
            "('systems', 'that') 1\n",
            "('that', 'return') 1\n",
            "('return', 'a') 1\n",
            "('with', 'weights') 1\n",
            "('weights', 'for') 1\n",
            "('each', 'occurrence') 1\n",
            "('occurrence', '.') 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvRQ64lwRO5T",
        "colab_type": "code",
        "outputId": "cc9f2902-c56f-47d4-b62b-c3feae35c326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "with open(\"/content/02-train-answer.txt\", \"r\") as f:\n",
        "  sample_text = f.read()\n",
        "  print(sample_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "</s>\t0.250000\n",
            "<s> a\t1.000000\n",
            "a\t0.250000\n",
            "a b\t1.000000\n",
            "b\t0.250000\n",
            "b c\t0.500000\n",
            "b d\t0.500000\n",
            "c\t0.125000\n",
            "c </s>\t1.000000\n",
            "d\t0.125000\n",
            "d </s>\t1.000000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ipULikbReth",
        "colab_type": "code",
        "outputId": "6d6141aa-4c68-409b-e01e-655c2c62a268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "with open(\"/content/02-train-input.txt\", \"r\") as f:\n",
        "  sample_text = f.read()\n",
        "\n",
        "tokens = nltk.word_tokenize(sample_text)\n",
        "bigram = nltk.ngrams(tokens,   2) # n=2 is the number of n in n_gram\n",
        "\n",
        "token_freq = nltk.FreqDist(bigram)\n",
        "for token in token_freq.keys():\n",
        "  print(token,token_freq[token])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a', 'b') 2\n",
            "('b', 'c') 1\n",
            "('c', 'a') 1\n",
            "('b', 'd') 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0DtkWKtGgOy",
        "colab_type": "code",
        "outputId": "17a5a3c5-1132-4d91-e7db-7fd288bc3bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(\"/content/02-train-input.txt\", \"r\") as f:\n",
        "  sample_text = f.read()\n",
        "  text = sample_text.strip()\n",
        "  words = text.split()\n",
        "  words.append('</s>')\n",
        "  words.insert(0, '<s>')\n",
        "\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', 'a', 'b', 'c', 'a', 'b', 'd', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRWnrpZPSxAc",
        "colab_type": "code",
        "outputId": "dc0a467c-f64a-44c6-af8e-14268bf31c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(\"/content/02-train-input.txt\") as f:\n",
        "  for line in f:\n",
        "    # strip() throws the first white space (if there is none -> do nothing!)\n",
        "    line = line.strip()\n",
        "    \n",
        "    #if line == '':\n",
        "    #   continue  \n",
        "\n",
        "    # split create a list from string\n",
        "    words = line.split()\n",
        "    words.append('</s>')\n",
        "    words.insert(0, '<s>')\n",
        "\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'a', 'b', 'd', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tt0GPEbFXlP",
        "colab_type": "code",
        "outputId": "0c454e05-3bb4-44a0-c537-65df627ec99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test = \" Here is the testing string, do everything you want Here, ok???\"\n",
        "test.split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here',\n",
              " 'is',\n",
              " 'the',\n",
              " 'testing',\n",
              " 'string,',\n",
              " 'do',\n",
              " 'everything',\n",
              " 'you',\n",
              " 'want',\n",
              " 'Here,',\n",
              " 'ok???']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOfcbu98Fj7t",
        "colab_type": "code",
        "outputId": "7222415a-2ca4-4ed4-9376-25e1ddc0b79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test.split(\", \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Here is the testing string', 'do everything you want Here', 'ok???']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK59xYRPFnwq",
        "colab_type": "code",
        "outputId": "ca08f9a3-9923-4909-be21-d6803f9e7356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test.strip()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the testing string, do everything you want Here, ok???'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS3vIagCFrHG",
        "colab_type": "code",
        "outputId": "f44fe259-2168-4aaa-92be-6f47f4c42a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# left side in elements only\n",
        "test.strip(\" Here\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is the testing string, do everything you want Here, ok???'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoxi8pyTGEog",
        "colab_type": "code",
        "outputId": "c53b62e6-7f14-460f-8abf-a562e4235c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_test = \"Hello there!!!\"\n",
        "new_test.strip()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello there!!!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtxHJfIzd0HC",
        "colab_type": "code",
        "outputId": "ded97d71-10b2-44fa-f3a1-af4230020419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token_freq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('a', 'b'): 2, ('b', 'c'): 1, ('b', 'd'): 1, ('c', 'a'): 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK4EWMp-Hh7Y",
        "colab_type": "code",
        "outputId": "c3614a2a-2090-42f0-e0e3-6ab14df60581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "counts = defaultdict(int)  # count the n-gram\n",
        "context_counts = defaultdict(int)   # count the context\n",
        "print(counts)\n",
        "print(context_counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {})\n",
            "defaultdict(<class 'int'>, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQyRptBSl7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLP course:\n",
        "# Application: QA, Information Extraction, Sentiment Analysis (positive&negative), Machine Translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8afVYVwWHiG4",
        "colab_type": "code",
        "outputId": "635fef04-8256-4f12-e33a-b6298aac2890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Regular expression: Regex\n",
        "import re\n",
        "\n",
        "txt = \"The rain in Spain\"\n",
        "\n",
        "# Sets [ant] (a or n or t)\n",
        "# [A-Z] from A to Z (capital letter)\n",
        "# [^a] not a (negation)\n",
        "\n",
        "x = re.findall(\"[a-m]\", txt)\n",
        "y = re.findall(\"[rai]\", txt)\n",
        "z = re.findall(\"[^h]\", txt)\n",
        "t = re.findall(\"[^h-z]\", txt)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['h', 'e', 'a', 'i', 'i', 'a', 'i']\n",
            "['r', 'a', 'i', 'i', 'a', 'i']\n",
            "['T', 'e', ' ', 'r', 'a', 'i', 'n', ' ', 'i', 'n', ' ', 'S', 'p', 'a', 'i', 'n']\n",
            "['T', 'e', ' ', 'a', ' ', ' ', 'S', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM4XgxcAQiXl",
        "colab_type": "code",
        "outputId": "90d59c9b-f7ce-413b-8404-f328cafc8b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "txt2 = \"And i saw him. The cat in the hat, the other in there, the blithe one.\"\n",
        "\n",
        "print(\"the with capital T and lower case t: \", re.findall(\"[Tt]he\", txt2))\n",
        "# \"blithe\" and \"there\" is still in count, to get rid of this\n",
        "print(re.findall(\"[Tt]he[^A-Za-z]\", txt2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the with capital T and lower case t:  ['The', 'the', 'the', 'the', 'the', 'the', 'the']\n",
            "['The ', 'the ', 'the ', 'the ', 'the ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atlxcUxv3UQb",
        "colab_type": "code",
        "outputId": "9bdf5c03-698e-4065-a2a5-167fb1fdc2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "for i in range(1,10):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQmEF5R3WgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}