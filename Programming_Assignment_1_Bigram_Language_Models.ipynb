{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Programming_Assignment_1_Bigram_Language_Models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanmanhtung/Natural-Language-Processing-Course/blob/master/Programming_Assignment_1_Bigram_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdJyC8fm_JR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Programming Assignment 1: Bigram Language Models\n",
        "\n",
        "Full name: PHAN MANH TUNG\n",
        "Student number: USTHBI8-160\n",
        "Email address: tungpm.b8160@st.usth.edu.vn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiPJy3jk_saG",
        "colab_type": "code",
        "outputId": "c1475c75-2c6d-477b-b03f-f22f36753d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "# Data\n",
        "!rm -f wiki-en-train.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
        "    \n",
        "!rm -f wiki-en-test.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
        "\n",
        "!rm -f 02-train-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
        "\n",
        "!rm -f 02-train-answer.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-08 14:44:48--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203886 (199K) [text/plain]\n",
            "Saving to: ‘wiki-en-train.word’\n",
            "\n",
            "\rwiki-en-train.word    0%[                    ]       0  --.-KB/s               \rwiki-en-train.word  100%[===================>] 199.11K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-02-08 14:44:48 (3.99 MB/s) - ‘wiki-en-train.word’ saved [203886/203886]\n",
            "\n",
            "--2020-02-08 14:44:51--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26989 (26K) [text/plain]\n",
            "Saving to: ‘wiki-en-test.word’\n",
            "\n",
            "wiki-en-test.word   100%[===================>]  26.36K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-02-08 14:44:51 (2.07 MB/s) - ‘wiki-en-test.word’ saved [26989/26989]\n",
            "\n",
            "--2020-02-08 14:44:54--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12 [text/plain]\n",
            "Saving to: ‘02-train-input.txt’\n",
            "\n",
            "02-train-input.txt  100%[===================>]      12  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-08 14:44:54 (1.76 MB/s) - ‘02-train-input.txt’ saved [12/12]\n",
            "\n",
            "--2020-02-08 14:44:57--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-answer.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144 [text/plain]\n",
            "Saving to: ‘02-train-answer.txt’\n",
            "\n",
            "02-train-answer.txt 100%[===================>]     144  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-08 14:44:57 (38.9 MB/s) - ‘02-train-answer.txt’ saved [144/144]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNmb6omgAPX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 1: Estimating probabilities\n",
        "# Part 1.0: Function train_unigram()\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# UNIGRAM\n",
        "def train_unigram(train_file, model_file):\n",
        "    counts = defaultdict(int)  # count the n-gram (a map count)\n",
        "    total_count = 0\n",
        "    #context_counts = defaultdict(int)   # count the context\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "\n",
        "            for word in words:\n",
        "              counts[word] += 1\n",
        "              total_count +=1\n",
        "              pass # do nothing\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "\n",
        "            fo.write((ngram + \"\\t\" + str(counts[ngram]/total_count))+ \"\\n\")\n",
        "\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWPwflCxAQR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_unigram('02-train-input.txt', 'train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pImm337bAQWF",
        "colab_type": "code",
        "outputId": "f894f19f-94e2-4bf7-b00d-7918c8d9ec31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!cat train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\t0.25\n",
            "b\t0.25\n",
            "c\t0.125\n",
            "</s>\t0.25\n",
            "d\t0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk-ICARZAQUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 1.1: Function train_bigram()\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# BIGRAM\n",
        "def train_bigram(train_file, model_file):\n",
        "    counts = defaultdict(int)  # count the n-gram (a map count)\n",
        "    total_count = 0\n",
        "    context_counts = defaultdict(int)\n",
        "    #context_counts = defaultdict(int)   # count the context\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "\n",
        "            for i in range(1, len(words)):\n",
        "                # Add bigram and bigram context\n",
        "                counts[words[i-1]+ \" \"+ words[i]] += 1             \n",
        "                context_counts[words[i-1]] += 1\n",
        "\n",
        "                # Add unigram and unigram context\n",
        "                total_count += 1\n",
        "                counts[words[i]] += 1\n",
        "                context_counts[\"\"] += 1\n",
        "                pass # do nothing\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "            # split ngram into an array of words\n",
        "            ngram_split = ngram.split()\n",
        "\n",
        "            # BIGRAM\n",
        "            if len(ngram_split) > 1: # \n",
        "              fo.write((ngram + \"\\t\" + str(counts[ngram]/context_counts[ngram_split[0]]))+\"\\n\")\n",
        "\n",
        "            # UNIGRAM\n",
        "            else:\n",
        "              fo.write((ngram + \"\\t\" + str(counts[ngram]/total_count))+\"\\n\")\n",
        "\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVzW0KvqAPV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bigram('02-train-input.txt', 'train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA132x01APSh",
        "colab_type": "code",
        "outputId": "159e33a5-7616-4aeb-f3f4-a35d550b6db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!cat train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> a\t1.0\n",
            "a\t0.25\n",
            "a b\t1.0\n",
            "b\t0.25\n",
            "b c\t0.5\n",
            "c\t0.125\n",
            "c </s>\t1.0\n",
            "</s>\t0.25\n",
            "b d\t0.5\n",
            "d\t0.125\n",
            "d </s>\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNK8fTlUA9R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 1.2: load the model file\n",
        "\n",
        "def load_bigram_model(model_file):\n",
        "    \"\"\"Load the model file\n",
        "\n",
        "    Args:\n",
        "        model_file (str): Path to the model file\n",
        "    \n",
        "    Returns:\n",
        "        probs (dict): Dictionary object that map from ngrams to their probabilities\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    with open(model_file, 'r') as f:\n",
        "        for line in f:\n",
        "            element = line.split(\"\\t\")\n",
        "            probs[element[0]] = float(element[1])\n",
        "            pass\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgherTj-A9jW",
        "colab_type": "code",
        "outputId": "a8d6b313-6968-4b12-88fe-9829acdab266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "probs = load_bigram_model('train-answer.txt')\n",
        "probs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'</s>': 0.25,\n",
              " '<s> a': 1.0,\n",
              " 'a': 0.25,\n",
              " 'a b': 1.0,\n",
              " 'b': 0.25,\n",
              " 'b c': 0.5,\n",
              " 'b d': 0.5,\n",
              " 'c': 0.125,\n",
              " 'c </s>': 1.0,\n",
              " 'd': 0.125,\n",
              " 'd </s>': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBG2sFFuA9PX",
        "colab_type": "code",
        "outputId": "39b3a824-d98b-4e5c-ad6c-a2729f1f3e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(probs[\"a b\"])\n",
        "print(probs.get(\"a b\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fng1KQF_A9Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 2: Evaluating Bigram Language Model\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "def test_bigram(test_file, model_file, lambda2=0.95, lambda1=0.95, N=1000000):\n",
        "    W = 0 # Total word count\n",
        "    H = 0\n",
        "    probs = load_bigram_model(model_file)\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "            # get rid of the whitespace\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "\n",
        "            # split line into array of words\n",
        "            words = line.split()\n",
        "\n",
        "            # add to the end and beginning\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "            \n",
        "            for i in range(1, len(words)):  # Note: starting at 1, after <s>\n",
        "                # TODO: Write code to calculate smoothed unigram probabilties\n",
        "                # and smoothed bigram probabilities\n",
        "                # You should use calculate p1 as smoothed unigram probability\n",
        "                # and p2 as smoothed bigram probability\n",
        "\n",
        "                # If the unigram exists in the dictionary\n",
        "                if (probs.get(words[i])):\n",
        "                  p1 = lambda1 * probs.get(words[i]) + (1-lambda1) * (1/N)\n",
        "                else: # We assume the value of probs.get(words[i]) to be 0\n",
        "                  p1 = lambda1 * 0 + (1-lambda1) * (1/N) \n",
        "                \n",
        "                # If the bigram exists in the dictionary\n",
        "                if (probs.get(words[i-1] + \" \" + words[i])):\n",
        "                  p2 = lambda2 * probs.get(words[i-1] + \" \" + words[i])+ (1-lambda2) * p1\n",
        "                else: # We assume the value of probs.get(words[i-1] + \" \" + words[i]) to be 0\n",
        "                  p2 = lambda2 * 0 + (1-lambda2) * p1\n",
        "\n",
        "                W += 1  # Count the words\n",
        "                H += -math.log2(p2) # We use logarithm to avoid underflow\n",
        "    H = H/W\n",
        "    P = 2**H\n",
        "    \n",
        "    print(\"Entropy: {}\".format(H))\n",
        "    print(\"Perplexity: {}\".format(P))\n",
        "\n",
        "    return P\n",
        "\n",
        "\n",
        "# So the error occur when there is no \"In computational\" in the testing set, which appear None type in the probs dictionary\n",
        "# The solution is to quantify the missing value (0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb8ajciOBfLY",
        "colab_type": "code",
        "outputId": "2061142f-6509-4850-de74-c6b74b939a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_bigram('wiki-en-train.word', 'bigram_model.txt')\n",
        "test_bigram('wiki-en-test.word', 'bigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy: 11.284859117885485\n",
            "Perplexity: 2495.0605603552463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2495.0605603552463"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}